# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/00_tests/tst.experiment_manager.ipynb (unless otherwise specified).

__all__ = ['init_em_fixture', 'test_set_path_experiments', 'test_set_alternative_path', 'test_get_path_alternative',
           'test_remove_previous_experiments', 'test_basic_usage', 'test_same_values', 'test_almost_same_values',
           'test_new_runs', 'test_second_experiment', 'test_new_parameter', 'test_new_parameter_default',
           'test_other_parameters', 'test_non_valid_parameters', 'test_remove_not_finished', 'test_repeat_experiment',
           'test_check_finished', 'test_recompute_metrics', 'test_prev_epoch', 'test_prev_epoch2', 'test_from_exp',
           'test_skip_interrupted', 'test_use_last_result', 'test_use_last_result_run_interrupted',
           'test_storing_em_args_and_parameters', 'TrackingEM', 'test_set_git_tracking_parameters', 'test_grid_search',
           'test_run_multiple_repetitions', 'parameter_sampler1', 'test_hp_optimization', 'test_hp_optimization_2',
           'test_hp_optimization_3', 'test_greedy_search', 'parameter_sampler2', 'test_rerun_experiment',
           'test_rerun_experiment_pipeline', 'test_rerun_experiment_par', 'test_get_git_revision_hash',
           'test_load_or_create_experiment_values', 'test_get_experiment_numbers', 'test_get_scalar_fields',
           'my_run_experiment', 'my_run_experiment2', 'test_track_experiment_decorator']

# Cell
import pytest
import pandas as pd
import numpy as np
import os
import joblib
from IPython.display import display
import optuna
import sh

from dsblocks.utils.nbdev_utils import md
from dsblocks.utils.utils import check_last_part, remove_previous_results

from hpsearch.experiment_manager import *
from hpsearch.utils.experiment_utils import read_df, write_df
from hpsearch.examples.complex_dummy_experiment_manager import ComplexDummyExperimentManager
from hpsearch.examples.complex_dummy_experiment_manager import init_em
import hpsearch.config.hp_defaults as dflt

# Cell
@pytest.fixture (name='init_em')
def init_em_fixture():
    return init_em()

# Comes from experiment_manager.ipynb, cell
def test_set_path_experiments ():
    em = init_em ('basic')
    check_last_part (em.path_experiments, 'test_basic/default')
    em.set_path_experiments (parent_path='another_path')
    check_last_part (em.path_experiments, 'another_path/default')
    em.set_path_experiments (folder='another_folder')
    check_last_part (em.path_experiments, 'another_path/another_folder')

    em = init_em ('basic', parent_path='test_parent')
    check_last_part (em.path_experiments, 'test_parent/default')

    em = init_em ('basic', folder='other_folder')
    check_last_part (em.path_experiments, 'test_basic/other_folder')

    em = init_em ('basic', parent_path='test_parent', folder='other_folder')
    check_last_part (em.path_experiments, 'test_parent/other_folder')

# Comes from experiment_manager.ipynb, cell
def test_set_alternative_path ():
    em = init_em ('basic')
    assert em.alternative_path is None
    em.set_alternative_path (alternative_parent_path='test_another_path')
    check_last_part (em.alternative_path, 'test_another_path/default')
    em.set_alternative_path (alternative_path='test_alternative_path/new_folder')
    check_last_part (em.alternative_path, 'test_alternative_path/new_folder')

    em = init_em ('basic', alternative_parent_path='test_alternative_parent')
    check_last_part (em.alternative_path, 'test_alternative_parent/default')

    em = init_em ('basic', alternative_path='test_alternative_path/new_folder')
    check_last_part (em.alternative_path, 'test_alternative_path/new_folder')

    em = init_em ('basic', alternative_path='test_alternative_path/new_folder',
                  alternative_parent_path='test_alternative_parent')
    check_last_part (em.alternative_path, 'test_alternative_path/new_folder')

# Comes from experiment_manager.ipynb, cell
def test_get_path_alternative ():
    em = init_em ('basic')

    em.set_alternative_path (alternative_parent_path='test_other_path')
    path_results = em.get_path_results (experiment_id=1, run_number=2)
    check_last_part (path_results,'test_basic/default/experiments/00001/2')
    path_alternative = em.get_path_alternative (path_results)
    check_last_part (path_alternative, 'test_other_path/default/experiments/00001/2')

    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def test_remove_previous_experiments ():
    em = init_em ('basic')

    em.set_alternative_path (alternative_parent_path='test_other_path_basic')
    em.path_experiments.mkdir (parents=True)
    joblib.dump ([1,2,3], em.path_experiments / 'myfile.pk')
    em.alternative_path.mkdir (parents=True)
    joblib.dump ([10,20,30], em.alternative_path / 'other_file.pk')
    print (em.path_experiments)
    print (os.listdir (em.path_experiments))
    print (em.alternative_path)
    print (os.listdir (em.alternative_path))

    # run function
    em.remove_previous_experiments (parent=True)

    # check results
    assert not em.path_experiments.parent.exists() and not em.alternative_path.parent.exists()

# Comes from experiment_manager.ipynb, cell
def test_basic_usage ():
    em = init_em ('basic')

    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})

    # The output is a tuple of two objects:
    #1. The main result metric. In our case, we didn't indicate the name of this metric,
    #and therefore we get None.
    #1. A dictionary containing all the performance metrics for this experiment.

    assert result==0.6
    assert dict_results == {'validation_accuracy': 0.6, 'test_accuracy': 0.5}

    # Eight files  are stored in *path_experiments*, and the `experiments` folder is created:

    files_stored = ['current_experiment_number.pkl', 'experiments', 'experiments_data.csv',
                    'experiments_data.pk', 'experiments_data_columns.pk', 'git_hash.json', 'managers',
                    'other_parameters.csv', 'parameters.pk', 'parameters.txt', 'summary.txt']
    display (files_stored)

    path_experiments = em.path_experiments

    assert (sorted(os.listdir (path_experiments))==
            files_stored)

    # TODO TEST: test content of the above files

    import pandas as pd

    df = read_df (path_experiments)
    #df = em.get_experiment_data ()
    expected_col = list(pd.MultiIndex.from_product([['parameters'],['offset','rate'],['']]))
    expected_col += list(pd.MultiIndex.from_product([['run_info'],['date','finished','time'],[0]]))
    expected_col += list(pd.MultiIndex.from_product([['scores'],['test_accuracy','validation_accuracy',],[0]]))
    assert df.columns.tolist()==expected_col
    assert df.shape[0]==1

    md ('experiment dataframe:'); display(df)

    list_exp = os.listdir (f'{path_experiments}/experiments')
    print (f'folder created in `{path_experiments}/experiments`:'); print(list_exp)
    assert list_exp == ['00000']
    print ('This folder has one sub-folder per run, since '
            'multiple runs can be done with the same parameters.')
    list_run = os.listdir (f'{path_experiments}/experiments/00000')
    print (f'contents of current run at `{path_experiments}/experiments/00000`:'); print(list_run)

    # the same data frame can be obtained by doing:
    df_bis = em.get_experiment_data ()
    pd.testing.assert_frame_equal(df,df_bis)

    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def test_same_values ():
    em = init_em ('same_values')
    path_experiments = em.path_experiments

    # first experiment
    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})

    em.raise_error_if_run = True
    # second experiment
    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})

    df = em.get_experiment_data ()

    expected_col = list(pd.MultiIndex.from_product([['parameters'],['offset','rate'],['']]))
    expected_col += list(pd.MultiIndex.from_product([['run_info'],['date','finished','time'],[0]]))
    expected_col += list(pd.MultiIndex.from_product([['scores'],['test_accuracy','validation_accuracy',],[0]]))
    assert df.columns.tolist()==expected_col
    assert df.shape[0]==1

    md ('experiment dataframe:'); display(df)

    # As we can see, no new experiment is added to the DataFrame, since the values of the parameters used
    # are already present in the first experiment.

    list_exp = os.listdir (f'{path_experiments}/experiments')

    print (f'folders created in `{path_experiments}/experiments`:'); print(list_exp)

    assert list_exp == ['00000']

    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def test_almost_same_values ():
    em = init_em ('almost_same_values')
    path_experiments = em.path_experiments

    # first experiment
    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})

    em.raise_error_if_run = True
    # second experiment: the difference between the values of rate parameter is 1.e-16:
    # too small to be considered different
    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05+1e-16})

    df = em.get_experiment_data ()
    expected_col = list(pd.MultiIndex.from_product([['parameters'],['offset','rate'],['']]))
    expected_col += list(pd.MultiIndex.from_product([['run_info'],['date','finished','time'],[0]]))
    expected_col += list(pd.MultiIndex.from_product([['scores'],['test_accuracy','validation_accuracy',],[0]]))
    assert df.columns.tolist()==expected_col
    assert df.shape[0]==1

    list_exp = os.listdir (f'{path_experiments}/experiments')
    assert list_exp == ['00000']

    # consider 1.e-17 difference big enough
    em.raise_error_if_run = False
    # second experiment: the difference between the values of rate parameter is 1.e-16:
    # too small to be considered different
    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05+1e-16},
                                                         precision=1e-17)

    df = em.get_experiment_data ()
    expected_col = list(pd.MultiIndex.from_product([['parameters'],['offset','rate'],['']]))
    expected_col += list(pd.MultiIndex.from_product([['run_info'],['date','finished','time'],[0]]))
    expected_col += list(pd.MultiIndex.from_product([['scores'],['test_accuracy','validation_accuracy',],[0]]))
    assert df.columns.tolist()==expected_col
    assert df.shape[0]==2

    display (df)
    list_exp = os.listdir (f'{path_experiments}/experiments')
    assert sorted(list_exp) == sorted(['00000', '00001'])

    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def test_new_runs ():
    em = init_em ('new_runs')
    path_experiments = em.path_experiments

    # first experiment
    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})

    # second experiment: in order to run another experiment with same parametres, we increase
    # the run number. The default run number used in the first experiment is 0, so we indicate
    # run_number=1
    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05},
                                                         run_number=1)

    df = em.get_experiment_data ()
    expected_col = list(pd.MultiIndex.from_product([['parameters'],['offset','rate'],['']]))
    expected_col += list(pd.MultiIndex.from_product([['run_info'],['date','finished','time'],[0, 1]]))
    expected_col += list(pd.MultiIndex.from_product([['scores'],['test_accuracy','validation_accuracy',],[0, 1]]))
    assert df.columns.tolist()==expected_col
    assert df.shape[0]==1

    md ('experiment dataframe:'); display(df)

    # another adding a new run number is to indicate run_number=None. This will make the experiment
    # manager find the next run number automatically. Since we have used run numbers 0 and 1,
    # the next run number will be 2
    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05},
                                                         run_number=None)

    df = em.get_experiment_data ()
    expected_col = list(pd.MultiIndex.from_product([['parameters'],['offset','rate'],['']]))
    expected_col += list(pd.MultiIndex.from_product([['run_info'],['date','finished','time'],[0, 1, 2]]))
    expected_col += list(pd.MultiIndex.from_product([['scores'],['test_accuracy','validation_accuracy',],[0, 1, 2]]))
    assert df.columns.tolist()==expected_col
    assert df.shape[0]==1

    md ('experiment dataframe:'); display(df)

    # As we can see, no new experiment is added to the DataFrame, since the values of the parameters used
    # are already present in the first experiment.

    list_exp = os.listdir (f'{path_experiments}/experiments')

    print (f'folders created in `{path_experiments}/experiments`:'); print(list_exp)

    assert list_exp == ['00000']

    list_runs = os.listdir (f'{path_experiments}/experiments/00000')
    if False:
        assert sorted(list_runs) == ['0',
                                     '1',
                                     '2',
                                     'other_parameters.json',
                                     'parameters.json',
                                     'parameters.pk',
                                     'parameters.txt']
    else:
        print (sorted(list_runs))

    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def test_second_experiment ():
    em = init_em ('second')
    path_experiments = em.path_experiments

    # first experiment
    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})

    md ('If we run a second experiment with new parameters, a new row is '
        'added to the dataframe, and a new folder is created:')

    # second experiment
    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.7, 'rate': 0.2})

    df = em.get_experiment_data ()
    expected_col = list(pd.MultiIndex.from_product([['parameters'],['offset','rate'],['']]))
    expected_col += list(pd.MultiIndex.from_product([['run_info'],['date','finished','time'],[0]]))
    expected_col += list(pd.MultiIndex.from_product([['scores'],['test_accuracy','validation_accuracy',],[0]]))
    assert df.columns.tolist()==expected_col
    assert df.shape[0]==2

    md ('experiment dataframe:'); display(df)

    list_exp = os.listdir (f'{path_experiments}/experiments')

    md (f'folders created in `{path_experiments}/experiments`:'); print(list_exp)

    assert sorted(list_exp) == sorted(['00000','00001'])

    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def test_new_parameter ():
    em = init_em ('another_parameter')
    path_experiments = em.path_experiments

    # first experiment
    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})

    # second experiment:
    # same parameters as before plus new parameter 'epochs' not indicated in first experiment
    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 5})

    df = em.get_experiment_data ()

    # a new experiment is added, and a new parameter `epochs` is added as additional column at the end
    expected_col = list(pd.MultiIndex.from_product([['parameters'],['epochs', 'offset', 'rate'],['']]))
    expected_col += list(pd.MultiIndex.from_product([['run_info'],['date', 'finished', 'time'],[0]]))
    expected_col += list(pd.MultiIndex.from_product([['scores'],['test_accuracy', 'validation_accuracy',],[0]]))
    assert df.columns.tolist()==expected_col
    assert df.shape[0]==2

    assert (df.index==[0,1]).all()

    # the new parameter has None value for all previous experiments that did not indicated its value
    # In our case, the first experiment has None value for parameter `epochs`
    # This means that the default value of epochs is used for that parameter.
    # In our case, if we look at the implementation of DummyExperimentManager, we can see that
    # the default value for epochs is 10.
    mi_epochs = (dflt.parameters_col, 'epochs', '')
    assert df.isna().loc[0, mi_epochs]
    assert df.loc[1, mi_epochs] == 5.0

    md ('experiment dataframe:'); display(df)

    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def test_new_parameter_default ():
    em = init_em ('another_parameter_default')
    path_experiments = em.path_experiments

    # first experiment
    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})

    # second experiment:
    # same parameters as before plus new parameter 'epochs' not indicated in first experiment
    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 10})

    df = em.get_experiment_data ()

    # in this case, no new experiment is added, since the new parameter has the same value as the default value
    # implicitly used in the first experiment.
    expected_col = list(pd.MultiIndex.from_product([['parameters'],['offset','rate'],['']]))
    expected_col += list(pd.MultiIndex.from_product([['run_info'],['date','finished','time'],[0]]))
    expected_col += list(pd.MultiIndex.from_product([['scores'],['test_accuracy','validation_accuracy',],[0]]))
    assert df.columns.tolist()==expected_col
    assert df.shape[0]==1

    assert (df.index==[0]).all()

    md ('experiment dataframe:'); display(df)

    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def test_other_parameters ():
    em = init_em ('other_parameters')
    path_experiments = em.path_experiments

    # first experiment:
    # we use the other_parameters argument to indicate a parameter that does not affect the outcome
    # of the experiment
    # in this example, we change the level of verbosity. This parameter should not affect how the
    # experiment runs, and therefore we tell our experiment manager to not create a new experiment
    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05},
                                                         other_parameters={'verbose': False})

    # second experiment:
    # same parameters as before except for the verbosity parameter. Our experiment manager considers
    # this experiment the same as before, and therefore it does not run it, but outputs the same results
    # obtained before
    em.raise_error_if_run = True
    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})

    df = em.get_experiment_data ()

    # in this case, no new experiment is added, since the new parameter has the same value as the default value
    # implicitly used in the first experiment.
    expected_col = list(pd.MultiIndex.from_product([['parameters'],['offset','rate'],['']]))
    expected_col += list(pd.MultiIndex.from_product([['run_info'],['date','finished','time'],[0]]))
    expected_col += list(pd.MultiIndex.from_product([['scores'],['test_accuracy','validation_accuracy',],[0]]))
    assert df.columns.tolist()==expected_col
    assert df.shape[0]==1

    assert (df.index==[0]).all()

    md ('experiment dataframe:'); display(df)

    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def test_non_valid_parameters ():
    em = init_em ('non_valid_parameters')
    path_experiments = em.path_experiments
    result, dict_results = em.create_experiment_and_run (parameters={'nonvalid': [1,2], 'bad': {'yes': 'no'} ,
                                                                    'good': 'yes', 'valid': None})
    df = em.get_experiment_data ()
    display (df)

    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def test_remove_not_finished ():
    em = init_em ('remove_not_finished')
    path_experiments = em.path_experiments

    # first experiment: we simulate that a halt before finishing
    with pytest.raises (KeyboardInterrupt):
        result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05},
                                                         other_parameters={'halt':True})

    df = em.get_experiment_data ()
    display(df)

    # second experiment: remove unfinished
    result, dict_results = em.create_experiment_and_run (parameters={'offset':1.0, 'rate': 0.2})

    df = em.get_experiment_data ()
    display(df)

    result, dict_results = em.create_experiment_and_run (parameters={'offset':1.0, 'rate': 0.3},
                                                         remove_not_finished=True)

    df = em.get_experiment_data ()
    display(df)

    # in this case, no new experiment is added, since the new parameter has the same value as the default value
    # implicitly used in the first experiment.

    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def test_repeat_experiment ():
    em = init_em ('repeat_experiment')
    path_experiments = em.path_experiments

    # first experiment
    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})

    df = em.get_experiment_data ()
    display(df)
    mi_date = (dflt.run_info_col, 'date', 0)
    date = df[mi_date].values[0]

    # second experiment
    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05},
                                                         repeat_experiment = True)

    df = em.get_experiment_data ()
    display(df)
    assert df[mi_date].values[0] != date


    # in this case, no new experiment is added, since the new parameter has the same value as the default value
    # implicitly used in the first experiment.
    expected_col = list(pd.MultiIndex.from_product([['parameters'],['offset','rate'],['']]))
    expected_col += list(pd.MultiIndex.from_product([['run_info'],['date','finished','time'],[0]]))
    expected_col += list(pd.MultiIndex.from_product([['scores'],['test_accuracy','validation_accuracy',],[0]]))
    assert df.columns.tolist()==expected_col
    assert df.shape[0]==1


    assert (df.index==[0]).all()

    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def test_check_finished ():
    em = init_em ('check_finished')
    path_experiments = em.path_experiments

    # first experiment: we simulate that we only run for half the number of epochs
    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 10},
                                                         other_parameters={'actual_epochs': 5})

    df = em.get_experiment_data ()
    mi_date = (dflt.run_info_col, 'date', 0)
    date = df[mi_date].values[0]
    mi_validation = ('scores', 'validation_accuracy', 0)
    score = df[mi_validation].values[0]

    # second experiment: same values in parameters dictionary, without other_parameters
    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 10})

    df = em.get_experiment_data ()

    assert (date==df[mi_date].values[0]) and (score==df[mi_validation].values[0])

    # third experiment: same values in parameters dictionary, with other_parameters indicating check_finished
    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 10},
                                                         check_finished=True)

    df = em.get_experiment_data ()
    expected_col = list(pd.MultiIndex.from_product([['parameters'],['offset','rate'],['']]))
    expected_col += list(pd.MultiIndex.from_product([['run_info'],['date','finished','time'],[0]]))
    expected_col += list(pd.MultiIndex.from_product([['scores'],['test_accuracy','validation_accuracy',],[0]]))
    assert df.columns.tolist()==expected_col
    assert df.shape[0]==1

    assert (df.index==[0]).all()
    assert (date!=df[mi_date].values[0]) and (score!=df[mi_validation].values[0])

    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def test_recompute_metrics ():
    em = init_em ('recompute_metrics')
    path_experiments = em.path_experiments

    # first experiment
    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})
    # second experiment: new values
    em.raise_error_if_run = True
    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.02},
                                                         recompute_metrics=True)

    df = em.get_experiment_data ()
    expected_col = list(pd.MultiIndex.from_product([['parameters'],['offset','rate'],['']]))
    expected_col += list(pd.MultiIndex.from_product([['run_info'],['date','finished','time'],[0]]))
    expected_col += list(pd.MultiIndex.from_product([['scores'],['test_accuracy','validation_accuracy',],[0]]))
    assert df.columns.tolist()==expected_col
    assert df.shape[0]==2

    mi_validation = ('scores', 'validation_accuracy', 0)
    assert np.isnan(df[mi_validation].values[1])

    # third experiment: new values
    em.raise_error_if_run = False
    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.02,
                                                                     'epochs': 10},
                                                         recompute_metrics=True,
                                                         force_recompute_metrics=True)

    df = em.get_experiment_data ()
    expected_col = list(pd.MultiIndex.from_product([['parameters'],['offset','rate'],['']]))
    expected_col += list(pd.MultiIndex.from_product([['run_info'],['date','finished','time'],[0]]))
    expected_col += list(pd.MultiIndex.from_product([['scores'],['test_accuracy','validation_accuracy',],[0]]))
    assert df.columns.tolist()==expected_col
    assert df.shape[0]==2

    assert (df.index==[0,1]).all()
    assert df[mi_validation].values[1]==0.3

    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def test_prev_epoch ():
    em = init_em ('prev_epoch')

    # get reference result
    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 17})
    reference_accuracy = em.model.accuracy
    reference_weight = em.model.weight
    df = em.get_experiment_data ()
    display (df)
    em.remove_previous_experiments (parent=True)

    # first 3 experiments
    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 10})
    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 20})
    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 15})

    # more epochs
    # in order to work, we need our experiment manager to make use of the
    # parameter 'resume' or the parameter 'prev_path_results'.
    # In particular, we need it to load the model file
    # whose path is indicated in parameters['resume'], or whose path is
    # indicated in f'{parameters["prev_path_results"]}/{self.model_file_name}'
    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 17},
                                      prev_epoch=True)




    assert em.model.epochs==2 and em.model.current_epoch==17

    assert reference_accuracy==em.model.accuracy and reference_weight==em.model.weight

    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 17},
                                      repeat_experiment = True)

    assert em.model.epochs==17 and em.model.current_epoch==17

    assert reference_accuracy==em.model.accuracy and reference_weight==em.model.weight

    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def test_prev_epoch2 ():
    em = init_em ('prev_epoch2')

    em.remove_previous_experiments (parent=True)
    score, results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},
                                          other_parameters={'actual_epochs': 2})


    assert score==0.16 and results['validation_accuracy']==0.16
    assert em.model.current_epoch==2 and em.model.epochs==2

    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.04, 'epochs': 5})


    # We use last result and have the required number of epochs to default number (50)
    # But we request to run the experiment until the end
    score, results = em.create_experiment_and_run (
        parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},
        prev_epoch=True, check_finished=True, use_previous_best=False
    )

    assert score==0.25 and results['validation_accuracy']==0.25
    assert em.model.current_epoch==5 and em.model.epochs==3
    df = em.get_experiment_data ()
    mi_validation = ('scores', 'validation_accuracy', 0)
    assert (df[mi_validation]==[0.25, 0.30]).all()


    em.remove_previous_experiments (parent=True)

    # **********************************
    with pytest.raises (KeyboardInterrupt):
        _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},
                                          other_parameters={'actual_epochs': 2, 'halt': True})

    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.04, 'epochs': 5})

    # We use last result and have the required number of epochs to default number (50)
    # But we request to run the experiment until the end
    score, results = em.create_experiment_and_run (
        parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},
        prev_epoch=True, check_finished_if_interrupted=True,
        use_previous_best=False)
    assert score==0.25 and results['validation_accuracy']==0.25
    assert em.model.current_epoch==5 and em.model.epochs==3
    df = em.get_experiment_data ()
    assert (df[mi_validation]==[0.25, 0.30]).all()
    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def test_from_exp ():
    em = init_em ('from_exp')
    path_experiments = em.path_experiments

    # get reference result
    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 5})
    reference_accuracy = em.model.accuracy
    reference_weight = em.model.weight
    em.remove_previous_experiments (parent=True)

    # first 3 experiments
    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.03, 'epochs': 2})
    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.04, 'epochs': 2})

    # the following resumes from experiment 0, and trains the model for 5 more epochs
    # using now different `offset` and `rate` hyper-parameters
    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 5},
                                      from_exp=0)

    assert em.model.epochs==5 and em.model.current_epoch==7
    correct_accuracy = (0.1 + 0.03*2 # accuracy of model from experiment 0
                        + 0.05*5)     # accuracy gained by training for 5 more epochs using
                                    #  new hyper-parameters: rate=0.05
    assert (em.model.accuracy-correct_accuracy) < 1e-10
    assert reference_accuracy!=em.model.accuracy

    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def test_skip_interrupted ():
    em = init_em ('skip_interrupted')
    path_experiments = em.path_experiments

    # first 3 experiments
    with pytest.raises (KeyboardInterrupt):
        _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},
                                          other_parameters={'halt': True})

    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.04, 'epochs': 5})

    em.raise_error_if_run = True
    score, results = em.create_experiment_and_run (
        parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},
        skip_interrupted=True)
    assert score is None and len(results)==0

    score, results = em.create_experiment_and_run (
        parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},
        skip_interrupted=True, min_iterations=1)
    assert score is None and len(results)==0

    em.model_file_name='wrong_file.pk'
    with pytest.raises (RuntimeError):
        score, results = em.create_experiment_and_run (
            parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},
            skip_interrupted=True)

    df = em.get_experiment_data ()
    display (df)

    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def test_use_last_result ():
    em = init_em ('use_last_result')
    path_experiments = em.path_experiments

    # first 3 experiments
    with pytest.raises (KeyboardInterrupt):
        _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},
                                          other_parameters={'halt': True})

    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.04, 'epochs': 5})

    df = em.get_experiment_data ()
    mi_validation = ('scores', 'validation_accuracy', 0)
    assert (df.isna()[mi_validation] == [True, False]).all()

    # We use last result but require that number of epochs is at least 50.
    # Since this is not true, the last result is not used.
    score, results = em.create_experiment_and_run (
        parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},
        use_last_result=True)
    assert score is None and results=={}
    df = em.get_experiment_data ()
    display(df)
    assert (df.isna()[mi_validation] == [True, False]).all()

    # We use last result and lower the required number of epochs to 2
    score, results = em.create_experiment_and_run (
        parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},
        use_last_result=True, min_iterations=2)
    print (score, results)
    assert score==0.25 and results=={'validation_accuracy': 0.25, 'test_accuracy': 0.35, 'accuracy': 0.25, 'last': 5}
    df = em.get_experiment_data ()
    display(df)
    assert (df.isna()[mi_validation] == [False, False]).all()
    assert (df[mi_validation] == [0.25, 0.30]).all()

    # We use last result and increase the required number of epochs to default number (50)
    # But we request to run the experiment until the end
    score, results = em.create_experiment_and_run (
        parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},
        use_last_result=True, run_if_not_interrumpted=True)
    print (score, results)
    #assert score==None and results=={'validation_accuracy': 0.25, 'test_accuracy': 0.35, 'accuracy': 0.25, 'last': 5}
    df = em.get_experiment_data ()
    display(df)

    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def test_use_last_result_run_interrupted ():
    em = init_em ('use_last_result_run_interrupted')
    path_experiments = em.path_experiments

    # first 3 experiments
    with pytest.raises (KeyboardInterrupt):
        _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},
                                          other_parameters={'actual_epochs': 2, 'halt': True})

    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.04, 'epochs': 5})

    df = em.get_experiment_data ()
    #display (df)
    mi_validation = ('scores', 'validation_accuracy', 0)
    assert (df.isna()[mi_validation] == [True, False]).all()

    # We use last result and have the required number of epochs to default number (50)
    # But we request to run the experiment until the end
    score, results = em.create_experiment_and_run (
        parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},
        use_last_result=True, run_if_not_interrumpted=True)
    print (score, results)
    #assert score==None and results=={'validation_accuracy': 0.25, 'test_accuracy': 0.35, 'accuracy': 0.25, 'last': 5}
    df = em.get_experiment_data ()
    #display(df)
    assert em.model.current_epoch==5 and em.model.epochs==5
    assert (df.isna()[mi_validation] == [False, False]).all()
    assert (df[mi_validation] == [0.25, 0.30]).all()

    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def test_storing_em_args_and_parameters ():
    em = init_em ('storing_em_args_and_parameters')

    path_experiment = em.get_path_experiment (0)
    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})
    check_last_part (path_experiment, 'test_storing_em_args_and_parameters/default/experiments/00000')
    ref_list = ['0', 'em_args.json', 'em_attrs.json',  'info.json', 'other_parameters.json',
     'parameters.json', 'parameters.pk', 'parameters.txt', 'test_experiment_manager.py',]
    ref_list2 = ref_list.copy()
    del ref_list2[3]
    if __name__ == '__main__':
        del ref_list[-1]
        del ref_list2[-1]
    result_list = sorted (os.listdir(path_experiment))
    assert result_list==ref_list or result_list==ref_list2
    par, other, em_args, info, em_attrs = joblib.load (path_experiment/'parameters.pk')
    print (em_args)
    #assert em_args ==  {'run_number': 0, 'log_message': None, 'stack_level': -3}

    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
class TrackingEM (ExperimentManager):
    def run_experiment (self, parameters = {},
                        path_results='test_set_git_tracking_parameters'):
        from dsblocks.utils.dummies import make_pipe1
        pipeline = make_pipe1 (**parameters,
                               path_results=path_results,
                               pipeline=dict(root=True))
        result = pipeline (np.array([parameters['a']]))
        dict_results = {'result': result, 'constant': 3}
        self.pipeline = pipeline
        return dict_results
    def get_default_parameters (self, parameters):
        return {}

@pytest.mark.no_github
def test_set_git_tracking_parameters ():
    from dsblocks.utils.utils import check_new_files
    path_experiments = 'test_set_git_tracking_parameters'

    # **********************************
    # new branch
    # **********************************
    git = sh.git.bake ()
    current_branch = str(git ('rev-parse --abbrev-ref HEAD'.split()))[:-1]
    git.checkout ('-b', path_experiments)
    before_status = git.status ('-s', '-uno')
    before_commit = git ('rev-parse', 'HEAD')

    # **********************************
    # test add_metadata
    # **********************************
    em = TrackingEM (path_experiments=path_experiments)
    em.create_experiment_and_run ({'a':1, 'b': 2}, {}, do_git_tracking=True)

    # **********************************
    # check results
    # **********************************
    after_commit = git ('rev-parse', 'HEAD')
    assert before_commit != after_commit
    after_status = git.status ('-s', '-uno')
    message = git.log('-1 --pretty=%B'.split())
    print (message)
    #assert 'new metadata files in 0' in str(message)
    git.diff ('HEAD..HEAD~1 --name-only'.split())
    path_experiment = 'test_set_git_tracking_parameters/experiments/00000'
    path_results = f'{path_experiment}/0'
    path_config = f'{path_results}/config'
    new_files = [f'{path_config}/call_stack.json',
                 f'{path_config}/root_metadata.json',
                 f'{path_results}/experiment.log',
                 f'{path_results}/parameters.json',
                 f'{path_results}/parameters.pk',
                 f'{path_results}/parameters.txt',
                 f'{path_results}/summary.txt',
                 f'{path_experiment}/em_args.json',
                 f'{path_experiment}/em_attrs.json',
                 f'{path_experiment}/info.json',
                 f'{path_experiment}/other_parameters.json',
                 f'{path_experiment}/parameters.json',
                 f'{path_experiment}/parameters.pk',
                 f'{path_experiment}/parameters.txt']

    check_new_files (git, new_files)
    after_status = git.status ('-s', '-uno')
    #assert str(after_status)==str(before_status)

    # **********************************
    # clean
    # **********************************
    git.checkout (current_branch)
    git.branch (f'-D {path_experiments}'.split())
    em.remove_previous_experiments (parent=False)

# Comes from experiment_manager.ipynb, cell
def test_grid_search ():
    em = init_em ('grid_search')

    # *********************************
    # *********************************
    em.grid_search (parameters_multiple_values={'rate': [0.03,0.01], 'epochs': [5, 7]},
                    parameters_single_value={'offset':0.1},
                    other_parameters={'verbose':False})
    df = em.get_experiment_data ()
    mi_epochs = (dflt.parameters_col, 'epochs', '')
    mi_rate = (dflt.parameters_col, 'rate', '')
    mi_offset = (dflt.parameters_col, 'offset', '')
    assert (df[mi_epochs]==[5.0, 5.0, 7.0, 7.0]).all()
    assert (df[mi_rate].values[[0,2]]==[0.03, 0.03]).all()
    assert (df.isna()[mi_rate]==[False, True, False, True]).all()
    assert (df[mi_offset]==0.1).all()
    mi_validation = ('scores', 'validation_accuracy', 0)
    mi_validation_1 = ('scores', 'validation_accuracy', 1)
    assert (np.abs(df[mi_validation]-[0.25, 0.15, 0.31, 0.17])<1.0e-15).all()

    #assert (df.isna()[mi_validation] == [True, False]).all()

    # *********************************
    # *********************************
    em.raise_error_if_run = True
    em.grid_search (parameters_multiple_values={'rate': [0.01,0.03], 'epochs': [7, 5]},
                    parameters_single_value={'offset':0.1})
    df = em.get_experiment_data ()
    assert (df[mi_epochs]==[5.0, 5.0, 7.0, 7.0]).all()
    assert (df[mi_rate].values[[0,2]]==[0.03, 0.03]).all()
    assert (df.isna()[mi_rate]==[False, True, False, True]).all()
    assert (df[mi_offset]==0.1).all()
    assert (np.abs(df[mi_validation]-[0.25, 0.15, 0.31, 0.17])<1.0e-15).all()

    # *********************************
    # *********************************
    em.remove_previous_experiments (parent=True)
    em.raise_error_if_run = False
    em.grid_search (parameters_multiple_values={'rate': [0.01,0.03], 'epochs': [7, 5]},
                    parameters_single_value={'offset':0.1, 'noise':0.0001}, nruns=2)
    df = em.get_experiment_data ()
    assert (df[mi_epochs]==[7.0, 7.0, 5.0, 5.0]).all()
    assert (df[mi_rate].values[[1,3]]==[0.03, 0.03]).all()
    assert (df.isna()[mi_rate]==[True, False, True, False]).all()
    assert (df[mi_offset]==0.1).all()
    assert (np.abs(df[mi_validation]-[0.17, 0.31, 0.15, 0.25])<0.1).all()
    assert (np.abs(df[mi_validation_1]-[0.17, 0.31, 0.15, 0.25])<0.1).all()
    assert (df[mi_validation]!=df[mi_validation_1]).all()

    # *********************************
    # *********************************
    em.remove_previous_experiments (parent=True)
    np.random.seed (42)
    em.grid_search (parameters_multiple_values={'rate': [0.01,0.03], 'epochs': [7, 5]},
                    parameters_single_value={'offset':0.1}, random_search=True,
                    other_parameters={'verbose':False})

    df = em.get_experiment_data ()
    assert (df[mi_epochs]==[7., 5., 7., 5.]).all()
    assert (df[mi_rate].values[[0,1]]==[0.03, 0.03]).all()
    assert (df.isna()[mi_rate]==[False, False, True, True]).all()
    assert (df[mi_offset]==0.1).all()
    assert (np.abs(df[mi_validation]-[0.31, 0.25, 0.17, 0.15])<1e-15).all()

    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def test_run_multiple_repetitions ():
    em = init_em ('run_multiple_repetitions')
    np.random.seed (42)

    mu, std, dict_results = em.run_multiple_repetitions (
        parameters={'rate': 0.03, 'epochs': 5, 'offset': 0.1},
        other_parameters = {'verbose': False, 'noise': 0.001}, nruns=5
    )
    df = em.get_experiment_data ()
    assert df.shape==(1,28)
    x=[(dflt.scores_col, 'validation_accuracy', i) for i in range(5)]; assert df.columns.isin(x).sum()==5
    assert (0 < np.abs(mu-0.25) < 1e-3) and (0 < std < 1e-3)

    # *********************************
    # *********************************
    em.remove_previous_experiments (parent=True)
    mu, std, dict_results = em.run_multiple_repetitions (
        parameters={'rate': 0.03, 'epochs': 5, 'offset': 0.1},
        other_parameters = {'verbose': False}
    )
    df = em.get_experiment_data ()
    assert df.shape==(1,8)
    assert mu==0.25 and std==0

    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def parameter_sampler1 (trial):
    rate = trial.suggest_uniform('rate', 0.001, 0.01)
    offset = trial.suggest_categorical('offset', [0.01, 0.05, 0.1])

    parameters = dict(rate=rate,
                      offset=offset)

    return parameters

def test_hp_optimization ():
    em = init_em ('hp_optimization')
    np.random.seed (42)

    parameters = {'epochs': 12}

    em.hp_optimization (parameter_sampler=parameter_sampler1, parameters=parameters,
                        study_name='test_hp_optimization_study',
                        n_trials=5)
    # show
    df = em.get_experiment_data ()
    display (df)
    assert df.shape == (5,8)

    # check
    offset_col = (dflt.parameters_col, 'offset', '')
    assert (df[offset_col]==[0.01,0.10,0.05,0.01,0.10]).all()
    rate_col = (dflt.parameters_col, 'rate', '')
    assert np.max(np.abs(df[rate_col]-[0.005939, 0.004813, 0.009673, 0.006112, 0.001182])) < 1e-5
    epochs_col = (dflt.parameters_col, 'epochs', '')
    assert (df[epochs_col]==12).all()

    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def test_hp_optimization_2 ():
    em = init_em ('hp_optimization_2')
    np.random.seed (42)
    parameters = {'epochs': 21}
    em.hp_optimization (sampler_method='skopt', pruner_method='halving',
                        parameter_sampler=parameter_sampler1, parameters=parameters,
                        study_name='test_hp_optimization_study_2',
                        n_trials=6, nruns=2)
    # show
    df = em.get_experiment_data ()
    display (df)

    # check
    assert df.shape == (6,13)
    assert sorted(df[(dflt.scores_col, 'test_accuracy')].columns)==[0,1]
    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def test_hp_optimization_3 ():
    em = init_em ('hp_optimization_3')
    np.random.seed (42)

    parameters = {'epochs': 21}

    em.hp_optimization (sampler_method='tpe', pruner_method='median',
                        parameter_sampler=parameter_sampler1, parameters=parameters,
                        study_name='test_hp_optimization_study_3',
                        n_trials=6, nruns_best=5)
    # show
    df = em.get_experiment_data ()
    display (df)

    # check
    assert df.shape==(6,28)
    assert sorted(df[(dflt.scores_col, 'test_accuracy')].columns)==list(range(5))
    assert all(df[(dflt.scores_col, 'test_accuracy')].loc[:,1:4].isna().all(axis=1)==[True]*5+[False])
    assert all(df[(dflt.scores_col, 'test_accuracy')].loc[:,1:4].isna().any(axis=1)==[True]*5+[False])

    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def test_greedy_search ():
    em = init_em ('greedy_search')

    em.greedy_search (parameters_multiple_values={'rate': [0.03, 0.01]},
                      parameters_greedy={'epochs': [5, 7], 'offset': [0.1, 0.2, 0.3]},
                      other_parameters={'verbose':False})

    df = em.get_experiment_data ()
    display (df)

    # checks
    assert (df[('parameters', 'rate')][0::2]==0.03).all() and df[('parameters', 'rate')][1::2].isna().all()
    assert (df[('parameters', 'epochs')].loc[:1]==5).all() and (df[('parameters', 'epochs')].loc[2:]==7).all()
    assert df[('parameters', 'offset')].loc[0:3].isna().all() and (df[('parameters', 'offset')].loc[4:]==[0.1,0.1,0.2,0.2,0.3,0.3]).all()

    em.greedy_search (parameters_multiple_values={'noise': [1.0, 0.0]},
                      parameters_greedy=[{'epochs': [10, 20]},
                                         {'offset': [0.3, 0.6, 0.5]},
                                         {'rate': [0.1, 0.2, 0.3]}],
                      other_parameters={'verbose':False})

    df = em.get_experiment_data ()
    display (df)

    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def parameter_sampler2 (trial):
    epochs = trial.suggest_categorical('epochs', [2, 4])
    offset = trial.suggest_categorical('offset', [0.02, 0.06])

    parameters = dict(epochs=epochs, offset=offset)

    return parameters

def test_rerun_experiment ():
    em = init_em ('rerun_experiment')

    # first 3 experiments
    with pytest.raises (KeyboardInterrupt):
        _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 5},
                                          other_parameters={'halt': True})
    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.03, 'epochs': 2})
    _ = em.create_experiment_and_run (parameters={'rate': 0.04})
    df = em.get_experiment_data ()
    assert df.shape==(3,8)

    # ****************************************
    # case 1: re-running finished experiment
    # ****************************************
    em.raise_error_if_run = True
    em.rerun_experiment (experiments=[1])

    # ****************************************
    # case 2: re-running interrupted experiment
    # ****************************************
    em.raise_error_if_run = False
    em.rerun_experiment (experiments=[0], other_parameters={'halt':False, 'verbose':False})
    df = em.get_experiment_data ()
    mi_validation = ('scores', 'validation_accuracy', 0)
    mi_validation_1 = ('scores', 'validation_accuracy', 1)
    assert df.loc[0,mi_validation]==0.35

    # ****************************************
    # case 3: adding more runs to previous experiment
    # ****************************************
    em.rerun_experiment (experiments=[1], nruns=5, other_parameters={'noise': 0.001, 'verbose':False})
    df = em.get_experiment_data ()
    x=[(dflt.scores_col, 'validation_accuracy', i) for i in range(5)]; assert df.columns.isin(x).sum()==5
    assert df.shape==(3,28)

    # ****************************************
    # case 4: using previous experiment parameters as fixed, and using grid search with other
    # parameters
    # ****************************************
    em.rerun_experiment (experiments=[2],
                         parameters_multiple_values={'offset': [0.01,0.05], 'epochs': [3,5]},
                         other_parameters={'verbose':False},
                         nruns=2)
    df = em.get_experiment_data ()
    assert df.shape==(7,28)
    assert np.max(np.abs(df[mi_validation].values- [0.35, 0.16, 0.9,  0.13, 0.17, 0.21, 0.25])) < 1e-10
    assert df.isna()[mi_validation_1].sum()==2
    n1 = (~df.isna())[mi_validation_1].sum()
    n0 = (~df.isna())[mi_validation].sum()

    # ****************************************
    # case 5: using previous experiment parameters as fixed, and using BO with other
    # parameters
    # ****************************************
    em.rerun_experiment (experiments=[2],
                         parameter_sampler=parameter_sampler2,
                         other_parameters={'verbose':False},
                         n_trials=4, sampler_method='skopt')
    df2 = em.get_experiment_data ()
    display (df2)
    print (df2.shape)
    assert df2.shape[0]>7
    n1 = (~df2.isna())[mi_validation_1].sum()
    n0 = (~df2.isna())[mi_validation].sum()
    #assert (n0+n1)==16

    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def test_rerun_experiment_pipeline ():
    em = init_em ('rerun_experiment_pipeline')

    # first 3 experiments
    with pytest.raises (KeyboardInterrupt):
        _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 5},
                                          other_parameters={'halt': True})
    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.03, 'epochs': 2})
    _ = em.run_multiple_repetitions (parameters={'rate': 0.04}, nruns=5)
    df = em.get_experiment_data ()
    display (df)
    mi_validation = ('scores', 'validation_accuracy', 0)
    mi_test = ('scores', 'test_accuracy', 0)
    assert np.abs(df.loc[1,mi_validation]-0.16)<1e-5 and (df.loc[1, mi_test]-0.26)<1e-5
    #print (df.shape)
    assert df.shape==(3,28)

    # ****************************************
    # case 1: re-running finished experiment
    # ****************************************
    # the following produces an error since run_numbers must be indicated
    with pytest.raises (TypeError):
        em.rerun_experiment_pipeline (experiments=[1])

    em.raise_error_if_run = True
    with pytest.raises (RuntimeError):
        em.rerun_experiment_pipeline (experiments=[1], run_numbers=[0])
    em.raise_error_if_run = False
    # ****************************************
    # case 2: changing parameters of prev experiment number
    # ****************************************
    em.rerun_experiment_pipeline (experiments=[1], run_numbers=[0],
                                  new_parameters={'rate': 0.04}, save_results=True)
    df = em.get_experiment_data ()
    assert np.abs(df.loc[1,mi_validation]-0.18)<1e-5 and np.abs(df.loc[1, mi_test]-0.28)<1e-5

    # ****************************************
    # case 2: re-running interrupted experiment
    # ****************************************
    # the following produces an error since halt is True in loaded parameters
    with pytest.raises (KeyboardInterrupt):
        em.rerun_experiment_pipeline (experiments=[0], run_numbers=[0])

    # ****************************************
    # case 3: adding more runs to previous experiment
    # ****************************************
    # the following produces an error since run_numbers must be a subset of those already run
    with pytest.raises (FileNotFoundError):
        em.rerun_experiment_pipeline (experiments=[1], run_numbers=list(range(5)))

    em.rerun_experiment_pipeline (experiments=[2], run_numbers=list(range(5)))
    df2 = em.get_experiment_data ()
    pd.testing.assert_frame_equal(df,df2)

    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def test_rerun_experiment_par ():
    em = init_em ('rerun_experiment_par')

    # first 3 experiments
    with pytest.raises (KeyboardInterrupt):
        _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 5},
                                          other_parameters={'halt': True})
    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.03, 'epochs': 2})
    _ = em.run_multiple_repetitions (parameters={'rate': 0.04}, nruns=5)
    df = em.get_experiment_data ()
    display (df)
    mi_validation = ('scores', 'validation_accuracy', 0)
    mi_test = ('scores', 'test_accuracy', 0)
    assert np.abs(df.loc[1,mi_validation]-0.16)<1e-5 and (df.loc[1, mi_test]-0.26)<1e-5
    #print (df.shape)
    assert df.shape==(3,28)

    # ****************************************
    # case 1: re-running finished experiment
    # ****************************************
    # the following produces an error since run_numbers must be indicated
    with pytest.raises (TypeError):
        em.rerun_experiment_par (experiments=[1])

    em.raise_error_if_run = True
    with pytest.raises (RuntimeError):
        em.rerun_experiment_par (experiments=[1], run_numbers=[0])
    em.raise_error_if_run = False
    # ****************************************
    # case 2: changing parameters of prev experiment number
    # ****************************************
    em.rerun_experiment_par (experiments=[1], run_numbers=[0],
                                  parameters={'rate': 0.04})
    df2 = em.get_experiment_data ()

    pd.testing.assert_frame_equal(df,df2)

    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def test_get_git_revision_hash ():
    global git_hash
    path_results = 'test_get_git_revision_hash'
    os.makedirs (path_results, exist_ok=True)

    # first option: git hash returned but not saved to disk
    git_hash = get_git_revision_hash ()
    assert git_hash != ''

    # second option: git hash saved to disk
    git_hash = get_git_revision_hash (path_results)
    assert os.listdir (path_results)==['git_hash.json']

    # third option: no git repo
    curdir = os.path.abspath('.')
    os.chdir ('..')
    git_hash = get_git_revision_hash (path_results)
    assert git_hash == ''

    os.chdir (curdir)
    remove_previous_results (path_results)

# Comes from experiment_manager.ipynb, cell
def test_load_or_create_experiment_values ():
    path_csv_folder = 'test_load_or_create_experiment_values'
    os.makedirs (path_csv_folder, exist_ok=True)
    parameters = dict (a='yes', b=1.2, c=True)
    experiment_number, experiment_data = load_or_create_experiment_values (path_csv_folder, parameters)
    display(experiment_data)
    assert experiment_data.shape==(1, 3)
    assert (experiment_data.columns==pd.MultiIndex.from_product (
                [[dflt.parameters_col], list(parameters.keys()), ['']])).all()
    assert experiment_data.values[0].tolist() == list(parameters.values())
    assert experiment_number==0

    parameters = dict (a='no', b=1.2, c=True)
    experiment_number, experiment_data = load_or_create_experiment_values (path_csv_folder, parameters)
    display(experiment_data)
    assert experiment_data.shape==(2, 3)
    assert (experiment_data.columns==pd.MultiIndex.from_product (
                [[dflt.parameters_col], list(parameters.keys()), ['']])).all()
    assert experiment_data.values[1].tolist() == list(parameters.values())
    assert experiment_number==1

    parameters = dict (a='no', d=12, c=True)
    experiment_number, experiment_data = load_or_create_experiment_values (path_csv_folder, parameters)
    assert experiment_data.shape==(3, 4)
    assert (experiment_data.columns==pd.MultiIndex.from_product (
                [[dflt.parameters_col], ['a','b','c','d'], ['']])).all()
    c = pd.MultiIndex.from_product (
                [[dflt.parameters_col], ['a','d','c'], ['']])
    assert experiment_data[c].values[2].tolist() == list(parameters.values())
    assert np.isnan(experiment_data.loc[2, (dflt.parameters_col, 'b', '')])
    experiment_data_before = experiment_data.copy()
    display(experiment_data)

    parameters = dict (a='no', b=1.2, c=True)
    experiment_number, experiment_data = load_or_create_experiment_values (path_csv_folder, parameters)
    assert experiment_number==1
    pd.testing.assert_frame_equal (experiment_data_before,experiment_data)

    remove_previous_results (path_csv_folder)

# Comes from experiment_manager.ipynb, cell
def test_get_experiment_numbers ():
    # get input data
    em = init_em ('get_experiment_numbers')
    parameters_single_value={'offset':0.1}
    parameters_multiple_values={'rate': [0.03,0.01], 'epochs': [5, 7]}
    em.grid_search (parameters_multiple_values=parameters_multiple_values,
                    parameters_single_value=parameters_single_value,
                    other_parameters={'verbose':False})
    df = em.get_experiment_data ()
    display(df)

    # run `get_experiment_numbers`
    experiment_numbers = get_experiment_numbers (em.path_experiments, parameters_single_value,
                                                 parameters_multiple_values)

    # check results
    assert experiment_numbers==[0, 1, 2, 3]

    # delete generated experiment data
    em.remove_previous_experiments (parent=True)

# Comes from experiment_manager.ipynb, cell
def test_get_scalar_fields ():
    d_input = {'a': None, 'b': 1.2, 'c': np.inf, 'd': np.nan, 'e': np.datetime64('NaT'), 'f': {'yes': 1},
               'g': [1, 2, 3], 'h': 'hello', 'i': np.array([1, 2, 3])}
    d_output_keys = ['a', 'b', 'c', 'd', 'e', 'h']
    assert list(get_scalar_fields (d_input).keys())==d_output_keys

# Comes from experiment_manager.ipynb, cell
@track_experiment
def my_run_experiment (self, parameters={}, path_results='./results'):
    print (parameters)
    print (path_results)
    print (self.folder)
    return {'s':10,'m':100}

@track_experiment (path_experiments='test_track_experiment/default')
def my_run_experiment2 (self, parameters={}, path_results='./results'):
    print (parameters)
    print (path_results)
    print (self.folder)
    return {'s':10,'m':100}

def test_track_experiment_decorator ():
    my_run_experiment ({'a':1,'b':2})
    my_run_experiment2 ({'a':1,'b':2})

    remove_previous_results ('results/all_runs')
    remove_previous_results ('test_track_experiment')