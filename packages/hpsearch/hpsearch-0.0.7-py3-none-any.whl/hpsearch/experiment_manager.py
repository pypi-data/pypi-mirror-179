# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/experiment_manager.ipynb (unless otherwise specified).

__all__ = ['__author__', '__copyright__', '__license__', 'ExperimentManager', 'get_git_revision_hash',
           'record_parameters', 'print_parameters', 'load_or_create_experiment_values', 'store_parameters', 'isnull',
           'get_experiment_number', 'get_experiment_numbers', 'insert_experiment_script_path', 'load_parameters',
           'get_scalar_fields', 'save_other_parameters', 'DecoratorExperimentManager', 'track_experiment']

# Cell
__author__ = "Jaume Amores"
__copyright__ = "Copyright 2021, Johnson Controls"
__license__ = "MIT"

# Cell
# coding: utf-8
import joblib
import sys
import os
import numpy as np
import pandas as pd
import time
import datetime
from sklearn.model_selection import ParameterGrid
from sklearn.utils import Bunch
import platform
import pprint
import subprocess
from multiprocessing import Process
import logging
import traceback
import shutil
from pathlib import Path
from functools import partial, wraps

from dsblocks.utils.utils import (set_logger, set_verbosity, store_attr, json_load, json_dump,
                                  add_commit_files)

# hpsearch core API
from .config.manager_factory import ManagerFactory
from .utils import experiment_utils
from .utils.experiment_utils import (remove_defaults, read_df, write_df,
                                             write_binary_df_if_not_exists)
from .utils.convert_legacy import update_data_format, convert_run_numbers
from .utils.organize_experiments import remove_defaults_from_experiment_data
import hpsearch.config.hp_defaults as dflt

# Cell
class ExperimentManager (object):
    def __init__ (self,
                  allow_base_class=dflt.allow_base_class,
                  path_experiments='hpsearch/results',
                  folder=None,
                  parent_path=None,
                  alternative_path=None,
                  alternative_parent_path=None,
                  defaults=dflt.defaults,
                  metric=dflt.metric,
                  op=dflt.op,
                  backend=dflt.backend,
                  path_data=None,
                  name_model_history=dflt.name_model_history,
                  model_file_name=dflt.model_file_name,
                  name_epoch=dflt.name_epoch,
                  result_file=dflt.result_file,
                  target_model_file=None,
                  destination_model_file=None,
                  manager_path=dflt.manager_path,
                  non_pickable_fields=[],
                  avoid_saving_fields=[],
                  do_git_tracking=False,
                  run_experiment=None,
                  logger=None,
                  verbose: int = dflt.verbose,
                  name_logger:str = dflt.name_logger,
                  **kwargs):

        # ********************
        # store_attr ()
        # ********************
        self.allow_base_class = allow_base_class
        self._path_experiments = path_experiments
        self.defaults = defaults
        self.key_score = metric
        self.op = op
        self.backend = backend
        self._alternative_path = alternative_path
        self.path_data = path_data
        self.name_model_history = name_model_history
        self.model_file_name = model_file_name
        self.name_epoch = name_epoch
        self.result_file = result_file
        self.target_model_file = target_model_file
        self.destination_model_file = destination_model_file
        self.name_logger = name_logger
        self.logger = logger
        self.verbose = verbose
        self.manager_path = manager_path
        self.do_git_tracking = do_git_tracking
        # ********************

        class_name = self.__class__.__name__

        self._path_experiments = Path (self._path_experiments).resolve ()
        if folder is not None or parent_path is not None:
            self.set_path_experiments (folder=folder, parent_path=parent_path)
        self._alternative_path = Path(self._alternative_path) if self._alternative_path is not None else None
        if alternative_path is None and alternative_parent_path is not None:
            self.set_alternative_path (alternative_parent_path=alternative_parent_path)
        self.path_data = Path(self.path_data) if self.path_data is not None else None

        if self.logger is None:
            self.logger = set_logger (self.name_logger, path_results=self.path_experiments, verbose=self.verbose)

        self.key_score = metric
        self.registered_name = f'{class_name}-{self.folder}'

        self.parameters_non_pickable = {}
        self.manager_factory = ManagerFactory(allow_base_class=allow_base_class, manager_path=self.manager_path,
                                              logger=self.logger)
        self.manager_factory.register_manager (self)
        non_pickable_fields = (non_pickable_fields if isinstance(non_pickable_fields, list)
                               else [non_pickable_fields])
        avoid_saving_fields = (avoid_saving_fields if isinstance(avoid_saving_fields, list)
                               else [avoid_saving_fields])
        self.non_pickable_fields = (non_pickable_fields + avoid_saving_fields +
                                    ['manager_factory', 'parameters_non_pickable', 'logger'])
        self.avoid_saving_fields = avoid_saving_fields

        if run_experiment is not None: self.run_experiment = partial (run_experiment, self)

    @property
    def folder (self):
        return self._path_experiments.name

    @property
    def parent_path (self):
        return self._path_experiments.parent

    @property
    def path_experiments (self):
        return self._path_experiments

    @property
    def alternative_parent_path (self):
        return self._alternative_path.parent

    @property
    def alternative_path (self):
        return self._alternative_path

    def set_path_experiments (self, path_experiments=None, folder=None, parent_path=None):
        if path_experiments is not None: self._path_experiments = Path(path_experiments).resolve()
        else:
            parent_path = Path(parent_path).resolve() if parent_path is not None else self.parent_path
            folder = folder if folder is not None else self.folder
            self._path_experiments = parent_path/folder

    def set_alternative_path (self, alternative_path=None, alternative_parent_path=None):
        if alternative_path is not None: self._alternative_path = Path(alternative_path).resolve()
        else:
            alternative_parent_path = (Path(alternative_parent_path).resolve()
                                       if alternative_parent_path is not None else
                                       self.alternative_parent_path)
            folder = self.folder
            self._alternative_path = alternative_parent_path/folder

    def set_verbose (self, verbose):
        self.verbose = verbose
        set_verbosity (logger=self.logger, verbose=verbose)

    def get_default_parameters (self, parameters):
        if not self.allow_base_class:
            raise ImportError ('call get_default_parameters from base class is not allowed')
        return self.defaults

    def get_default_operations (self):
        return {'folder': self.folder, 'op': self.op, 'metric': self.key_score}

    def get_path_experiment (self, experiment_id):
        path_experiment = self.path_experiments/f'experiments/{experiment_id:05d}'
        return path_experiment

    def get_path_results (self, experiment_id=None, run_number=0, path_experiment=None):
        assert experiment_id is not None or path_experiment is not None
        if path_experiment is None:
            path_experiment = self.get_path_experiment (experiment_id)
        path_results = path_experiment/f'{run_number}'
        return path_results

    def get_path_alternative (self, path_results):
        if self.alternative_path is None:
            return path_results
        path_alternative = str(path_results).replace (str(self.path_experiments), str(self.alternative_path))

        return path_alternative

    def get_path_data (self, run_number, parameters={}):
        if self.path_data is None:
            return self.path_experiments/'data'
        else:
            return self.path_data

    def get_experiment_data (self, experiments=None):
        experiment_data = read_df (self.path_experiments)
        if experiment_data is not None:
            if experiment_data.columns.nlevels==1: experiment_data = update_data_format (experiment_data)
            if dflt.scores_col in experiment_data.columns:
                types = list (map (type, experiment_data[dflt.scores_col].columns.get_level_values (1)))
                if not all([x==int for x in types]): experiment_data = convert_run_numbers (experiment_data)
            if experiments is not None: experiment_data = experiment_data.loc[experiments,:]
        return experiment_data

    def remove_previous_experiments (self, parent=False, only_test=True, include_alternative=True,
                                     use_alternative_path=False):
        if use_alternative_path:
            path_to_remove = self.alternative_path.parent if parent else self.alternative_path
        else:
            path_to_remove = self.path_experiments.parent if parent else self.path_experiments
        if not str(path_to_remove.name).startswith ('test_') and only_test:
            raise ValueError (f'path to remove does not start with test_: {path_to_remove}')
        if path_to_remove.exists():
            shutil.rmtree (path_to_remove)
        if include_alternative and self.alternative_path is not None:
            self.remove_previous_experiments (parent=parent,
                                              only_test=only_test,
                                              use_alternative_path=True,
                                              include_alternative=False)

    def experiment_visualization (self, **kwargs):
        raise ValueError ('this type of experiment visualization is not recognized')

    def run_experiment_pipeline (self, run_number=0, path_results='./results', parameters = {},
                                 use_process=False):
        """ Runs complete learning pipeline: loading / generating data, building and learning model, applying it to data,
        and evaluating it."""
        start_time = time.time()

        # record all parameters except for non-pickable ones
        record_parameters (path_results, parameters)

        # integrate non-pickable parameters into global dictionary
        parameters.update (self.parameters_non_pickable)
        self.parameters_non_pickable = {}

        # #####################################
        # Evaluation
        # #####################################
        time_before = time.time()
        score_dict = self._run_experiment (parameters=parameters, path_results=path_results,
                                           run_number=run_number, use_process=use_process)
        self.logger.info ('time spent on this experiment: {}'.format(time.time()-time_before))

        # #####################################
        # Final scores
        # #####################################
        score_name = self.key_score
        if len(score_name) > 0:
            if score_name[0] == '_':
                score_name = score_name[1:]
            if score_dict.get(score_name) is not None:
                self.logger.info (f'score: {score_dict.get(score_name)}')

        spent_time = time.time() - time_before

        return score_dict, spent_time

    # *************************************************************************
    #   run_experiment methods
    # *************************************************************************
    def _run_experiment (self, parameters={}, path_results='./results', run_number=None, use_process=False):

        parameters['run_number'] = run_number

        # wrap parameters
        parameters = Bunch(**parameters)

        if use_process:
            return self.run_experiment_in_separate_process (parameters, path_results)
        else:
            return self.run_experiment (parameters=parameters, path_results=path_results)

    def run_experiment_in_separate_process (self, parameters={}, path_results='./results'):

        parameters['return_results']=False
        p = Process(target=self.run_experiment_saving_results, args=(parameters, path_results))
        p.start()
        p.join()
        if p.exitcode != 0:
            self.logger.warning ('process exited with non-zero code: there might be an error '
                                 'in run_pipeline function')

        path_dict_results = f'{path_results}/dict_results.pk'
        try:
            dict_results = joblib.load (path_dict_results)
        except FileNotFoundError:
            raise RuntimeError (f'{path_dict_results} not found: probably there is an error in run_pipeline'
                                'function. Please run in debug mode, without multi-processing')

        return dict_results

    def run_experiment_saving_results (self, parameters={}, path_results='./results'):
        dict_results = self.run_experiment (parameters=parameters, path_results=path_results)
        joblib.dump (dict_results, '%s/dict_results.pk' %path_results)

    def run_experiment (self, parameters={}, path_results='./results'):
        raise NotImplementedError ('This method needs to be defined in subclass')


    # *************************************************************************
    # *************************************************************************
    def create_experiment_and_run (self, parameters={}, other_parameters={},
                                   info=Bunch(), em_args=Bunch(),
                                   run_number=0, log_message=None, stack_level=-3,
                                   precision=1e-15, experiment_number=None,
                                   repeat_experiment=False, remove_not_finished=False,
                                   only_remove_not_finished=False, check_finished=False,
                                   recompute_metrics=False, force_recompute_metrics=False,
                                   check_finished_if_interrupted=False, prev_epoch=False,
                                   use_previous_best=dflt.use_previous_best, from_exp=None,
                                   skip_interrupted=False, use_last_result=False,
                                   run_if_not_interrumpted=False, use_last_result_from_dict=False,
                                   previous_model_file_name=None, model_extension='h5',
                                   model_name='checkpoint_', epoch_offset=0, name_best_model='best_model',
                                   name_last_epoch=dflt.name_last_epoch, min_iterations=dflt.min_iterations,
                                   use_process=False, do_git_tracking=None, include_model_history=True,
                                   add_pipeline=False):

        """
        Log experiment parameters, run experiment if not run before, and log results.

        Can resume previously run experiment.
        """
        current_em_args = Bunch ()
        store_attr (store_args=False, self=current_em_args, but='parameters, other_parameters, info, em_args')
        em_args.update (current_em_args)
        requested_experiment_number = experiment_number
        # ****************************************************
        #  preliminary set-up: logger and path_experiments
        # ****************************************************
        if log_message is not None:
            self.logger.info ('**************************************************')
            self.logger.info (log_message)
            self.logger.info ('**************************************************')

        # insert path to experiment script file that called the experiment manager
        insert_experiment_script_path (info, self.logger, stack_level=stack_level)

        # create directories
        path_experiments = self.path_experiments
        path_experiments.mkdir (parents=True, exist_ok=True)

        # ****************************************************
        # register (subclassed) manager so that it can be used by decoupled modules
        # ****************************************************
        self.register_and_store_subclassed_manager ()

        # ****************************************************
        #   get experiment number given parameters
        # ****************************************************
        original_parameters = parameters
        parameters = get_scalar_fields (parameters)
        if set(parameters.keys()) != set(original_parameters.keys()):
            diff_parameters = {k:original_parameters[k] for k in
                               set(original_parameters.keys()).difference(parameters.keys())}
            self.logger.warning ('The following parameters were not considered for tracking: \n'
                                 f'{diff_parameters}')
        parameters = remove_defaults (parameters)

        experiment_number, experiment_data = load_or_create_experiment_values (
            path_experiments, parameters, precision=precision)

        # if old experiment, we can require that given parameters match with experiment number
        if (requested_experiment_number is not None
            and experiment_number != requested_experiment_number):
            raise ValueError (f'expected number: {requested_experiment_number}, '
                              f'found: {experiment_number}')
        other_parameters['experiment_number'] = experiment_number

        # ****************************************************
        # get key_score
        # ****************************************************
        key_score = self.key_score
        # if key_score not found until now, we change it
        if (dflt.scores_col in experiment_data.columns.get_level_values(0) and
            key_score not in experiment_data[dflt.scores_col].columns.get_level_values(0) and
            len(experiment_data[dflt.scores_col].columns.get_level_values(0)) > 0):
            key_score = experiment_data[dflt.scores_col].columns.get_level_values(0)[0]

        # ****************************************************
        #   get run_id, if not given
        # ****************************************************
        if run_number is None:
            run_number = 0
            mi_score = (dflt.scores_col, key_score, run_number)
            while not isnull(experiment_data, experiment_number, mi_score):
                score = experiment_data.loc[experiment_number, mi_score]
                self.logger.info (f'found previous run for experiment number {experiment_number}, '
                                  f'run {run_number}, with score {key_score} = {score}')
                run_number += 1
                mi_score = (dflt.scores_col, key_score, run_number)
            self.logger.info (f'starting experiment {experiment_number} with run number {run_number}')

        else:
            mi_score = (dflt.scores_col, key_score, run_number)
            if not isnull(experiment_data, experiment_number, mi_score):
                previous_result = experiment_data.loc[experiment_number, mi_score]
                self.logger.info (f'found completed: experiment number: {experiment_number}, '
                                  f'run number: {run_number} - score: {previous_result}')
                self.logger.info (parameters)
                if repeat_experiment:
                    self.logger.info ('redoing experiment')

        # ****************************************************
        #   remove unfinished experiments
        # ****************************************************
        if remove_not_finished:
            name_finished = (dflt.run_info_col, 'finished', run_number)
            if not isnull(experiment_data, experiment_number, name_finished):
                finished = experiment_data.loc[experiment_number, name_finished]
                self.logger.info (f'experiment {experiment_number}, run number {run_number}, finished {finished}')
                if not finished:
                    experiment_data.loc[experiment_number, mi_score] = None
                    write_df (experiment_data, path_experiments)
                    self.logger.info (f'removed experiment {experiment_number}, '
                                 f'run number {run_number}, finished {finished}')
            if only_remove_not_finished:
                return None, {}

        unfinished_flag = False
        name_epoch = self.name_epoch
        current_path_results = self.get_path_results (experiment_number, run_number=run_number)

        # ****************************************************
        #   check conditions for skipping experiment
        # ****************************************************
        if not isnull(experiment_data, experiment_number, mi_score) and not repeat_experiment:
            if (check_finished
                and not self.finished_all_epochs (parameters, current_path_results)):
                unfinished_flag = True
            else:
                self.logger.info ('skipping...')
                return previous_result, {key_score: previous_result}
        elif (isnull(experiment_data, experiment_number, mi_score)
              and recompute_metrics
              and not force_recompute_metrics):
            self.logger.info (f'experiment not found, skipping {run_number} due to only recompute_metrics')
            return None, {}

        # ****************************************************
        # log info
        # ****************************************************
        self.logger.info ('running experiment %d' %experiment_number)
        self.logger.info ('run number: %d' %run_number)
        self.logger.info ('\nparameters:\n%s' %print_parameters(parameters))

        # ****************************************************
        #  get paths
        # ****************************************************
        # path_experiment folder
        path_experiment = self.get_path_experiment (experiment_number)
        path_experiment.mkdir (parents=True, exist_ok=True)

        # path_results folder (where results are)
        path_results = self.get_path_results (run_number=run_number, path_experiment=path_experiment)
        os.makedirs (path_results, exist_ok=True)

        # path to save big files
        path_results_big_size = self.get_path_alternative (path_results)
        os.makedirs (path_results_big_size, exist_ok = True)
        other_parameters['path_results_big'] = path_results_big_size

        # ****************************************************
        # get git and record parameters
        # ****************************************************
        # get git revision number
        info['git_hash'] = get_git_revision_hash(path_experiments)

        # write parameters in experiment folder
        record_parameters (path_experiment, parameters, other_parameters, em_args, info, self.__dict__)

        # store hyper_parameters in dictionary that maps experiment_number with hyper_parameter values
        store_parameters (path_experiments, experiment_number, parameters)

        # ****************************************************************
        # loggers
        # ****************************************************************
        logger_experiment = set_logger ("experiment", path_results, verbose=self.verbose)
        logger_experiment.info (f'script: {info["script_path"]}, line number: {info["lineno"]}')
        if os.path.exists(info['script_path']):
            shutil.copy (info['script_path'], path_results)
            shutil.copy (info['script_path'], path_experiment)

        # summary logger
        logger_summary = set_logger ("summary", path_experiments, mode='w', stdout=False, just_message=True,
                                     filename='summary.txt', verbose=self.verbose,
                                     verbose_out=self.verbose)
        logger_summary.info (f'\n\n{"*"*100}\nexperiment: {experiment_number}, run: {run_number}\n'
                             f'script: {info["script_path"]}, line number: {info["lineno"]}\n'
                             f'parameters:\n{print_parameters(parameters)}{"*"*100}')
        if info.get('rerun_script') is not None:
            logger_summary.info ('\nre-run:\n{}'.format(info['rerun_script']))
        # same file in path_results
        logger_summary2 = set_logger ("summary", path_results, mode='w', stdout=False,
                                      just_message=True, filename='summary.txt', verbose=self.verbose,
                                      verbose_out=self.verbose)
        logger_summary2.info (f'\n\n{"*"*100}\nexperiment: {experiment_number}, run: {run_number}\nscript: '
                              f'{info["script_path"]}, line number: {info["lineno"]}\n'
                              f'parameters:\n{print_parameters(parameters)}{"*"*100}')

        # ****************************************************************
        # Do final adjustments to parameters
        # ****************************************************************
        parameters = parameters.copy()
        original_parameters = parameters.copy()
        parameters.update(other_parameters)

        # add default parameters - their values are overwritten by input values, if given
        defaults = self.get_default_parameters(parameters)
        parameters_with_defaults = defaults.copy()
        parameters_with_defaults.update(parameters)
        parameters = parameters_with_defaults

        # indicate files to track with git
        self.set_git_tracking_parameters (parameters, experiment_number, run_number,
                                          do_git_tracking=do_git_tracking,
                                          include_model_history=include_model_history,
                                          add_pipeline=add_pipeline)

        # ***********************************************************
        # resume from previous experiment
        # ***********************************************************
        if (isnull(experiment_data, experiment_number, mi_score)
            and check_finished_if_interrupted
            and not self.finished_all_epochs (parameters, current_path_results)):
            unfinished_flag = True

        resuming_from_prev_epoch_flag = False
        if prev_epoch:
            self.logger.info('trying prev_epoch')
            experiment_data2 = experiment_data.copy()
            if (not unfinished_flag
                and (repeat_experiment or isnull(experiment_data, experiment_number, mi_score))):
                    experiment_data2 = experiment_data2.drop(experiment_number,axis=0)
            prev_experiment_number = self.find_closest_epoch (experiment_data2, original_parameters)
            if prev_experiment_number is not None:
                self.logger.info(f'using prev_epoch: {prev_experiment_number}')
                prev_path_results = self.get_path_results (prev_experiment_number,
                                                           run_number=run_number)
                found = self.make_resume_from_checkpoint (parameters, prev_path_results)
                if found:
                    self.logger.info (f'found previous exp: {prev_experiment_number}')
                    if prev_experiment_number == experiment_number:
                        other_parameters['use_previous_best'] = use_previous_best
                        if not use_previous_best and unfinished_flag:
                            prev_epoch = self.get_last_epoch (parameters,
                                                              current_path_results)
                            prev_epoch = max (int(prev_epoch), 0)
                            parameters[name_epoch] = parameters[name_epoch] - prev_epoch
                        self.logger.info ('using previous best')
                    else:
                        mi_epoch = (dflt.parameters_col, name_epoch, '')
                        prev_epoch = experiment_data.loc[prev_experiment_number,mi_epoch]
                        prev_epoch = (int(prev_epoch) if prev_epoch is not None
                                      else defaults.get(name_epoch))
                        parameters[name_epoch] = parameters[name_epoch] - prev_epoch

                resuming_from_prev_epoch_flag = found


        if not resuming_from_prev_epoch_flag and from_exp is not None:
            prev_experiment_number = from_exp
            self.logger.info('using previous experiment %d' %prev_experiment_number)
            prev_path_results = self.get_path_results (prev_experiment_number, run_number=run_number)
            self.make_resume_from_checkpoint (parameters, prev_path_results, use_best=True,
                                              previous_model_file_name=previous_model_file_name,
                                              model_extension=model_extension, model_name=model_name,
                                              epoch_offset=epoch_offset, name_best_model=name_best_model,
                                              name_last_epoch=name_last_epoch)

        # ****************************************************************
        #   Analyze if experiment was interrupted
        # ****************************************************************
        if skip_interrupted:
            was_interrumpted = self.exists_current_checkpoint (parameters, path_results)
            was_interrumpted = (was_interrumpted or
                                self.obtain_last_result (
                                    parameters, path_results,
                                    use_last_result_from_dict=use_last_result_from_dict,
                                    min_iterations=min_iterations) is not None)
            if was_interrumpted:
                self.logger.info ('found intermediate results, skipping...')
                return None, {}

        # ****************************************************************
        # retrieve last results in interrupted experiments
        # ****************************************************************
        run_pipeline = True
        if use_last_result:
            dict_results = self.obtain_last_result (
                parameters, path_results, use_last_result_from_dict=use_last_result_from_dict,
                min_iterations=min_iterations)
            if dict_results is None and run_if_not_interrumpted:
                run_pipeline = True
            elif dict_results is None:
                return None, {}
            else:
                run_pipeline = False

        # ****************************************************************
        # run experiment
        # ****************************************************************
        if run_pipeline:
            dict_results, time_spent = self.run_experiment_pipeline (run_number, path_results,
                                                                          parameters=parameters,
                                                                          use_process=use_process)
            finished = True
        else:
            finished = False
            time_spent = None

        # ****************************************************************
        #  Retrieve and store results
        # ****************************************************************
        self.log_results (dict_results, experiment_data, experiment_number, run_number, time_spent,
                          finished=finished)

        try:
            save_other_parameters (experiment_number, {**other_parameters, **em_args, **info}, path_experiments)
        except Exception as e:
            print (f'error saving other parameters: {e}')

        logger_summary2.info ('\nresults:\n{}'.format(dict_results))
        self.logger.info ('finished experiment %d' %experiment_number)

        self.commit_results (experiment_number, run_number, do_git_tracking=do_git_tracking,
                     include_model_history=include_model_history)


        # return final score
        result = dict_results.get(key_score)
        return result, dict_results

    def log_results (self, dict_results, experiment_data, experiment_number, run_number,
                time_spent, finished=True, str_run_number=False):
        if not isinstance(dict_results, dict): dict_results = {self.key_score: dict_results}
        if str_run_number:
            experiment_data.columns = pd.MultiIndex.from_tuples(
                [(c[0],c[1],str(c[2])) for c in experiment_data.columns])
            run_number = str(run_number)
        columns = pd.MultiIndex.from_product ([[dflt.scores_col],
                                               list(dict_results.keys()),
                                               [run_number]])
        experiment_data[[x for x in columns if x not in experiment_data]] = None
        experiment_data.loc[experiment_number, columns]=dict_results.values()

        mi_col = (dflt.run_info_col, 'time', run_number)
        if isnull(experiment_data, experiment_number,  mi_col) and finished:
            experiment_data.loc[experiment_number, mi_col]=time_spent
        mi_col = (dflt.run_info_col, 'date', run_number)
        experiment_data.loc[experiment_number, mi_col]=datetime.datetime.time(datetime.datetime.now())
        mi_col = (dflt.run_info_col, 'finished', run_number)
        experiment_data.loc[experiment_number, mi_col]=finished

        try:
            experiment_data = experiment_data[experiment_data.columns.sort_values()]
        except TypeError as e:
            self.logger.warning (f'received exception {e}, we cannot sort the columns')
        write_df (experiment_data, self.path_experiments)

    def grid_search (self, parameters_multiple_values={}, parameters_single_value={}, other_parameters={},
                     info=Bunch(), run_numbers=[0], random_search=False, load_previous=False,
                     log_message='', nruns=None, keep='multiple', **kwargs):

        other_parameters = other_parameters.copy()

        os.environ['TF_CPP_MIN_LOG_LEVEL']='2'
        if nruns is not None:
            run_numbers = range (nruns)

        path_experiments = self.path_experiments
        path_results_base = path_experiments

        os.makedirs (path_results_base,exist_ok=True)

        if keep=='multiple':
            parameters_single_value = {k:parameters_single_value[k]
                                       for k in parameters_single_value.keys()
                                       if k not in parameters_multiple_values}
        elif keep=='single':
            parameters_multiple_values = {k:parameters_multiple_values[k]
                                          for k in parameters_multiple_values.keys()
                                          if k not in parameters_single_value}
        else:
            raise ValueError (f'parameter keep {keep} not recognized: it must be either multiple or single')

        parameters_multiple_values_all = parameters_multiple_values
        parameters_multiple_values_all = list(ParameterGrid(parameters_multiple_values_all))

        if log_message != '':
            info['log_message'] = log_message
        insert_experiment_script_path (info, self.logger)

        if random_search:
            path_random_hp = '%s/random_hp.pk' %path_results_base
            if load_previous and os.path.exists(path_random_hp):
                parameters_multiple_values_all = joblib.load (path_random_hp)
            else:
                parameters_multiple_values_all = list (np.random.permutation(parameters_multiple_values_all))
                joblib.dump (parameters_multiple_values_all, path_random_hp)
        for (i_hp, parameters_multiple_values) in enumerate (parameters_multiple_values_all):
            parameters = parameters_multiple_values.copy()
            parameters.update(parameters_single_value)

            for (i_run, run_number) in enumerate (run_numbers):
                self.logger.info (f'processing hyper-parameter {i_hp+1} '
                                 f'out of {len(parameters_multiple_values_all)}')
                self.logger.info (f'doing run {i_run+1} out of {len(run_numbers)}')
                self.logger.info (log_message)

                self.create_experiment_and_run (parameters=parameters, other_parameters=other_parameters,
                                                info=info, run_number=run_number, **kwargs)

        # This solves an intermitent issue found in TensorFlow (reported as bug by community)
        import gc
        gc.collect()

    def run_multiple_repetitions (self, parameters={}, other_parameters={},
                     log_message='', nruns=None, run_numbers=[0], **kwargs):

        other_parameters = other_parameters.copy()

        if nruns is not None: run_numbers = range (nruns)

        path_experiments = self.path_experiments
        path_experiments.mkdir (parents=True, exist_ok = True)

        results = np.zeros((len(run_numbers),))
        for (i_run, run_number) in enumerate(run_numbers):
                self.logger.info('doing run %d out of %d' %(i_run+1, len(run_numbers)))
                self.logger.info('%s' %log_message)

                results[i_run], dict_results  = self.create_experiment_and_run (
                    parameters=parameters, other_parameters=other_parameters,
                    run_number=run_number, **kwargs)
                if dict_results.get('is_pruned', False):
                    break

        mu, std = results.mean(), results.std()
        self.logger.info (f'mean {self.key_score}: {mu}, std: {std}')

        dict_results[self.key_score] = mu

        return mu, std, dict_results

    def hp_optimization (self, parameter_sampler=None, log_message=None,
                         parameters={}, other_parameters={}, info=Bunch(),
                         nruns=None, stack_level=-3, sampler_method='random',
                         pruner_method='none', n_evaluations=20, seed=0,
                         n_startup_trials=5, n_startup_trials_pruner=5,
                         n_trials=10, study_name='hp_study',
                         run_number=None, n_jobs=1, nruns_best=0,
                         n_warmup_steps=None, **kwargs):

        import optuna
        from optuna.pruners import SuccessiveHalvingPruner, MedianPruner
        from optuna.samplers import RandomSampler, TPESampler
        from optuna.integration.skopt import SkoptSampler

        optuna.logging.disable_propagation()

        em_args = Bunch ()
        store_attr (store_args=False, self=em_args, but='parameters, other_parameters')

        path_experiments = self.path_experiments
        path_experiments.mkdir (parents=True, exist_ok=True)

        other_parameters = other_parameters.copy()

        if log_message != '':
            info['log_message'] = log_message
        insert_experiment_script_path (info, self.logger, stack_level=stack_level)

        if sampler_method == 'random':
            sampler = RandomSampler(seed=seed)
        elif sampler_method == 'tpe':
            sampler = TPESampler(n_startup_trials=n_startup_trials, seed=seed)
        elif sampler_method == 'skopt':
            # cf https://scikit-optimize.github.io/#skopt.Optimizer
            # GP: gaussian process
            # Gradient boosted regression: GBRT
            sampler = SkoptSampler(skopt_kwargs={'base_estimator': "GP", 'acq_func': 'gp_hedge'})
        else:
            raise ValueError('Unknown sampler: {}'.format(sampler_method))

        if pruner_method == 'halving':
            pruner = SuccessiveHalvingPruner(min_resource=1, reduction_factor=4, min_early_stopping_rate=0)
        elif pruner_method == 'median':
            # n_warmup_steps: Disable pruner until the trial reaches the given number of step.
            if n_warmup_steps is None: n_warmup_steps = n_evaluations // 3
            pruner = MedianPruner(n_startup_trials=n_startup_trials_pruner, n_warmup_steps=n_warmup_steps)
        elif pruner_method == 'none': # Do not prune
            pruner = optuna.pruners.NopPruner ()
        else:
            raise ValueError(f'Unknown pruner: {pruner_method}')

        self.logger.info (f'Sampler: {sampler_method} - Pruner: {pruner_method}')

        #study = optuna.create_study(sampler=sampler, pruner=pruner)
        direction = 'maximize' if self.op=='max' else 'minimize'
        study = optuna.create_study(direction=direction,
                                    study_name=study_name,
                                    storage=f'sqlite:///{path_experiments}/{study_name}.db',
                                    sampler=sampler, pruner=pruner, load_if_exists=True)

        key_score = self.key_score

        def objective(trial):

            hp_parameters = parameters.copy()
            self.parameters_non_pickable = dict(trial=trial)

            if parameter_sampler is not None:
                hp_parameters.update(parameter_sampler(trial))

            if nruns is None or nruns < 2:
                _, dict_results = self.create_experiment_and_run (
                    parameters=hp_parameters, other_parameters=other_parameters,
                    run_number=run_number, info=info, em_args=em_args, **kwargs
                )
            else:
                mu_best, std_best, dict_results = self.run_multiple_repetitions (
                    parameters=hp_parameters, other_parameters=other_parameters,
                    info=info, em_args=em_args, nruns=nruns, **kwargs
                )

            if dict_results.get('is_pruned', False):
                raise optuna.structs.TrialPruned()

            assert key_score in dict_results, f'metric {key_score} not found in results'

            return dict_results[key_score]

        study.optimize(objective, n_trials=n_trials, n_jobs=n_jobs)

        self.logger.info ('Number of finished trials: {}'.format(len(study.trials)))
        self.logger.info ('Best trial:')
        trial = study.best_trial
        self.logger.info ('Value: {}'.format(trial.value))
        self.logger.info ('best params: {}'.format (study.best_params))
        best_value = trial.value

        if nruns_best > 0:
            self.logger.info ('running best configuration %d times' %nruns_best)
            parameters.update (study.best_params)
            mu_best, std_best, _ = self.run_multiple_repetitions (
                parameters=parameters, other_parameters=other_parameters,
                nruns=nruns_best, info=info, em_args=em_args, **kwargs)
            best_value = mu_best

        return best_value

    def greedy_search (self, run_numbers=[0], nruns=None, other_parameters={},
                       em_args={}, parameters_greedy={},
                       parameters_single_value={},
                       parameters_multiple_values={}, log_message='',
                       only_if_exists=False, check_experiment_matches=True,
                       query_args={}, info=Bunch(), n_iterations=1, **kwargs):

        from .tools.query import query

        em_args=kwargs
        parameters_multiple_values_original = parameters_multiple_values.copy()
        script_parameters = {}
        insert_experiment_script_path (script_parameters, self.logger)
        info['rerun_script'] = script_parameters
        em_args['info'] = info
        parameters_greedy_list = (parameters_greedy if isinstance (parameters_greedy, list)
                                  else [{k:parameters_greedy[k]} for k in parameters_greedy])
        for iteration in range(n_iterations):
            for parameters_greedy in parameters_greedy_list:
                parameters_multiple_values = parameters_multiple_values_original.copy()
                parameters_multiple_values.update (parameters_greedy)

                df = query (folder=self.folder, metric=self.key_score, op=self.op, stats=['mean'], **query_args)
                if df is None or df.empty:
                    message = 'starting set of experiments with grid-search'
                    print (message); self.logger.info (message)
                    self.grid_search (parameters_multiple_values=parameters_multiple_values,
                                      parameters_single_value=parameters_single_value,
                                      other_parameters=other_parameters,
                                      run_numbers=run_numbers, log_message=log_message,
                                      nruns=nruns, **em_args)
                else:
                    experiment_number = df.index[0]
                    dict_par = df.loc[experiment_number, dflt.parameters_col].to_dict()
                    dict_par = {k[0]:dict_par[k] for k in dict_par}
                    message = '- Continuing search starting from previous best\n'
                    message += (f'    Exp#={experiment_number}, '
                                f'Parameters={dict_par}\n')
                    message += (f'    Best metric {self.key_score}: '
                                f'{df.loc[experiment_number, (dflt.stats_col, self.key_score)].values}\n')
                    message += (f'- Starting new round with variable parameters: {parameters_multiple_values}\n'
                                f'And fixed parameters: {parameters_single_value}')
                    print (message); self.logger.info (message)
                    self.rerun_experiment (experiments=[experiment_number], nruns=nruns,
                                           other_parameters=other_parameters,
                                           parameters=parameters_single_value,
                                           parameters_multiple_values=parameters_multiple_values,
                                           log_message=log_message, only_if_exists=only_if_exists,
                                           check_experiment_matches=check_experiment_matches, **em_args)

    def rerun_experiment (self, experiments=[], run_numbers=[0], nruns=None,
                          other_parameters={}, em_args={}, parameters={},
                          parameter_sampler=None, parameters_multiple_values=None,
                          log_message='', only_if_exists=False, check_experiment_matches=True,
                          info=Bunch (), **kwargs):

        other_parameters = other_parameters.copy()
        em_args = kwargs
        path_experiments = self.path_experiments

        if nruns is not None:
            run_numbers = range (nruns)

        parameters_original = parameters
        other_parameters_original = other_parameters
        em_args_original = em_args
        for experiment_id in experiments:
            path_experiment = self.get_path_experiment (experiment_id)
            check_experiment_matches = (check_experiment_matches and
                                        parameters_multiple_values is None
                                        and parameter_sampler is None)
            parameters, other_parameters, em_args = load_parameters (em=self,
                experiment=experiment_id,
                other_parameters=other_parameters_original, em_args=em_args_original,
                parameters=parameters_original, check_experiment_matches=check_experiment_matches
            )

            if 'log_message' in em_args:
                info['old_log_message'] = em_args['log_message']
                del em_args['log_message']
            if 'run_number' in em_args:
                info['old_run_number'] = em_args['run_number']
                del em_args['run_number']

            # we need to set the following flag to False, since otherwise when we request to store the intermediate results
            # and the experiment did not start, we do not run the experiment
            if (em_args.get('use_last_result', False)
                and not em_args_original.get('use_last_result', False)):
                self.logger.debug ('changing other_parameters["use_last_result"] to False')
                em_args['use_last_result'] = False
            self.logger.info (f'running experiment {experiment_id} with parameters:\n{parameters}\n'
                         f'other_parameters:\n{other_parameters}')

            if parameter_sampler is not None:
                self.logger.info ('running hp_optimization')
                if 'parameter_sampler' in em_args:
                    info['old_parameter_sampler'] = em_args['parameter_sampler']
                    del em_args['parameter_sampler']
                insert_experiment_script_path (info, self.logger)
                em_args['info'] = info
                self.hp_optimization (parameter_sampler=parameter_sampler,
                                      log_message=log_message, parameters=parameters,
                                      other_parameters=other_parameters, **em_args)
            elif parameters_multiple_values is not None:
                script_parameters = {}
                insert_experiment_script_path (script_parameters, self.logger)
                info['rerun_script'] = script_parameters
                em_args['info'] = info
                self.grid_search (
                    parameters_multiple_values=parameters_multiple_values,
                    parameters_single_value=parameters, other_parameters=other_parameters,
                    run_numbers=run_numbers, log_message=log_message, **em_args)
            else:
                if only_if_exists:
                    run_numbers = [run_number for run_number in run_numbers
                                   if (path_experiment/run_number).exists()]

                script_parameters = {}
                insert_experiment_script_path (script_parameters, self.logger)
                info['rerun_script'] = script_parameters
                em_args['info'] = info
                self.run_multiple_repetitions (
                    parameters=parameters, other_parameters=other_parameters,
                    log_message=log_message, run_numbers=run_numbers, **em_args
                )

    def rerun_experiment_pipeline (self, experiments, run_numbers=None,
                                   new_parameters={}, save_results=False):

        path_experiments = self.path_experiments
        for experiment_id in experiments:
            path_experiment = self.get_path_experiment (experiment_id)

            parameters, other_parameters, em_args, info, em_attrs = joblib.load (
                f'{path_experiment}/parameters.pk'
            )
            parameters = parameters.copy()
            parameters.update(other_parameters)
            parameters.update(new_parameters)
            for run_number in run_numbers:
                path_results = path_experiment/f'{run_number}'
                path_data = self.get_path_data (run_number, parameters)
                dict_results, time_spent = self.run_experiment_pipeline (run_number, path_results,
                                                         parameters=parameters)

                if save_results:
                    experiment_number = experiment_id
                    experiment_data = read_df (path_experiments)
                    self.log_results (dict_results, experiment_data, experiment_number, run_number, time_spent)

    def rerun_experiment_par (self, experiments, run_numbers=None, parameters={}):

        path_experiments = self.path_experiments
        for experiment_id in experiments:
            path_experiment = self.get_path_experiment (experiment_id)

            for run_number in run_numbers:
                path_results = path_experiment/f'{run_number}'
                self.run_experiment_pipeline (run_number, path_results, parameters=parameters)

    def find_closest_epoch (self, experiment_data, parameters):
        """Finds experiment with same parameters except for number of epochs.

        Takes the epochs that are closer but lower than the one in parameters."""
        name_epoch = self.name_epoch
        experiment_numbers, _, _ = experiment_utils.find_rows_with_parameters_dict (
            experiment_data, parameters, ignore_keys=[name_epoch,'prev_epoch'])

        defaults = self.get_default_parameters(parameters)
        current_epoch = parameters.get(name_epoch, defaults.get(name_epoch))
        mi_epoch = (dflt.parameters_col, name_epoch, '')
        if current_epoch is None:
            current_epoch = -1
        if len(experiment_numbers) > 1:
            epochs = experiment_data.loc[experiment_numbers, mi_epoch].copy()
            epochs[epochs.isnull()]=defaults.get(name_epoch)
            epochs = epochs.loc[epochs<=current_epoch]
            if epochs.shape[0] == 0:
                return None
            else:
                return epochs.astype(int).idxmax()
        elif len(experiment_numbers) == 1:
            return experiment_numbers[0]
        else:
            return None

    def get_last_epoch (self, parameters, path_results, name_last_epoch=dflt.name_last_epoch):

        name_epoch = self.name_epoch
        name_model_history = self.name_model_history
        path_model_history = f'{path_results}/{name_model_history}'

        prev_epoch = -1
        if os.path.exists(path_model_history):
            summary = joblib.load (path_model_history)
            prev_epoch = summary.get(name_last_epoch)
            if prev_epoch is None:
                key_score = self.key_score
                if key_score in summary and (isinstance(summary[key_score], list)
                                             or isinstance(summary[key_score], np.array)):
                    prev_epoch = (~np.isnan(summary[key_score])).sum()

        return prev_epoch

    def finished_all_epochs (self, parameters, path_results,
                             name_last_epoch=dflt.name_last_epoch):
        defaults = self.get_default_parameters (parameters)
        current_epoch = parameters.get(self.name_epoch, defaults.get(self.name_epoch))
        prev_epoch = self.get_last_epoch (parameters, path_results,
                                          name_last_epoch=name_last_epoch)

        if prev_epoch >= current_epoch:
            finished = True
        else:
            finished = False

        return finished

    def make_resume_from_checkpoint (self, parameters, prev_path_results, use_best=False, previous_model_file_name=None,
                                    model_extension='h5', model_name='checkpoint_', epoch_offset=0, name_best_model='best_model',
                                    name_last_epoch=dflt.name_last_epoch):

        found = False
        path_model_history = f'{prev_path_results}/{self.name_model_history}'
        if os.path.exists(path_model_history):
            parameters['resume_summary'] = path_model_history
            found = True
            parameters['prev_path_results'] = prev_path_results
            if parameters.get('previous_model_file_name') is not None:
                parameters['resume'] = f'{prev_path_results}/{previous_model_file_name}'
            elif use_best:
                parameters['resume'] = f'{prev_path_results}/{name_best_model}.{model_extension}'
            else:
                summary = joblib.load (path_model_history)
                prev_epoch = summary.get(name_last_epoch)
                key_score = self.key_score
                if prev_epoch is None:
                    if key_score in summary and (isinstance(summary[key_score], list)
                                                 or isinstance(summary[key_score], np.array)):
                        prev_epoch = (~np.isnan(summary[key_score])).sum()
                    else:
                        prev_epoch = 0

                if prev_epoch >= 0:
                    parameters['resume'] = (f'{prev_path_results}/'
                                            f'{model_name}{prev_epoch+epoch_offset}.{model_extension}')
            if not os.path.exists(parameters['resume']):
                path_resume2 = f'{prev_path_results}/{self.model_file_name}'
                if os.path.exists (path_resume2):
                    parameters['resume'] = path_resume2
                else:
                    parameters['resume'] = ''
                    parameters['prev_path_results'] = ''
                    found = False

        return found

    def exists_current_checkpoint (self, parameters, path_results):
        model_file_name = self.model_file_name
        return os.path.exists (f'{path_results}/{model_file_name}')

    def obtain_last_result (self, parameters, path_results, use_last_result_from_dict=False,
                            min_iterations=dflt.min_iterations):

        if use_last_result_from_dict:
            return self.obtain_last_result_from_dict (parameters, path_results,
                                                      use_last_result_from_dict=use_last_result_from_dict,
                                                      min_iterations=min_iterations)
        name_result_file = self.name_model_history
        path_results_file = f'{path_results}/{name_result_file}'
        dict_results = None
        if os.path.exists (path_results_file):
            history = joblib.load (path_results_file)
            metrics = parameters.get('key_scores')
            if metrics is None:
                metrics = history.keys()
            ops = parameters.get('ops')
            if ops is None:
                ops = ['max'] * len(metrics)
            if type(ops) is str:
                ops = [ops] * len(metrics)
            if type(ops) is dict:
                ops_dict = ops
                ops = ['max'] * len(metrics)
                i = 0
                for k in metrics:
                    if k in ops_dict.keys():
                        ops[i] = ops_dict[k]
                    i += 1
            dict_results = {}
            max_last_position = -1
            for metric, op in zip(metrics, ops):
                if metric in history.keys():
                    history_array = history[metric]
                    score = min(history_array) if op == 'min' else max(history_array)
                    last_position = np.where(np.array(history_array).ravel()==0)[0]
                    if len(last_position) > 0:
                        last_position = last_position[0] - 1
                    else:
                        last_position = len(history_array)
                    dict_results[metric] = score
                else:
                    last_position = -1
                max_last_position = max(last_position, max_last_position)

            dict_results['last'] = max_last_position
            if max_last_position < min_iterations:
                dict_results = None
                print (f'not storing result from {path_results} with iterations {max_last_position}')
            else:
                print (f'storing result from {path_results} with iterations {max_last_position}')

        return dict_results

    #export
    def obtain_last_result_from_dict (self, parameters, path_results, use_last_result_from_dict=False,
                            min_iterations=dflt.min_iterations):
        name_result_file = self.result_file
        path_results_file = f'{path_results}/{name_result_file}'
        dict_results = None
        if os.path.exists (path_results_file):
            dict_results = joblib.load (path_results_file)
            if 'last' not in dict_results.keys() and 'epoch' in dict_results.keys():
                dict_results['last'] = dict_results['epoch']
            if 'last' not in dict_results:
                dict_results_from_history = self.obtain_last_result (
                    parameters, path_results, use_last_result_from_dict=False,
                    min_iterations=min_iterations)
                if dict_results_from_history is not None:
                    dict_results['last'] = dict_results_from_history['last']
            if 'last' not in dict_results:
                raise RuntimeError ('dict_results has no entry named "last", and '
                                    'the value of last could not be retrieved from '
                                    'a model history file')
            max_last_position = dict_results['last']
            if max_last_position < min_iterations:
                dict_results = None
                print (f'not storing result from {path_results} with iterations {max_last_position}')
            else:
                print (f'storing result from {path_results} with iterations {max_last_position}')

        return dict_results

    def register_and_store_subclassed_manager (self):
        #self.logger.debug ('registering')
        self.manager_factory.register_manager (self)
        self.manager_factory.write_manager (self)

    def set_git_tracking_parameters (self, parameters, experiment_number, run_number,
                                     do_git_tracking=None, include_model_history=True,
                                     add_pipeline=False):
        do_git_tracking = do_git_tracking if do_git_tracking is not None else self.do_git_tracking
        if do_git_tracking:
            path_experiment = self.get_path_experiment (experiment_number)
            path_results = self.get_path_results (path_experiment=path_experiment, run_number=run_number)
            base_files = [f'{dflt.name_logger}.log']
            experiment_files = ['em_args.json', 'em_attrs.json',  'info.json', 'other_parameters.json',
                                'parameters.json', 'parameters.pk', 'parameters.txt']
            result_files = ['parameters.pk', 'summary.txt', 'dict_results.pk', 'experiment.log',
                            'parameters.json', 'parameters.txt']
            tracked_files = []
            for base_file in base_files:
                tracked_files.append (f'{self.path_experiments}/{base_file}')
            for experiment_file in experiment_files:
                tracked_files.append (f'{path_experiment}/{experiment_file}')
            for result_file in result_files:
                tracked_files.append (f'{path_results}/{result_file}')
            if include_model_history:
                tracked_files.append (f'{path_results}/{self.name_model_history}')

            commit_message = f'new experiment {self.folder}/experiments/{experiment_number:05d}/{run_number}'
            parameters.update (additional_file_paths=tracked_files, track_files=True,
                               commit_message=commit_message, copy_no_repo_files=True,
                               out_of_repo_path=str(path_results))

    def commit_results (self, experiment_number, run_number, do_git_tracking=None,
                        include_model_history=True):
        do_git_tracking = do_git_tracking if do_git_tracking is not None else self.do_git_tracking
        if do_git_tracking:
            path_experiment = self.get_path_experiment (experiment_number)
            path_results = self.get_path_results (path_experiment=path_experiment, run_number=run_number)
            result_files = ['dict_results.pk', f'{dflt.name_logger}.log']
            tracked_files = []
            for result_file in result_files:
                tracked_files.append (f'{path_results}/{result_file}')
            if include_model_history:
                tracked_files.append (f'{path_results}/{self.name_model_history}')
            commit_message = f'new experiment {self.folder}/experiments/{experiment_number:05d}/{run_number}'
            add_commit_files (tracked_files, message=commit_message)

# Cell
def get_git_revision_hash (path_experiments=None):
    path_experiments = Path(path_experiments).resolve() if path_experiments is not None else None
    try:
        git_hash = subprocess.check_output(['git', 'rev-parse', 'HEAD'])
        git_hash = str(git_hash)
        if path_experiments is not None:
            json_dump(git_hash, path_experiments/'git_hash.json')
    except:
        logger = logging.getLogger("experiment_manager")
        if path_experiments is not None and (path_experiments/'git_hash.json').exists():
            logger.info ('could not get git hash, retrieving it from disk...')
            git_hash = json_load (path_experiments/'git_hash.json')
        else:
            logger.info ('could not get git hash, using empty string...')
            git_hash = ''

    return str(git_hash)

# Cell
def record_parameters (path_save, parameters, other_parameters=None, em_args=None, info=None,
                      em_attrs=None):
    if em_attrs is not None:
        em_attrs = get_scalar_fields (em_attrs)
    with open(f'{path_save}/parameters.txt', 'wt') as f:
        f.write(f'{print_parameters(parameters, dict_name="parameters")}\n')
        if other_parameters is not None:
            f.write(f'\n\n{print_parameters(other_parameters, dict_name="other_parameters")}\n')
        if em_args is not None:
            f.write(f'\n\n{print_parameters(em_args, dict_name="em_args")}\n')
        if info is not None:
            f.write(f'\n\n{print_parameters(info, dict_name="info")}\n')
        if em_attrs is not None:
            f.write(f'\n\n{print_parameters(em_attrs, dict_name="em_attrs")}\n')

    to_pickle = [parameters]
    if other_parameters is not None:
        to_pickle.append (other_parameters)
    if em_args is not None:
        to_pickle.append(em_args)
    if info is not None:
        to_pickle.append(info)
    if em_attrs is not None:
        to_pickle.append(em_attrs)
    if len(to_pickle) == 1: to_pickle = to_pickle[0]
    joblib.dump (to_pickle,f'{path_save}/parameters.pk')

    try:
        json_dump(parameters, f'{path_save}/parameters.json')
    except:
        pass
    if other_parameters is not None:
        try:
            json_dump(other_parameters, f'{path_save}/other_parameters.json')
        except:
            pass
    if em_args is not None:
        try:
            json_dump(em_args, f'{path_save}/em_args.json')
        except:
            pass
    if info is not None:
        try:
            json_dump(info, f'{path_save}/info.json')
        except:
            pass
    if em_attrs is not None:
        try:
            json_dump(em_attrs, f'{path_save}/em_attrs.json')
        except:
            pass

# Cell
def print_parameters(parameters, dict_name=None):
    if dict_name is not None:
        text = '%s=dict(' %dict_name
        tpad = ' ' * len(text)
    else:
        text = '\t'
        tpad = '\t'
    for idx, (key, value) in enumerate(sorted(parameters.items(), key=lambda x: x[0])):
        if type(value) is str:
            value = '%s%s%s' %("'",value,"'")
        text += '{}={}'.format(key, value)
        if idx < (len(parameters)-1):
            text += ',\n{}'.format(tpad)

    if dict_name is not None:
        text += ')\n'
    else:
        text += '\n'

    return text

# Cell
def load_or_create_experiment_values (path_experiments, parameters, precision=1e-15, logger=None):
    if logger is None: logger = logging.getLogger("experiment_manager")
    experiment_numbers = []
    changed_dataframe = False

    experiment_data = read_df (path_experiments)
    if experiment_data is not None:
        write_binary_df_if_not_exists (experiment_data, path_experiments)
        experiment_data = experiment_data.copy()
        experiment_data, removed_defaults = remove_defaults_from_experiment_data (experiment_data)

        # Finds rows that match parameters. If the dataframe doesn't have any parameter with that name,
        # a new column is created and changed_dataframe is set to True
        experiment_numbers, changed_dataframe, _ = experiment_utils.find_rows_with_parameters_dict (
            experiment_data, parameters, precision=precision
        )

        changed_dataframe = changed_dataframe or removed_defaults

        if len(experiment_numbers) > 1:
            logger.info ('more than one matching experiment: ', experiment_numbers)
    else:
        experiment_data = pd.DataFrame()

    if len(experiment_numbers) == 0:
        columns = pd.MultiIndex.from_product (
            [[dflt.parameters_col], list(parameters.keys()), ['']])
        experiment_number = experiment_data.shape[0]
        if experiment_data.empty:
            experiment_data = experiment_data.append (parameters, ignore_index=True)
            experiment_data.columns = columns
        else:
            experiment_data = pd.concat(
                [experiment_data, pd.DataFrame(columns=experiment_data.columns, index=[experiment_number])],
                axis=0)
            experiment_data[[c for c in columns if c not in experiment_data]] = None
            experiment_data.loc [experiment_number, columns] = parameters.values()
            experiment_data = experiment_data[experiment_data.columns.sort_values()]
        changed_dataframe = True
    else:
        experiment_number = experiment_numbers[0]

    if changed_dataframe:
        write_df (experiment_data, path_experiments)

    return experiment_number, experiment_data

# Cell
def store_parameters (path_experiments, experiment_number, parameters):
    """ Keeps track of dictionary to map experiment number and parameters values for the different experiments."""
    path_experiments = Path(path_experiments).resolve() if path_experiments is not None else None
    path_hp_dictionary = path_experiments/'parameters.pk'
    if os.path.exists(path_hp_dictionary):
        all_parameters = joblib.load (path_hp_dictionary)
    else:
        all_parameters = {}
    if experiment_number not in all_parameters.keys():
        str_par = '\n\nExperiment %d => parameters: \n%s\n' %(experiment_number,print_parameters(parameters))
        f = open(path_experiments/'parameters.txt', 'at')
        f.write(str_par)
        f.close()
        all_parameters[experiment_number] = parameters
        joblib.dump (all_parameters, path_hp_dictionary)

    # pickle number of current experiment, for visualization
    joblib.dump (experiment_number, path_experiments/'current_experiment_number.pkl')

# Cell
def isnull (experiment_data, experiment_number, name_column):
    return ((name_column not in experiment_data.columns) or
            (experiment_data.loc[experiment_number, name_column] is None) or
            np.isnan(float(experiment_data.loc[experiment_number, name_column])))

# Cell
def get_experiment_number (path_experiments, parameters = {}):
    experiment_number, _ = load_or_create_experiment_values (path_experiments, parameters)

    return experiment_number

# Cell
def get_experiment_numbers (path_results_base, parameters_single_value, parameters_multiple_values_all):

    experiment_numbers = []

    parameters_multiple_values_all = list(ParameterGrid(parameters_multiple_values_all))

    for (i_hp, parameters_multiple_values) in enumerate(parameters_multiple_values_all):
        parameters = parameters_multiple_values.copy()
        parameters.update(parameters_single_value)
        parameters = remove_defaults (parameters)

        experiment_number = get_experiment_number (path_results_base, parameters=parameters)
        experiment_numbers.append(experiment_number)

    return experiment_numbers

# Cell
def insert_experiment_script_path (info, logger, stack_level=-3):
    if info.get('script_path') is None:
        stack_level = info.get('stack_level', stack_level)
        stack = traceback.extract_stack()[stack_level]
        info['script_path'] = stack.filename
        info['lineno'] = stack.lineno
        logger.info ('experiment script: {}, line: {}'.format(stack.filename, stack.lineno))
        if 'stack_level' in info:
            del info['stack_level']

# Cell
def load_parameters (experiment=None,
                     other_parameters={}, em_args={}, parameters = {},
                     check_experiment_matches=True, em=None):

    if em is None:
        from .config.hpconfig import get_experiment_manager
        em = get_experiment_manager ()

    path_experiments = em.path_experiments

    path_experiment = em.get_path_experiment (experiment)

    if (path_experiment/'parameters.pk').exists():
        try:
            parameters2, other_parameters2, em_args2, *_ = joblib.load (path_experiment/'parameters.pk')
        except ValueError:
            parameters2, other_parameters2 = joblib.load (path_experiment/'parameters.pk')
            em_args2 = {}

        other_parameters2.update(other_parameters)
        other_parameters = other_parameters2
        em_args2.update(em_args)
        em_args = em_args2

        # if we don't add or modify parameters, we require that the old experiment number matches the new one
        if (len(parameters) == 0) and check_experiment_matches:
            em.logger.info (f'requiring experiment number to be {experiment}')
            em_args['experiment_number'] = experiment
        elif 'experiment_number' in em_args:
            del em_args['experiment_number']

        parameters2.update(parameters)
        parameters = parameters2
    else:
        raise FileNotFoundError (f'file {path_experiment/"parameters.pk"} not found')

    return parameters, other_parameters, em_args

# Cell
def get_scalar_fields (other_parameters):
    parameters_to_save = {}
    for k in other_parameters.keys():
        if type(other_parameters[k]) is str:
            parameters_to_save[k] = other_parameters[k]
        elif np.isscalar(other_parameters[k]):
            parameters_to_save[k] = other_parameters[k]
        elif other_parameters[k] is None:
            parameters_to_save[k] = other_parameters[k]
    return parameters_to_save

def save_other_parameters (experiment_number, other_parameters, path_experiments):

    parameters_to_save = get_scalar_fields (other_parameters)

    path_csv = f'{str(path_experiments)}/other_parameters.csv'
    df = pd.DataFrame (index=[experiment_number], data=parameters_to_save)

    if os.path.exists (path_csv):
        df_all = pd.read_csv (path_csv, index_col=0)
        df_all = pd.concat([df_all, df], sort=True)
        df_all = df_all.loc[~df_all.index.duplicated(keep='last')]
    else:
        df_all = df
    df_all.to_csv (path_csv)

# Cell
class DecoratorExperimentManager (ExperimentManager):
    default_path_experiments='results/all_runs'
    def __init__ (self,
              path_experiments=default_path_experiments,
              folder=None,
              em_attrs={},
              wrapped_run_experiment=None,
              allow_base_class=True,
              do_git_tracking=False,
              **kwargs):

        super().__init__(path_experiments=path_experiments,
                         folder=folder,
                         allow_base_class=allow_base_class,
                         do_git_tracking=do_git_tracking,
                         **em_attrs)

        self.run_experiment = partial (wrapped_run_experiment, self)

def track_experiment(_func=None, *,
                     path_experiments=DecoratorExperimentManager.default_path_experiments,
                     do_git_tracking=False):
    def decorator_track_experiment(func):
        @wraps(func)
        def create_and_run_experiment_manager (*args, **kwargs):
            em = DecoratorExperimentManager (wrapped_run_experiment=func,
                                             path_experiments=path_experiments,
                                             do_git_tracking=do_git_tracking)
            em.create_experiment_and_run (**kwargs)
        return create_and_run_experiment_manager
    if _func is None:
        return decorator_track_experiment                      # 2
    else:
        return decorator_track_experiment (_func)