# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/examples/dummy_experiment_manager.ipynb (unless otherwise specified).

__all__ = ['__author__', '__copyright__', '__license__', 'FakeModel', 'DummyExperimentManager',
           'run_multiple_experiments', 'generate_data', 'remove_previous_experiments']

# Cell
__author__ = "Jaume Amores"
__copyright__ = "Copyright 2021, Johnson Controls"
__license__ = "MIT"

# Cell
import numpy as np
import joblib

# Cell
class FakeModel (object):

    overfitting_epochs = 20

    def __init__ (self, offset=0.5, rate=0.01, epochs=10, noise=0.0, verbose=True):
        # hyper-parameters
        self.offset = offset
        self.rate = rate
        self.epochs = epochs

        # fake internal weight
        self.weight = 0

        # fake accuracy
        self.accuracy = 0

        # noise
        self.noise = noise

        # other parameters
        self.verbose = verbose

        self.history = {}
        self.current_epoch = 0
        self.dict_results = None

    def fit (self):
        number_epochs = int(self.epochs)
        if self.verbose:
            print (f'fitting model with {number_epochs} epochs')

        if self.current_epoch==0:
            self.accuracy = self.offset

        for epoch in range(number_epochs):
            self.weight += self.rate
            if self.current_epoch < self.overfitting_epochs:
                self.accuracy += self.rate
            else:
                self.accuracy -= self.rate
            if self.verbose:
                print (f'epoch {epoch}: accuracy: {self.accuracy}')

            # increase current epoch by 1
            self.current_epoch += 1

            # we keep track of the evolution of different metrics to later be able to visualize it
            self.store_intermediate_metrics ()

    def store_intermediate_metrics (self):
        validation_accuracy, test_accuracy = self.score()
        if 'validation_accuracy' not in self.history:
            self.history['validation_accuracy'] = []
        self.history['validation_accuracy'].append(validation_accuracy)

        if 'test_accuracy' not in self.history:
            self.history['test_accuracy'] = []
        self.history['test_accuracy'].append(test_accuracy)

        if 'accuracy' not in self.history:
            self.history['accuracy'] = []
        self.history['accuracy'].append(self.accuracy)

        self.dict_results = dict (accuracy=self.accuracy,
                                  validation_accuracy=validation_accuracy,
                                  test_accuracy=test_accuracy)

    def save_model_and_history (self, path_results):
        joblib.dump (self.weight, f'{path_results}/model_weights.pk')
        joblib.dump (self.history, f'{path_results}/model_history.pk')
        joblib.dump (self.dict_results, f'{path_results}/dict_results.pk')

    def load_model_and_history (self, path_results):
        if os.path.exists(f'{path_results}/model_weights.pk'):
            print (f'reading model from {path_results}/model_weights.pk')
            self.weight = joblib.load (f'{path_results}/model_weights.pk')
            self.history = joblib.load (f'{path_results}/model_history.pk')
            self.current_epoch = len(self.history['accuracy'])
            if self.current_epoch > 0:
                self.accuracy = self.history['accuracy'][-1]
            else:
                self.accuracy = self.offset
        else:
            print (f'model not found in {path_results}')

    def retrieve_score (self):
        if self.dict_results is None:
            self.dict_results = joblib.load (f'{path_results}/dict_results.pk')
        return self.dict_results['validation_accuracy'], self.dict_results['test_accuracy']

    def score (self):
        # validation accuracy
        validation_accuracy = self.accuracy + np.random.randn() * self.noise

        # test accuracy
        if self.current_epoch < 10:
            test_accuracy = self.accuracy + 0.1
        else:
            test_accuracy = self.accuracy - 0.1
        test_accuracy = test_accuracy + np.random.randn() * self.noise

        # make accuracy be in interval [0,1]
        validation_accuracy = max(min(validation_accuracy, 1.0), 0.0)
        test_accuracy = max(min(test_accuracy, 1.0), 0.0)

        return validation_accuracy, test_accuracy

    # fake load_data which does nothing
    def load_data (self):
        pass



# Cell
from ..experiment_manager import ExperimentManager
import hpsearch
import os
from ..visualization import plot_utils

class DummyExperimentManager (ExperimentManager):

    def __init__ (self,
                  path_experiments=None,
                  folder=None,
                  metric='validation_accuracy',
                  op='max',
                  **kwargs):

        if path_experiments is None: path_experiments = f'{os.path.dirname(hpsearch.__file__)}/../results'

        super().__init__ (path_experiments=path_experiments,
                          folder=folder,
                          metric=metric,
                          op=op,
                          **kwargs)

    def run_experiment (self, parameters={}, path_results='./results'):
        # extract hyper-parameters used by our model. All the parameters have default values if they are not passed.
        offset = parameters.get('offset', 0.5)   # default value: 0.5
        rate = parameters.get('rate', 0.01)   # default value: 0.01
        epochs = parameters.get('epochs', 10) # default value: 10
        noise = parameters.get('noise', 0.0)

        # other parameters that do not form part of our experiment definition
        # changing the values of these other parameters, does not make the ID of the experiment change
        verbose = parameters.get('verbose', True)

        # build model with given hyper-parameters
        model = FakeModel (offset=offset, rate=rate, epochs=epochs, noise = noise, verbose=verbose)

        # load training, validation and test data (fake step)
        model.load_data()

        # fit model with training data
        model.fit ()

        # save model weights and evolution of accuracy metric across epochs
        model.save_model_and_history(path_results)

        # evaluate model with validation and test data
        validation_accuracy, test_accuracy = model.retrieve_score()

        # store model
        self.model = model

        # the function returns a dictionary with keys corresponding to the names of each metric.
        # We return result on validation and test set in this example
        dict_results = dict (validation_accuracy = validation_accuracy,
                             test_accuracy = test_accuracy)

        return dict_results

    # implementing the following method is not necessary but recommended
    def get_default_parameters (self, parameters):
        """Indicate the default value for each of the hyper-parameters used."""
        defaults = dict(offset=0.5,
                        rate=0.01,
                        epochs=10)

        if parameters.get('rate', defaults['rate']) < 0.001:
            defaults.update (epochs=100)

        return defaults

    def experiment_visualization (self, experiments=None, run_number=0,
                                  name_file='model_history.pk', metrics=['test_accuracy'],
                                  backend='matplotlib', **kwargs):
        path_experiments = self.path_experiments
        if not isinstance(metrics, list): metrics = [metrics]
        if isinstance(run_number, list) or isinstance(run_number, range): run_number = run_number[0]
        for metric in metrics:
            traces = []
            for experiment_id in experiments:
                path_results = self.get_path_results (experiment_id, run_number=run_number)
                if os.path.exists('%s/%s' %(path_results, name_file)):
                    history = joblib.load('%s/%s' %(path_results, name_file))
                    label = '{}'.format(experiment_id)
                    traces = plot_utils.add_trace ((1-np.array(history[metric]))*20, style='A.-',
                                                   label=label, backend=backend, traces=traces)
            plot_utils.plot(title=metric, xlabel='epoch', ylabel=metric, traces=traces, backend=backend)

# Cell
def run_multiple_experiments (nruns=1, noise=0.0, verbose=True, rate=0.03, parameters_multiple_values=None,
                              parameters_single_value=None, other_parameters={},
                              EM=DummyExperimentManager, em=None, em_args={},
                              **kwargs):
    if em is None:
        em = EM (**kwargs)
    # parameters where we use a fixed value
    if parameters_single_value is None: parameters_single_value = dict(rate=rate, noise=noise)
    if parameters_multiple_values is None:
        parameters_multiple_values=dict(offset=[0.1, 0.3, 0.6], epochs=[5, 15, 30]) # parameters where we try multiple values
    else:
        parameters_multiple_values=parameters_multiple_values
    other_parameters_original = other_parameters
    other_parameters = dict (verbose=verbose) # parameters that control other aspects that are not part of our experiment definition (a new experiment is not created if we assign different values for these parametsers)
    other_parameters.update (other_parameters_original)
    em.grid_search (log_message='fixed rate, multiple epochs values',
            parameters_single_value=parameters_single_value,
            parameters_multiple_values=parameters_multiple_values,
            other_parameters=other_parameters,
            nruns=nruns,
            **em_args)

# Cell
def generate_data (name_folder, parameters_multiple_values=None, rate=0.03,
                   parameters_single_value=None, other_parameters={}, verbose=0, em_args={}, **kwargs):
    np.random.seed (42)
    path_experiments=f'test_{name_folder}/default'
    manager_path = f'{path_experiments}/managers'
    em = DummyExperimentManager (path_experiments=path_experiments, manager_path=manager_path,
                                 verbose=verbose, **kwargs)
    em.remove_previous_experiments (parent=True)
    run_multiple_experiments (em=em, nruns=5, noise=0.1, rate=rate, verbose=False,
                              parameters_multiple_values=parameters_multiple_values,
                              parameters_single_value=parameters_single_value,
                              other_parameters=other_parameters,  em_args=em_args)
    return em

# Cell
import shutil
import os

def remove_previous_experiments (EM=DummyExperimentManager):
    em = EM ()
    em.remove_previous_experiments (parent=True)