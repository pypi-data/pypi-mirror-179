# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/blocks/preprocessing.ipynb.

# %% auto 0
__all__ = ['OneHotEncoder', 'WindowGenerator', 'WindowAggregator']

# %% ../../nbs/blocks/preprocessing.ipynb 3
__author__ = "Jaume Amores"
__copyright__ = "Copyright 2021, Johnson Controls"
__license__ = "MIT"

# %% ../../nbs/blocks/preprocessing.ipynb 4
import numpy as np
import pandas as pd
import sklearn.preprocessing
from sklearn.utils.validation import check_is_fitted
from sklearn.exceptions import NotFittedError

from ..core.components import PandasComponent, SamplingComponent
from ..config import bt_defaults as dflt

# %% ../../nbs/blocks/preprocessing.ipynb 7
class OneHotEncoder (PandasComponent):
    def __init__ (self,
                  categories='auto',
                  handle_unknown='ignore',
                  dtype=int,
                  **kwargs):
        super().__init__ (**kwargs)
        self.categories = categories
        self.handle_unknown = handle_unknown
        self.one_hot_encoder = sklearn.preprocessing.OneHotEncoder(categories=categories,
                                                                    handle_unknown=handle_unknown,
                                                                    sparse=False)
        self.dtype = dtype

    def _fit (self, X, y=None):
        self.one_hot_encoder.fit (X)
        return self

    def _apply (self, df):
        try:
            check_is_fitted (self.one_hot_encoder)
        except NotFittedError:
            if self.categories != 'auto':
                self.one_hot_encoder.fit(df)
            else:
                raise NotFittedError('OneHotEncoder must be fitted first')
        X = self.one_hot_encoder.transform(df)
        if self.dtype is int or self.dtype == 'int':
            X = X.astype(int)
        df = pd.DataFrame (data=X,
                           columns=self.one_hot_encoder.get_feature_names(input_features=df.columns),
                           index=df.index)
        return df

# %% ../../nbs/blocks/preprocessing.ipynb 12
class WindowGenerator (SamplingComponent):
    """
    Creates a DataFrame of time-ordered sliding windows extracted from an input time-series.

    The input time-series is given as a DataFrame with N rows and D+1 columns, where N is
    the number of observations (i.e., number of time-steps) and D is the number of variables
    and there is an additional column for the ground-truth label.

    The output DataFrame has one row per sliding window, and W*D+1 columns, where W is
    the size of the sliding windows (or sequence length).

    While not implemented at this moment, the WindowGenerator can use, in addition to
    label information, other types of metadata attributes such as chiller ID. All this metadata
    is handled the same way as we do with the label. In case of large missing gaps, the
    DataFrame can implicitly be split into sections, where each section is contiguous
    (i.e., without gaps). In this case, an additional column indicates the index of the section,
    from 0 to S-1. This column is then treated as the other two metadata attributes (label
    and chiller ID), making sure that sliding windows have only one value for each one of
    these attributes (i.e, contains only normal or anomaly data but not both, contains data
    from only one chiller, and contains data from only one section). See documentation for an
    illustration.
    """
    def __init__ (self,
                  sequence_length=None,
                  sequence_stride=None,
                  mixed_labels=False,
                  sampling_interval=None,
                  **kwargs):
        """
        Initialize fields.

        Parameters
        ----------
        sequence_length : int, optional
            Size of each window, in number of timesteps.
        sequence_stride : int, optional
            Period between successive output sequences.
            For stride s, output samples would start at index
            data[i], data[i + s], data[i + 2 * s], etc.:
            https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array
        mixed_labels : bool, optional
            Whether the same window can contain both anomaly and normal
            data (Default=False).
        sampling_interval : int, optional
            Take one sample every `sampling_interval` minutes.
        """

        super().__init__ (**kwargs)
        self.sequence_length = sequence_length
        self.sequence_stride = sequence_stride
        self.mixed_labels = mixed_labels
        self.sampling_interval = sampling_interval

    def _fit (self, X, y=None):
        return self

    def _transform (self, X):

        df = X.resample(f'{self.sampling_interval}min').asfreq()

        df = self.construct_dataframe_of_sequences (df)

        # select only those windows that are appropriate
        df = df[~df.isna().any(axis=1)]

        if not self.mixed_labels:
            all_normal = df.label.max(axis=1)==0
            all_anomaly = df.label.min(axis=1)==1
            df = df[all_normal | all_anomaly]
            label = df.pop('label')
            df['label'] = 0
            df.loc[all_anomaly, 'label'] = 1
        else:
            label = df.pop('label')
            df['label'] = label.mean(axis=1)

        return df

    def construct_dataframe_of_sequences (self, df):
        """Given input time-series `df`, creates a DataFrame of sliding windows (also called sequences)"""

        from tensorflow.keras.preprocessing import timeseries_dataset_from_array
        
        self.original_index = df.index
        self.original_columns = df.columns
        self.original_shape = df.shape
        # obtain data
        ds = timeseries_dataset_from_array (df.values, None, self.sequence_length,
                                            sequence_stride=self.sequence_stride, batch_size=1)
        ds = ds.unbatch().as_numpy_iterator()
        x = np.array([x.tolist() for x in ds])
        x = np.transpose (x,[0,2,1])
        x = x.reshape(-1, self.sequence_length*df.shape[1])

        # obtain columns
        columns = pd.MultiIndex.from_product([list(df.columns),list(range(self.sequence_length))])

        # obtain index
        ts = timeseries_dataset_from_array (np.arange(df.shape[0]), None, self.sequence_length,
                                            sequence_stride=self.sequence_stride, batch_size=1)
        ts = ts.unbatch().as_numpy_iterator()
        list_ts = list(ts)
        array_ts = np.array([x.tolist() for x in list_ts])
        df_ts=pd.DataFrame(list_ts)
        df_ts=df_ts.applymap(lambda x: df.index[x])
        arrays = [df_ts.min(axis=1).values,
                 df_ts.max(axis=1).values]
        tuples = list(zip(*arrays))
        index = pd.MultiIndex.from_tuples(tuples, names=["start", "end"])

        # keep original timestamps before converting
        timestamps = df.index

        df = pd.DataFrame (x, index=index, columns=columns)

        self.array_ts = array_ts
        self.index = df.index
        self.columns = df.columns
        self.shape = df.shape

        return df

    def plot (self, df, columns=None, **kwargs):
        if columns is None:
            if df.columns.nlevels > 1:
                columns = df.columns.get_level_values(0).unique()
            else:
                columns = None
        elif type(columns) is not list:
            columns = [columns]
        for idx in df.index:
            index = pd.date_range(idx[0], idx[1], periods=self.sequence_length)
            df_idx = df.loc[idx, columns]
            pooled_time_steps = df_idx.values.reshape(len(columns), self.sequence_length)
            df2 = pd.DataFrame (index=index, columns=columns, data=pooled_time_steps.T)
            df2.plot(**kwargs)

    def describe (self, df, **kwargs):
        if df.columns.nlevels > 1:
            columns = df.columns.get_level_values(0).unique()
        else:
            columns = None
        pooled_time_steps = df.values.T.reshape(-1, self.sequence_length * df.shape[0]).T
        pooled_time_steps = pd.DataFrame(pooled_time_steps, columns=columns)
        return pooled_time_steps.describe()

# %% ../../nbs/blocks/preprocessing.ipynb 17
class WindowAggregator (SamplingComponent):
    """This class produces the the inverse transform of the WindowGeneratorclass.

    In other words, when applied to the result of WindowGenerator, we obtain the
    original time-series.

    This class can be useful for aggregating the windowed data after it has been
    transformed by other components in the pipeline, so that we get back the same
    type of time-series we had at the beginning, but with all the transformations
    applied to it. For instance it can be used to unroll the error signal coming
    out of the ResidualComputer. In principle, the aggregation can use different
    statistics, although currently we only allow the mean operator, which obtains
    the inverse transform."""
    def __init__ (self, sequence_length=None, sequence_stride=None, sampling_interval=None,
                  mixed_labels=False, **kwargs):
        """
        Initialize fields.

        Parameters
        ----------
        sequence_length : int, optional
            Size of each window, in number of timesteps.
        sequence_stride : int, optional
            Period between successive output sequences.
            For stride s, output samples would start at index
            data[i], data[i + s], data[i + 2 * s], etc.:
            https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array
        sampling_interval : int, optional
            Take one sample every `sampling_interval` minutes.
        mixed_labels : bool, optional
            Whether the same window can contain both anomaly and normal
            data (Default=False).
        """
        super().__init__ (**kwargs)
        self.sequence_length = sequence_length
        self.sequence_stride = sequence_stride
        self.sampling_interval = sampling_interval
        self.mixed_labels = mixed_labels

    def _fit (self, X, y=None):
        return self

    def _transform (self, X):
        df = self.aggregate (X)

        # select only those windows that are appropriate
        df = df[~df.isna().any(axis=1)]

        if not self.mixed_labels:
            all_normal = (df.label == 0)
            all_anomaly = (df.label == 1)
            df = df[all_normal | all_anomaly]
            label = df.pop('label')
            df['label'] = 0
            df.loc[all_anomaly, 'label'] = 1

        df.index.name = 'timestamp'
        df['label'] = df.label.astype(float)

        return df

    def aggregate(self, df):
        '''Reconstruct time-series from windows.

        The resulting time-series has a shape similar to the original one before the windowing was applied'''

        from tensorflow.keras.preprocessing import timeseries_dataset_from_array

        # fill gaps with NaN
        index = df.index.get_level_values(0)
        label = df['label']
        df = df.drop(columns='label')
        original_columns = df.columns.get_level_values(0).unique()
        #df = pd.DataFrame(np.arange(len(index)), index=index)

        # resample timestamps and labels
        df.index = index
        label.index = index
        df = df.resample(f'{self.sampling_interval}min').asfreq()
        index = df.index
        label = label.resample(f'{self.sampling_interval}min').fillna(method='ffill')

        last_window = pd.date_range(index[-1], periods=self.sequence_length,
                                    freq=f'{self.sampling_interval}min')
        index = np.r_[index, last_window[1:]]
        label = np.r_[label.values[:], [label.values[-1]]*(self.sequence_length-1)]

        # generate windows of time indexes and labels
        ts = timeseries_dataset_from_array (np.arange(index.shape[0]), None, self.sequence_length,
                                                    sequence_stride=self.sequence_stride, batch_size=1)
        ls = timeseries_dataset_from_array (label, None, self.sequence_length,
                                                    sequence_stride=self.sequence_stride, batch_size=1)
        ts = ts.unbatch().as_numpy_iterator()
        ls = ls.unbatch().as_numpy_iterator()
        list_ts = list(ts)
        list_ls = list(ls)
        array_ts = np.array([x.tolist() for x in list_ts])
        array_ls = np.array([x.tolist() for x in list_ls])
        dft = pd.DataFrame (array_ts)
        dfl = pd.DataFrame (array_ls)

        times = dft.values.ravel()
        label = dfl.values.ravel()
        number_rows = times.shape[0]
        number_columns = len(original_columns) + 2
        df_ravel = pd.DataFrame (np.zeros((number_rows, number_columns)),
                                 columns=list(original_columns)+['label', 'times'])


        df_ravel['times'] = times
        df_ravel['label'] = label

        for c in original_columns:
            df_ravel[c] = df[c].values.ravel()

        df_aggregated = df_ravel.groupby('times').mean()

        df_aggregated.index = index[df_aggregated.index]

        return df_aggregated
