# AUTOGENERATED! DO NOT EDIT! File to edit: ../../../nbs/00_tests/core/tst.compose.ipynb.

# %% auto 0
__all__ = ['sh_imported', 'df_ensemble', 'SimpleMultiComponent', 'Transform1', 'Transform2', 'SimplePipeline',
           'build_pipeline_construct_diagram_1', 'build_pipeline_construct_diagram_2', 'PandasTransformWithLabels1',
           'PandasTransformWithLabels2', 'SimplePandasPipeline', 'TransformWithLabels1', 'TransformWithLabels2',
           'SimplePandasPipelineNoPandasComponent', 'DummyEvaluator', 'DataPipelineSTM', 'CompletePipelineSTM',
           'DataPipelineSTM2', 'CompletePipelineSTM2', 'TransformM', 'column_transformer_data', 'multi_split_data',
           'multi_split_data_df_column', 'get_cross_validator_input_data', 'DummyHistoryClassifier', 'run_study',
           'run_multiple_studies', 'DummyClassifierInstance', 'SumColumn', 'MultiplyColumn', 'test_multi_comp_io',
           'test_multi_comp_desc', 'test_multi_comp_desc2', 'test_list', 'test_show', 'test_gather_and_save_info',
           'test_multi_comp_hierarchy', 'test_multi_comp_profiling', 'test_multi_comp_all_equal',
           'test_multi_component_setters', 'test_show_result_statistics', 'test_pass_components', 'test_chain_folders',
           'test_set_root', 'test_set_root_2', 'test_pass_functions_to_multi_component', 'test_pass_tuples_classes',
           'test_set_suffix', 'test_multi_comp_propagate_args', 'test_pass_no_component_class',
           'test_compare_kwargs_compose', 'test_propagate_attribute', 'test_separate_data', 'test_pipeline_fit_apply',
           'test_pipeline_fit_apply_bis', 'test_pipeline_new_comp', 'test_pipeline_set_comp',
           'test_athena_pipeline_training', 'test_pipeline_load_estimator', 'test_construct_diagram',
           'test_show_summary', 'test_multi_comp_profiling2', 'test_some_components_return_nothing',
           'test_fit_apply_multi_exist_all', 'test_make_pipeline', 'test_pipeline_factory', 'test_pandas_pipeline',
           'test_sequential_with_tracking', 'test_seq_with_track_memory', 'test_seq_with_track_memory2',
           'test_parallel', 'test_pipeline_find_last_result', 'test_pipeline_find_last_fitted_model_seq',
           'test_pipeline_find_last_fitted_model_parallel', 'test_multi_modality', 'test_column_selector',
           'test_concat', 'test_identity', 'test_make_column_transformer', 'test_make_column_transformer_passthrough',
           'test_make_column_transformer_remainder', 'test_make_column_transformer_descendants',
           'test_make_column_transformer_fit_transform', 'test_make_column_transformer_different_indexes',
           'test_multi_split_transform', 'test_multi_split_fit', 'test_multi_split_chain', 'test_multi_split_io',
           'test_multi_split_non_dict', 'test_multi_split_non_dict_bis', 'test_multi_split_with_function',
           'test_multi_split_root', 'test_multi_split_tuples', 'test_multi_split_df_column_transform_whole_df',
           'test_multi_split_df_column_transform', 'test_multi_split_df_column_fit', 'test_cross_validator_1',
           'test_cross_validator_2', 'test_cross_validator_3', 'test_cross_validator_4', 'test_optuna_pruner',
           'test_optuna_pruner_2', 'test_optuna_pruner_3', 'test_optuna_pruner_4', 'test_optuna_pruner_5',
           'test_cross_validator_pruner', 'test_instances_ensembler_1', 'test_model_ensembler_1',
           'test_ensemble_different_models', 'test_ensembler_separate_paths', 'test_instances_ensembler_2']

# %% ../../../nbs/00_tests/core/tst.compose.ipynb 2
from dsblocks.core.compose import *

# %% ../../../nbs/00_tests/core/tst.compose.ipynb 3
from pathlib import Path
import pytest 
import time
import numpy as np
import os
import shutil
import joblib
from IPython.display import display
import pandas as pd
#try:
#    import sh
#    sh_imported = True
#except ImportError:
#    sh_imported = False

from sklearn.preprocessing import FunctionTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.utils import Bunch
from sklearn.model_selection import KFold

from dsblocks.core.components import Component, PandasComponent, PickleSaverComponent
from dsblocks.utils.utils import (remove_previous_results, 
                                  set_empty_logger, 
                                  check_new_files)
import dsblocks.config.bt_defaults as dflt
from dsblocks.core.utils import PickleIO
from dsblocks.utils.utils import check_last_part, check_new_files
from dsblocks.core.data_conversion import DataConverter, PandasConverter
sh_imported = False
from dsblocks.utils.dummies import Higher
from dsblocks.utils.dummies import Intermediate, Higher
from dsblocks.utils.dummies import Intermediate, Higher
from dsblocks.utils.dummies import Intermediate
from dsblocks.utils.dummies import Sum1, DummyEstimator
from dsblocks.utils.dummies import Higher
from dsblocks.utils.dummies import Higher
from dsblocks.utils.dummies import Sum1, DummyEstimator
from dsblocks.utils.utils import set_logger
from dsblocks.core.components import Component, PandasComponent
from dsblocks.utils.dummies import Max10, get_data_frame
from dsblocks.utils.dummies import make_pipe1, make_pipe2
from dsblocks.utils.dummies import make_pipe_fit1
from dsblocks.utils.dummies import make_pipe_fit2
import pytest 
from dsblocks.utils.utils import remove_previous_results
from dsblocks.core.utils import PickleIO
from dsblocks.utils.dummies import DummyClassifier
from dsblocks.blocks.blocks import SkSplitGenerator
from dsblocks.blocks.blocks import PandasEvaluator 
import optuna
import optuna
import glob

# %% ../../../nbs/00_tests/core/tst.compose.ipynb 4
class SimpleMultiComponent (MultiComponent):
    def __init__ (self, **kwargs):
        data_io = PickleIO (**kwargs)
        super().__init__(data_io=data_io,
                         **kwargs)

        self.tr1 = Component(FunctionTransformer (lambda x: x*3),
                             data_io=data_io,
                             name='tr1')
        self.tr2 = Component(FunctionTransformer (lambda x: x*2),
                             data_io=data_io,
                             name='tr2')

    def _apply (self, X):
        return self.tr1 (X) + self.tr2(X)

# %% ../../../nbs/00_tests/core/tst.compose.ipynb 5
class Transform1 (Component):
    def __init__ (self, **kwargs):
        super().__init__ (**kwargs)
        self.estimator = Bunch(sum = 1)

    def _fit (self, X, y=None):
        self.estimator.sum = X.sum(axis=0)

    def _apply (self, x):
        return x*1000 + self.estimator.sum

class Transform2 (Component):

    def __init__ (self, **kwargs):
        super().__init__ (**kwargs)
        self.estimator = Bunch(maxim = 1)

    def _fit (self, X, y=None):
        self.estimator.maxim = X.max(axis=0)

    def _apply (self, x):
        return x*100 + self.estimator.maxim
    
class SimplePipeline (Pipeline):
    def __init__ (self, **kwargs):
        super().__init__ (**kwargs)

        # custom transform
        self.tr1 = Transform1(**kwargs) 

        # slklearn transform
        self.tr2 = Transform2(**kwargs)

# %% ../../../nbs/00_tests/core/tst.compose.ipynb 6
def build_pipeline_construct_diagram_1 (path_results):
    class Intermediate (MultiComponent):
        def __init__ (self, name=None, **kwargs):
            super().__init__ (name=name, **kwargs)
            self.first = Component (name='first_component', **kwargs)
            self.second = Component (name='second_component', **kwargs)

    class Higher (Pipeline):
        def __init__ (self, **kwargs):
            super().__init__ (**kwargs)
            self.first = Intermediate (name='first_intermediate', class_name='First', **kwargs)
            self.second = Intermediate (name='second_intermediate', class_name='Second', **kwargs)

    pipeline = Higher(path_results=path_results)
    
    return pipeline

def build_pipeline_construct_diagram_2 (path_results):
    class NewPipeline (Pipeline):
        def __init__ (self, **kwargs):
            data_io = PickleIO (**kwargs)
            super().__init__(data_io=data_io,
                             **kwargs)

            self.tr1 = Component(FunctionTransformer (lambda x: x*3),
                                 data_io=data_io,
                                 class_name='FirstTransform',
                                 name='tr1')
            self.tr2 = Component(FunctionTransformer (lambda x: x*2),
                                 data_io=data_io,
                                 class_name='SecondTransform',
                                 name='tr2')
            
    pipeline = NewPipeline (path_results=path_results)
    
    return pipeline

# %% ../../../nbs/00_tests/core/tst.compose.ipynb 7
# **********************************************
# Good example
# **********************************************
class PandasTransformWithLabels1 (PandasComponent):

    def __init__ (self, **kwargs):
        super().__init__ (**kwargs)
        self.estimator= Bunch(sum = 1)

    def _fit (self, X, y=None):
        self.estimator.sum = X[y==1].sum(axis=0)

    def _apply (self, X):
        r = X*1000 + self.estimator.sum
        return r
    
class PandasTransformWithLabels2 (PandasComponent):

    def __init__ (self, **kwargs):
        super().__init__ (**kwargs)
        self.estimator= Bunch(maxim = 1)

    def _fit (self, X, y=None):
        self.estimator.maxim = X[y==0].max(axis=0)

    def _apply (self, X):
        r = X*100 + self.estimator.maxim
        return r
            
class SimplePandasPipeline (PandasPipeline):
    def __init__ (self, **kwargs):
        super().__init__ (**kwargs)

        # custom transform
        self.tr1 = PandasTransformWithLabels1(**kwargs) 

        # slklearn transform
        self.tr2 = PandasTransformWithLabels2(**kwargs)
        
# **********************************************
# Bad example
# **********************************************
class TransformWithLabels1 (Component):

    def __init__ (self, **kwargs):
        super().__init__ (**kwargs)
        self.estimator= Bunch(sum = 1)

    def _fit (self, X, y=None):
        self.estimator.sum = X[y==1].sum(axis=0)

    def _apply (self, x):
        return x*1000 + self.estimator.sum

class TransformWithLabels2 (Component):

    def __init__ (self, **kwargs):
        super().__init__ (**kwargs)
        self.estimator= Bunch(maxim = 1)

    def _fit (self, X, y=None):
        self.estimator.maxim = X[y==0].max(axis=0)

    def _apply (self, x):
        return x*100 + self.estimator.maxim
    

class SimplePandasPipelineNoPandasComponent (PandasPipeline):
    def __init__ (self, **kwargs):
        super().__init__ (**kwargs)

        # custom transform
        self.tr1 = TransformWithLabels1(**kwargs) 

        # slklearn transform
        self.tr2 = TransformWithLabels2(**kwargs)

# %% ../../../nbs/00_tests/core/tst.compose.ipynb 8
class DummyEvaluator (Component):
    def __init__ (self, **kwargs):
        super().__init__ (**kwargs)
    def _apply (self, df, **kwargs):
        return {'sum': df.values.ravel().sum()}

# %% ../../../nbs/00_tests/core/tst.compose.ipynb 9
# Elements required for having SequentialWithTracking using a data pipeline
# with separate in-memory data storage:
# 1.- memory argument only passed to top-level data pipeline
# 2.- memory=True when constructing the data pipeline
# 3.- stop_propagation=True when constructing the data pipeline
# 4.- path_results=path_data when constructing the data pipeline
# 5.- (optional) give a name or class_name to data pipeline and its components
def DataPipelineSTM (memory=dflt.memory, **kwargs):
    def s (): return 0
    def f1 (x, fa=0.1): return x+fa
    def f2 (x, fb=0.2): return x+fb
    def f3 (x, fc=1): return x+fc
    def f4 (x, fd=2): return x+fd
    return Sequential (s,
                       Sequential (f1, f2, name='seq1', **kwargs),
                       Sequential (f3, f4, name='seq2', **kwargs),
                       memory=memory, name='data_pipeline', **kwargs)
def CompletePipelineSTM (path_results=None, path_data=None, **kwargs):
    def g1 (x, ga=100): return x+ga
    def g2 (x, gb=200): return {'result': x+gb}
    data_pipeline = DataPipelineSTM (path_results=path_data, memory=True, 
                                     stop_propagation=True, **kwargs)
    return SequentialWithTracking (data_pipeline, g1, g2, 
                                   path_results=path_results,
                                   **kwargs)

# %% ../../../nbs/00_tests/core/tst.compose.ipynb 10
def DataPipelineSTM2 (**kwargs):
    def s (): return 0
    def f1 (x, fa=0.1): return x+fa
    def f2 (x, fb=0.2): return x+fb
    def f3 (x, fc=1): return x+fc
    def f4 (x, fd=2): return x+fd
    return Sequential (s,
                       Sequential (f1, f2, name='seq1', **kwargs),
                       Sequential (f3, f4, name='seq2', **kwargs),
                       name='data_pipeline', **kwargs)
def CompletePipelineSTM2 (path_results=None, path_data=None, **kwargs):
    def g1 (x, ga=100): return x+ga
    def g2 (x, gb=200): return {'result': x+gb}
    data_pipeline = DataPipelineSTM2 (**kwargs)
    data_pipeline.set_separate_data (path_data)
    return SequentialWithTracking (data_pipeline, g1, g2, 
                                   path_results=path_results,
                                   **kwargs)

# %% ../../../nbs/00_tests/core/tst.compose.ipynb 11
class TransformM (Component):
    def __init__ (self, modality='', factor=1000, **kwargs):
        super().__init__ (**kwargs)
        self.estimator= Bunch(sum = 1)

    def _fit (self, X, y=None):
        self.estimator.sum = X.sum(axis=0)

    def _apply (self, x):
        return x*self.factor + self.estimator.sum

# %% ../../../nbs/00_tests/core/tst.compose.ipynb 12
def column_transformer_data ():
    df = pd.DataFrame ({'cont1': list(range(5)),
                        'cont2': list(range(5,10)),
                        'cont3': list(range(15,20)),
                        'cont4': list(range(25,30)),
                        'cat_1': list([1,2,3,2,1]),
                        'cat_2': list([0,1,1,0,0])
                        })
        
    tr1 = Component(FunctionTransformer (lambda x: x+1), name='tr1')
    
    return df, tr1

# %% ../../../nbs/00_tests/core/tst.compose.ipynb 13
class Transform1 (Component):
    def __init__ (self, **kwargs):
        super().__init__ (**kwargs)
        self.estimator= Bunch(sum = 1)

    def _fit (self, X, y=None):
        self.estimator.sum = X.sum(axis=0)

    def _apply (self, x):
        return x*1000 + self.estimator.sum
    
class Transform2 (Component):
    def __init__ (self, **kwargs):
        super().__init__ (**kwargs)
        self.estimator= Bunch(maxim = 1)

    def _fit (self, X, y=None, validation_data=None, test_data=None):
        self.estimator.maxim = X.max(axis=0)

        print (f'validation_data: {validation_data}')
        print (f'test_data: {test_data}')

        self.data = dict (validation=validation_data,
                          test=test_data)

    def _apply (self, x):
        return x*100 + self.estimator.maxim

def multi_split_data ():
    data = dict(training = np.array([1,2,3]).reshape(-1,1),
            validation = np.array([10,20,30]).reshape(-1,1),
            test = np.array([100,200,300]).reshape(-1,1)
            )
    
    multi_transform1 = MultiSplitDict (component = Transform1())
    
    tr2 = Transform2()
    multi_transform2 = MultiSplitDict (component=tr2,
                                            fit_additional = ['validation', 'test'])
    
    return data, multi_transform1, multi_transform2, tr2

# %% ../../../nbs/00_tests/core/tst.compose.ipynb 14
def multi_split_data_df_column ():
    # create input data-frame
    data = dict(training = np.array([1,2,3]).reshape(-1,1),
                validation = np.array([10,20,30]).reshape(-1,1),
                test = np.array([100,200,300]).reshape(-1,1))
    values = np.concatenate((data['training'], data['validation'], data['test']))
    df = pd.DataFrame (data=values)
    split = [[k]*data[k].shape[0] for k in data]
    df['split'] = split[0] + split[1] + split[2]

    tr2 = Transform2()
    return df, data, tr2

# %% ../../../nbs/00_tests/core/tst.compose.ipynb 15
def get_cross_validator_input_data ():
    df = pd.DataFrame ({'a': list(range(10)),
                        'b': list (range(10)),
                        'label': [0]*5+[1]*5})
    return df

# %% ../../../nbs/00_tests/core/tst.compose.ipynb 16
class DummyHistoryClassifier (Component):
    def __init__ (self, **kwargs):
        super ().__init__ (**kwargs)
    def _apply (self, X, **kwargs):
        return X
    def _fit (self, X, y=None, **kwargs):
        self.dict_results = {
            'score': np.abs (X['a'].values-5) if (X['a'].values[0] % 2) == 1 else np.abs(X['a'].values-4)}
    def history (self):
        return self.dict_results

# %% ../../../nbs/00_tests/core/tst.compose.ipynb 17
def run_study (n_startup_trials=5, n_warmup_steps=0, study_suffix='', nfolds=1,
               trial_outlier=4, fold_outlier='all', indicate_same_step=True, 
               same_value_per_step=True, n_trials=10):
    
    print ('\n\nnew study')
    print (dict(n_startup_trials=n_startup_trials, n_warmup_steps=n_warmup_steps, 
                study_suffix=study_suffix, nfolds=nfolds, trial_outlier=trial_outlier, 
                fold_outlier=fold_outlier, indicate_same_step=indicate_same_step, 
                same_value_per_step=same_value_per_step))
    
    path_results = f'test_optuna_pruner{study_suffix}'
    study_name=f'test_pruner{study_suffix}'
    pruner = optuna.pruners.MedianPruner(n_startup_trials=n_startup_trials, n_warmup_steps=n_warmup_steps)
    os.makedirs (path_results, exist_ok=True)   
    
    def objective (trial):
        shoulds = []
        for fold in range (nfolds):
            if trial.number==trial_outlier and (fold==fold_outlier or fold_outlier=='all'):
                v = 0
            else:
                v = 5
            step = 0 if indicate_same_step else fold
            fold_value = v if same_value_per_step else fold*100+v 
            trial.report (fold_value, step)
            shoulds.append (trial.should_prune())
        print (f'trial {trial.number}, shoulds: {shoulds}')
        if False:
            if trial.number==4: 
                assert trial.should_prune()
            else:
                assert not trial.should_prune()
        return 5.0
    
    study = optuna.create_study(direction='maximize',
                                study_name=study_name,
                                storage=f'sqlite:///{path_results}/{study_name}.db',
                                pruner=pruner, load_if_exists=True)

    study.optimize(objective, n_trials=n_trials, n_jobs=1)
    remove_previous_results (path_results)
    
def run_multiple_studies (n_startup_trials=2, n_warmup_steps=0, study_suffix='', **kwargs):
    run_study (n_startup_trials=2, n_warmup_steps=0, study_suffix=study_suffix, **kwargs)
    run_study (n_startup_trials=5, n_warmup_steps=0, study_suffix=study_suffix + '_b', **kwargs)

# %% ../../../nbs/00_tests/core/tst.compose.ipynb 18
class DummyClassifierInstance (Component):
    def __init__ (self, **kwargs):
        super ().__init__ (**kwargs)
    def _apply (self, X, **kwargs):
        return (X.values[:,0] > self.estimator.threshold).astype(float)
    def _fit (self, X, y=None, **kwargs):
        self.row = self.suffix
        self.create_estimator (threshold = X.values[self.row, 0])
df_ensemble = get_cross_validator_input_data ()

# %% ../../../nbs/00_tests/core/tst.compose.ipynb 19
class SumColumn (Component):
    def __init__ (self, column=0, **kwargs):
        super ().__init__ (**kwargs)
    def _apply (self, X):
        return X.values[:, self.column]+self.estimator.summand
    def _fit (self, X):
        self.create_estimator (summand=X.values[:,self.column].sum()*100)

class MultiplyColumn (Component):
    def __init__ (self, column=0, **kwargs):
        super ().__init__ (**kwargs)
    def _apply (self, X):
        return X.values [:, self.column]*self.estimator.factor
    def _fit (self, X):
        self.create_estimator (factor=X.values[:, self.column].max()*10)

# %% ../../../nbs/00_tests/core/tst.compose.ipynb 20
def test_multi_comp_io ():
    path_results = 'multi_component_loading_saving'
    remove_previous_results (path_results=path_results)
    
    X = np.array([1,2,3])
    composition1 = SimpleMultiComponent (path_results=path_results)
    result1 = composition1 (X)
    
    composition2 = SimpleMultiComponent (path_results=path_results)
    result2 = composition2.data_io.load_result()
    assert (result1==result2).all()
    
    resut_tr1_2 = composition2.tr1.data_io.load_result()
    resut_tr2_2 = composition2.tr2.data_io.load_result()
    assert (resut_tr1_2==composition1.tr1(X)).all()
    assert (resut_tr2_2==composition1.tr2(X)).all()
    
    assert sorted(os.listdir (f'{path_results}/whole'))==['simple_multi_component_result.pk', 'tr1_result.pk', 'tr2_result.pk']
    
    composition1.set_split ('validation')
    result1b = composition1 (X)
    assert sorted(os.listdir (f'{path_results}/validation'))==['simple_multi_component_result.pk', 'tr1_result.pk', 'tr2_result.pk']
    
    remove_previous_results (path_results=f'{path_results}/whole')
    
    resut_tr1_2 = composition2.tr1.data_io.load_result(split='validation')
    resut_tr2_2 = composition2.tr2.data_io.load_result()
    
    assert (resut_tr1_2==composition1.tr1(X)).all()
    assert resut_tr2_2 is None
    
    composition2.set_split('validation')
    resut_tr1_2 = composition2.tr1.data_io.load_result()
    resut_tr2_2 = composition2.tr2.data_io.load_result()
    
    assert (resut_tr1_2==composition1.tr1(X)).all()
    assert (resut_tr2_2==composition1.tr2(X)).all()
    
    remove_previous_results (path_results=path_results)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 21
def test_multi_comp_desc ():
    class Intermediate (MultiComponent):
        def __init__ (self, name=None, **kwargs):
            super().__init__ (name=name, **kwargs)
            self.first = Component (name='first_component', **kwargs)
            self.second = Component (name='second_component', **kwargs)
    
    class Higher (MultiComponent):
        def __init__ (self, **kwargs):
            super().__init__ (**kwargs)
            self.first = Intermediate (name='first_intermediate', **kwargs)
            self.second = Intermediate (name='second_intermediate', **kwargs)
            self.gather_descendants(nick_name=False)
    
    higher = Higher()
    
    assert sorted(higher.obj.keys())==['first_component', 'first_intermediate', 'second_component', 'second_intermediate']
    
    # check types
    types = map(lambda x: type(x[1]), sorted(higher.obj.items()))
    assert list(types)==[list, Intermediate, list, Intermediate]
    
    sorted_list = list(sorted(higher.obj.items()))
    types = map(type, sorted_list[0][1])
    assert list(types)==[Component,Component]
    
    sorted_list = list(sorted(higher.obj.items()))
    types = map(type, sorted_list[2][1])
    assert list(types)==[Component,Component]
    
    sorted_keys=list(sorted(higher.cls.keys()))
    assert sorted_keys == ['FirstComponent', 'Intermediate', 'SecondComponent']
    
    assert list(map(type,higher.cls[sorted_keys[0]]))==[Component, Component]
    
    assert list(map(type,higher.cls[sorted_keys[1]]))==[Intermediate, Intermediate]
    
    
    # ***********************
    # recursive behaviour: higher.first
    intermediate = higher.first
    assert sorted(intermediate.obj.keys())==['first_component', 'second_component']
    
    # check types
    types = map(lambda x: type(x[1]), sorted(intermediate.obj.items()))
    assert list(types)==[Component, Component]
    
    sorted_keys=list(sorted(intermediate.cls.keys()))
    assert sorted_keys==['FirstComponent', 'SecondComponent']
    
    assert type (intermediate.cls[sorted_keys[0]]) == Component
    
    # **********************************************
    # recursive behaviour: higher.second
    # **********************************************
    intermediate = higher.second
    assert sorted(intermediate.obj.keys())==['first_component', 'second_component']
    
    # check types
    types = map(lambda x: type(x[1]), sorted(intermediate.obj.items()))
    assert list(types)==[Component, Component]
    
    sorted_keys=list(sorted(intermediate.cls.keys()))
    assert sorted_keys==['FirstComponent', 'SecondComponent']
    
    assert type(intermediate.cls[sorted_keys[0]])==Component
    
    # **********************************************
    # full hierarchical paths
    # **********************************************
    assert list(sorted(higher.full_cls.keys()))==['FirstComponent', 'Intermediate', 'SecondComponent']
    assert higher.full_cls['Intermediate']==['higher.first_intermediate', 'higher.second_intermediate']
    assert higher.full_cls['FirstComponent']==['higher.first_intermediate.first_component', 
                                               'higher.second_intermediate.first_component']
    assert higher.full_cls['SecondComponent']==['higher.first_intermediate.second_component', 
                                                'higher.second_intermediate.second_component']
    
    assert higher.first.full_cls['FirstComponent']=='higher.first_intermediate.first_component'
    assert higher.first.full_cls['SecondComponent']=='higher.first_intermediate.second_component'
    
    assert list(sorted(higher.full_obj))==['first_component', 'first_intermediate', 'second_component', 'second_intermediate']
    
    assert higher.full_obj['first_intermediate']=='higher.first_intermediate'
    
    assert higher.full_obj['first_component']==['higher.first_intermediate.first_component',
      'higher.second_intermediate.first_component']
    
    assert higher.full_obj['second_component']==['higher.first_intermediate.second_component',
      'higher.second_intermediate.second_component']
    
    assert higher.full_obj['second_intermediate']=='higher.second_intermediate'
    
    assert list(sorted(higher.second.full_obj))==['first_component', 'second_component']
    
    assert higher.second.full_obj['first_component']=='higher.second_intermediate.first_component'
    
    assert higher.second.full_obj['second_component']=='higher.second_intermediate.second_component'
    
    assert higher.hierarchy_path=='higher'
    
    assert higher.first.hierarchy_path=='higher.first_intermediate'
    
    # with nick_names
    higher.clear_descendants()
    higher.gather_descendants(nick_name=True)
    
    assert higher.full_cls['FirstComponent']==['higher.first.first', 'higher.second.first']
    assert higher.full_cls['SecondComponent']==['higher.first.second', 'higher.second.second']
    
    assert higher.full_obj['first_intermediate']=='higher.first'
    assert higher.full_obj['first_component']==['higher.first.first', 'higher.second.first']
    assert higher.full_obj['second_component']==['higher.first.second', 'higher.second.second']
    assert higher.full_obj['second_intermediate']=='higher.second'
    
    #Check that we always have attributes for each component name
    assert higher.first_intermediate is higher.first
    assert higher.second_intermediate is higher.second
    assert higher.components==[higher.first, higher.second]
    
    #check that set_components and add_component create self attrs called 
    # the same as the component
    
    class Higher (MultiComponent):
        def __init__ (self, **kwargs):
            super().__init__ (**kwargs)
            self.set_components (Intermediate (name='first_intermediate', **kwargs),
                                 Intermediate (name='second_intermediate', **kwargs))
    higher = Higher()
    
    assert higher.components == (higher.first_intermediate, higher.second_intermediate)
    
    class Higher (MultiComponent):
        def __init__ (self, **kwargs):
            super().__init__ (**kwargs)
            self.add_component (Intermediate (name='first_intermediate', **kwargs))
            self.add_component (Intermediate (name='second_intermediate', **kwargs))
    higher = Higher()
    
    assert higher.components == [higher.first_intermediate, higher.second_intermediate]


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 22
def test_multi_comp_desc2 ():
    # second example
    class Intermediate (MultiComponent):
        def __init__ (self, name=None, **kwargs):
            super().__init__ (name=name, **kwargs)
            self.first = Component (name=f'{name}_first_component', **kwargs)
            self.second = Component (name=f'{name}_second_component', **kwargs)
    
    class Higher (MultiComponent):
        def __init__ (self, **kwargs):
            super().__init__ (**kwargs)
            self.first = Intermediate (name='first_intermediate', **kwargs)
            self.second = Intermediate (name='second_intermediate', **kwargs)
            self.gather_descendants(nick_name=False)
    
    
    higher = Higher()
    
    assert sorted(higher.obj.keys())==['first_intermediate', 'first_intermediate_first_component', 'first_intermediate_second_component', 'second_intermediate', 'second_intermediate_first_component', 'second_intermediate_second_component']
    
    # check types
    types = map(lambda x: type(x[1]), sorted(higher.obj.items()))
    
    assert list(types)==[Intermediate, Component, Component, Intermediate, Component, Component]
    
    sorted_keys=list(sorted(higher.cls.keys()))
    assert sorted_keys == ['FirstIntermediateFirstComponent', 'FirstIntermediateSecondComponent', 'Intermediate', 
                           'SecondIntermediateFirstComponent', 'SecondIntermediateSecondComponent']
    
    assert type (higher.cls[sorted_keys[0]])==Component
    assert type (higher.cls[sorted_keys[1]])==Component
    assert list(map(type,higher.cls[sorted_keys[2]]))==[Intermediate, Intermediate]
    
    # ***********************
    # recursive behaviour: higher.first
    intermediate = higher.first
    assert sorted(intermediate.obj.keys())==['first_intermediate_first_component', 'first_intermediate_second_component']
    
    # check types
    types = map(lambda x: type(x[1]), sorted(intermediate.obj.items()))
    assert list(types)==[Component, Component]
    
    sorted_keys=list(sorted(intermediate.cls.keys()))
    assert sorted_keys==['FirstIntermediateFirstComponent', 
                         'FirstIntermediateSecondComponent']
    
    assert type (intermediate.cls[sorted_keys[0]])==Component
    
    # ***********************
    # recursive behaviour: higher.second
    intermediate = higher.second
    assert sorted(intermediate.obj.keys())==['second_intermediate_first_component', 'second_intermediate_second_component']
    
    # check types
    types = map(lambda x: type(x[1]), sorted(intermediate.obj.items()))
    assert list(types)==[Component, Component]
    
    sorted_keys=list(sorted(intermediate.cls.keys()))
    assert sorted_keys==['SecondIntermediateFirstComponent', 'SecondIntermediateSecondComponent']
    
    assert type (intermediate.cls[sorted_keys[0]])==Component
    
    
    # **********************************************
    # full hierarchical paths
    # **********************************************
    assert list(sorted(higher.full_cls))==['FirstIntermediateFirstComponent', 'FirstIntermediateSecondComponent',
                                           'Intermediate', 'SecondIntermediateFirstComponent', 
                                           'SecondIntermediateSecondComponent']
    
    assert higher.full_cls['FirstIntermediateFirstComponent']=='higher.first_intermediate.first_intermediate_first_component'
    assert higher.full_cls['FirstIntermediateSecondComponent']=='higher.first_intermediate.first_intermediate_second_component'
    assert higher.full_cls['SecondIntermediateFirstComponent']=='higher.second_intermediate.second_intermediate_first_component'
    assert higher.full_cls['SecondIntermediateSecondComponent']=='higher.second_intermediate.second_intermediate_second_component'
    
    assert higher.full_cls['Intermediate']==['higher.first_intermediate', 'higher.second_intermediate']
    
    assert list(higher.first.full_cls)==['FirstIntermediateFirstComponent', 'FirstIntermediateSecondComponent']
    
    assert higher.first.full_cls['FirstIntermediateFirstComponent']=='higher.first_intermediate.first_intermediate_first_component'
    assert higher.first.full_cls['FirstIntermediateSecondComponent']=='higher.first_intermediate.first_intermediate_second_component'
    
    assert sorted(list(higher.full_obj))==['first_intermediate',
     'first_intermediate_first_component',
     'first_intermediate_second_component',
     'second_intermediate',
     'second_intermediate_first_component',
     'second_intermediate_second_component']
    
    assert higher.full_obj['first_intermediate']=='higher.first_intermediate'
    
    assert higher.full_obj['first_intermediate_first_component']=='higher.first_intermediate.first_intermediate_first_component'
    
    assert higher.full_obj['first_intermediate_second_component']=='higher.first_intermediate.first_intermediate_second_component'
    
    assert higher.full_obj['second_intermediate']=='higher.second_intermediate'
    
    assert higher.full_obj['second_intermediate_first_component']=='higher.second_intermediate.second_intermediate_first_component'
    
    assert higher.full_obj['second_intermediate_second_component']=='higher.second_intermediate.second_intermediate_second_component'
    
    assert list(sorted(higher.second.full_obj))==['second_intermediate_first_component', 'second_intermediate_second_component']
    
    assert higher.second.full_obj['second_intermediate_first_component']=='higher.second_intermediate.second_intermediate_first_component'
    
    assert higher.second.full_obj['second_intermediate_second_component']=='higher.second_intermediate.second_intermediate_second_component'
    
    assert higher.hierarchy_path=='higher'
    
    assert higher.first.hierarchy_path=='higher.first_intermediate'


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 23
def test_list ():
    higher = Higher (Higher=dict(root=True))
    higher.list()


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 24
def test_show ():
    multi_comp = MultiComponent (Component(class_name='A'), Component(class_name='B'))
    multi_comp.show(color_lines=False)
    multi_comp = MultiComponent (Component(class_name='A'), 
                                 MultiComponent (Component (class_name='S1'),
                                                 Component (class_name='S2'),
                                                 class_name='MC'),
                                 Component(class_name='B'))
    multi_comp.show (color_lines=False)
    multi_comp.show ()
    multi_comp = MultiComponent (Component(class_name='A'), 
                                 MultiComponent (Component (class_name='S1'),
                                                 Component (class_name='S2'),
                                                 MultiComponent (Component (class_name='T1'),
                                                                 Component (class_name='T2'),
                                                                 class_name='MC2'),
                                                 MultiComponent (MultiComponent(Component (class_name='U1'),
                                                                                Component (class_name='U2')),
                                                                 Component(class_name='W'),
                                                                 class_name='MC3'),
                                                 class_name='MC'),
                                 Component(class_name='B'))
    multi_comp.show ()
    print ('\nSecond\n')
    multi_comp.show(vline=False)
    
    print ('\nThird\n')
    multi_comp.show(max_level=1, vline=False)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 25
def test_gather_and_save_info ():
    higher = Higher(x=20, y=30, z=60)
    
    path_results = 'test_gather_and_save_info'
    path_session = f'{path_results}_session'
    remove_previous_results (path_results)
    remove_previous_results (path_session)
    with pytest.raises (FileNotFoundError):
        os.listdir (path_session)
    
    higher.gather_and_save_info (path_session=path_session)
    
    assert os.listdir (path_session)==['last_run']
    assert sorted(os.listdir (f'{path_session}/last_run'))==sorted(['higher.pk', 'pipeline.pk'])
    
    pipe = joblib.load (f'{path_session}/last_run/pipeline.pk')
    assert pipe.x==20
    assert pipe.first.z==60
    assert pipe.first.x==3
    
    with pytest.raises (FileNotFoundError):
        os.listdir (path_results)
    
    remove_previous_results (path_session)
    higher = Higher(x=20, y=30, z=60, path_results=path_results)
    
    path_results = 'test_gather_and_save_info'
    path_session = f'{path_results}_session'
    
    with pytest.raises (FileNotFoundError):
        os.listdir (path_session)
    
    higher.gather_and_save_info (path_session=path_session)
    
    assert os.listdir (path_session)==['last_run']
    assert sorted(os.listdir (f'{path_session}/last_run'))==sorted(['higher.pk', 'pipeline.pk'])
    pipe = joblib.load (f'{path_session}/last_run/pipeline.pk')
    assert pipe.x==20
    assert pipe.first.z==60
    assert pipe.first.x==3
    
    assert 'pipeline.pk' in os.listdir (path_results)
    pipe = joblib.load (f'{path_results}/pipeline.pk')
    assert pipe.x==20
    assert pipe.first.z==60
    assert pipe.first.x==3
    
    remove_previous_results (path_results)
    remove_previous_results (path_session)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 26
def test_multi_comp_hierarchy ():
    higher = Higher()
    
    levels=dict(
        until=1,
        verbose=1
    )
    higher = Higher (levels=levels, verbose=0)
    
    assert higher.hierarchy_level==0 and higher.first.hierarchy_level==1 and higher.first.first.hierarchy_level==2
    
    assert higher.verbose==1 and higher.first.verbose==1 and higher.first.first.verbose==0
    
    levels['until']=0
    higher = Higher (levels=levels, verbose=0)
    assert higher.verbose==1 and higher.first.verbose==0 and higher.first.first.verbose==0
    
    levels['until']=2
    higher = Higher (levels=levels, verbose=0)
    assert higher.verbose==1 and higher.first.verbose==1 and higher.first.first.verbose==1


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 27
def test_multi_comp_profiling ():
    class A(Component):
        def __init__ (self, time=1, **kwargs):
            super().__init__(**kwargs)
    
        def _fit (self, X, y=None):
            time.sleep(self.time*2)
    
        def _apply (self, X):
            time.sleep(self.time)
            return 1
    
    class Intermediate (MultiComponent):
        def __init__ (self, name=None, **kwargs):
            super().__init__ (name=name, **kwargs)
            self.first = A (name=f'{name}_first_component', time=0.01, **kwargs)
            self.second = A (name=f'{name}_second_component', time=0.03, **kwargs)
        def _fit (self, X, y=None):
            self.first.fit (X,y)
            self.second.fit (X,y)
        def _apply (self, X):
            _ = self.first.apply (X)
            _ = self.second.apply (X)
            return 1
    
    class Higher (MultiComponent):
        def __init__ (self, **kwargs):
            super().__init__ (**kwargs)
            self.first = Intermediate (name='first', **kwargs)
            self.second = Intermediate (name='second', **kwargs)
        def _fit (self, X, y=None):
            self.first.fit (X,y)
            self.second.fit (X,y)
        def _apply (self, X):
            _ = self.first.apply (X)
            _ = self.second.apply (X)
            return 1
    
    higher = Higher()
    higher.fit (1)
    _ = higher.apply (1)
    dfd = higher.gather_times()
    
    values = dfd.avg[('whole','apply')].values
    assert np.abs(values[1:3].sum() - values[0]) < 0.1
    assert np.abs(values[3:5].sum() - values[1]) < 0.1
    assert np.abs(values[5:7].sum() - values[2]) < 0.1
    
    values = dfd.avg[('whole','fit')].values
    assert np.abs(values[1:3].sum() - values[0]) < 0.1
    assert np.abs(values[3:5].sum() - values[1]) < 0.1
    assert np.abs(values[5:7].sum() - values[2]) < 0.1
    
    display('avg', dfd.avg)
    
    assert (dfd.novh_avg <= dfd.avg).all().all()
    assert (dfd.novh_avg < dfd.avg).any().any()
    
    assert ((dfd.novh_avg.iloc[-4:].sum(axis=0).to_frame().T) == dfd.no_overhead_total).all(axis=1).all()
    
    assert ((dfd.avg.iloc[0]-dfd.novh_avg.iloc[-4:].sum(axis=0)).values == dfd.overhead_total.values).all()
    
    display('no_overhead_total', dfd.no_overhead_total)
    display('overhead_total', dfd.overhead_total)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 28
def test_multi_comp_all_equal ():
    path_results = 'multi_component_assert_equal'
    remove_previous_results (path_results=path_results)
    
    # 1. by setting components as attributes:
    class NewComposition(MultiComponent):
    
        def __init__ (self, noise = 0, **kwargs):
            super().__init__(**kwargs)
    
            self.tr1 = Component(FunctionTransformer (lambda x: x*3 + noise),
                                 name='tr1',
                                 **kwargs)
            self.tr2 = Component(FunctionTransformer (lambda x: x*2),
                                 name='tr2',
                                 **kwargs)
    
        def _apply (self, X):
            return self.tr1 (X) + self.tr2(X)
    
    X = np.array([1,2,3])
    
    composition1 = NewComposition (path_results=path_results)
    result1 = composition1 (X)
    
    path_results2 = 'multi_component_assert_equal_2'
    remove_previous_results (path_results=path_results2)
    composition2 = NewComposition (path_results=path_results2)
    result2 = composition2 (X)
    assert composition1.assert_all_equal (path_results2)
    
    remove_previous_results (path_results=path_results2)
    composition2 = NewComposition (path_results=path_results2, noise=0.1)
    result2 = composition2 (X)
    assert not composition1.assert_all_equal (path_results2)
    
    # *************************
    # check verbosity
    # *************************
    composition1 = NewComposition (path_results=path_results, verbose=1)
    composition1.logger.info ('\n******************************')
    composition1.logger.info ('verbose')
    composition1.logger.info ('******************************')
    assert not composition1.assert_all_equal (path_results2)
    
    composition1.logger.info ('\n******************************')
    composition1.logger.info ('not verbose')
    composition1.logger.info ('******************************')
    assert not composition1.assert_all_equal (path_results2, verbose=0)
    composition1.logger.info ('logger works again')
    
    # *******************************
    # check recursion
    # *******************************
    class NewComposition (MultiComponent):
    
        def __init__ (self, noise = 0, **kwargs):
            super().__init__(**kwargs)
    
            self.tr1 = Component(FunctionTransformer (lambda x: x*2 + noise),
                                 name='tr1',
                                 **kwargs)
            self.tr2 = Component(FunctionTransformer (lambda x: x*3),
                                 name='tr2',
                                 **kwargs)
    
        def _apply (self, X):
            return self.tr1 (X) + self.tr2(X)
    
    path_results3 = 'multi_component_assert_equal_3'
    composition3 = NewComposition (path_results=path_results3)
    result3 = composition3 (X)
    assert not composition3.assert_all_equal (path_results)
    assert composition3.assert_all_equal (path_results, max_recursion=0)
    assert not composition3.assert_all_equal (path_results, max_recursion=1)
    
    class NewComposition (MultiComponent):
    
        def __init__ (self, noise = 0, **kwargs):
            super().__init__(**kwargs)
    
            self.tr1 = Component(FunctionTransformer (lambda x: x*4 + noise),
                                 name='tr1',
                                 **kwargs)
            self.tr2 = Component(FunctionTransformer (lambda x: x*5),
                                 name='tr2',
                                 **kwargs)
    
        def _apply (self, X):
            return self.tr1 (X) + self.tr2(X)
    
    remove_previous_results (path_results=path_results3)
    composition4 = NewComposition (path_results=path_results3)
    result3 = composition4 (X)
    assert not composition4.assert_all_equal (path_results)
    assert not composition4.assert_all_equal (path_results, max_recursion=0)
    assert not composition4.assert_all_equal (path_results, max_recursion=1)
    
    # *************************
    # remove results
    # *************************
    remove_previous_results (path_results=path_results)
    remove_previous_results (path_results=path_results2)
    remove_previous_results (path_results=path_results3)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 29
def test_multi_component_setters ():
    multi_component = SimpleMultiComponent ()
    
    all_true = lambda x: all([x.data_io.save_splits[k] for k in x.data_io.save_splits])
    assert all_true (multi_component)
    
    c0, c1 = multi_component.components[0], multi_component.components[1]
    
    assert all_true (c0) and all_true (c1)
    
    multi_component.set_save_splits ({'training': True, 'test': False, 'validation': False, 'whole': True})
    
    tv_false = lambda x: (all ([x.data_io.save_splits[k] for k in ['training', 'whole']])
                          and all ([(not x.data_io.save_splits[k]) for k in ['test', 'validation']]))
    assert tv_false(c0) and tv_false (c1) and tv_false (multi_component)
    
    cond = lambda x: x.data_io.load_model_flag
    assert cond (c0) and cond (c1) and cond (multi_component)
    
    multi_component.set_load_model (False)
    cond = lambda x: not x.data_io.load_model_flag
    assert cond (c0) and cond (c1) and cond (multi_component)
    
    cond = lambda x: x.data_io.save_model_flag
    assert cond (c0) and cond (c1) and cond (multi_component)
    
    multi_component.set_save_model (False)
    cond = lambda x: not x.data_io.save_model_flag
    assert cond (c0) and cond (c1) and cond (multi_component)
    
    #set_save_result
    cond = lambda x: x.data_io.save_result_flag
    assert cond (c0) and cond (c1) and cond (multi_component)
    
    multi_component.set_save_result (False)
    cond = lambda x: not x.data_io.save_result_flag
    assert cond (c0) and cond (c1) and cond (multi_component)
    
    # set_load_result
    cond = lambda x: x.data_io.load_result_flag
    assert cond (c0) and cond (c1) and cond (multi_component)
    
    multi_component.set_load_result (False)
    cond = lambda x: not x.data_io.load_result_flag
    assert cond (c0) and cond (c1) and cond (multi_component)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 30
def test_show_result_statistics ():
    path_results = 'show_result_statistics'
    remove_previous_results (path_results=path_results)
    multi_component = SimpleMultiComponent (path_results=path_results)
    X=np.array([1,2,3])
    r = multi_component(X)
    multi_component.show_result_statistics();
    
    remove_previous_results (path_results=path_results)
    X=pd.DataFrame({'a':[4,5,6], 'b':[7,8,9]})
    r = multi_component(X)
    multi_component.show_result_statistics();
    remove_previous_results (path_results=path_results)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 31
def test_pass_components ():
    config = dict (path_results='my_path', Second=dict(path_results='other_path'))
    multi = MultiComponent (Component (name='first', class_name='First', folder='one', **config),
                            Component (name='second', class_name='Second', folder='two', **config),
                            name='Inner',
                            **config)
    
    check_last_part (multi.path_results, 'my_path')
    check_last_part (multi.second.path_results, 'other_path')
    check_last_part (multi.first.path_results, 'my_path')
    
    assert multi.first.data_io.folder=='one'
    assert multi.second.data_io.folder=='two'
    
    def make_inner (folder, name, **kwargs):
        return MultiComponent (Component (name='first', class_name='First', folder='folder_first', **kwargs),
                               Component (name='second', class_name='Second', folder='folder_second', **kwargs),
                               folder=folder,
                               class_name='Inner',
                               name=name,
                               **kwargs)
    multi = MultiComponent (make_inner ('one', 'inner1', **config), make_inner ('two', 'inner2', **config), **config)
    
    check_last_part (multi.inner1.first.data_io.get_path_result_file(), 
        'my_path/one/folder_first/whole/first_result.pk')
    
    check_last_part (multi.inner1.second.data_io.get_path_result_file(),
        'other_path/one/folder_second/whole/second_result.pk')
    
    check_last_part (multi.inner2.first.data_io.get_path_result_file(), 
        'my_path/two/folder_first/whole/first_result.pk')
    
    check_last_part (multi.inner2.second.data_io.get_path_result_file(), 
        'other_path/two/folder_second/whole/second_result.pk')
    
    config = dict (path_results='my_path', Inner=dict(path_results='other_path'))
    def make_inner (folder, name, **kwargs):
        return MultiComponent (Component (name='first', class_name='First', folder='folder_first', **kwargs),
                               Component (name='second', class_name='Second', folder='folder_second', **kwargs),
                               folder=folder,
                               class_name='Inner',
                               name=name,
                               **kwargs)
    multi = MultiComponent (make_inner ('one', 'inner1', **config), 
                            make_inner ('two', 'inner2', **config), 
                            folder='__class__',
                            class_name='Higher',
                            name='higher',
                            **config)
    
    check_last_part (multi.inner1.first.data_io.get_path_result_file(), 
        'my_path/higher/one/folder_first/whole/first_result.pk')
    
    check_last_part (multi.inner1.second.data_io.get_path_result_file(), 
        'my_path/higher/one/folder_second/whole/second_result.pk')
    
    check_last_part (multi.inner2.first.data_io.get_path_result_file(), 
        'my_path/higher/two/folder_first/whole/first_result.pk')
    
    check_last_part (multi.inner2.second.data_io.get_path_result_file(), 
        'my_path/higher/two/folder_second/whole/second_result.pk')
    
    check_last_part (multi.inner1.data_io.get_path_result_file(), 
        'other_path/higher/one/whole/inner1_result.pk')
    
    check_last_part (multi.inner2.data_io.get_path_result_file(), 
        'other_path/higher/two/whole/inner2_result.pk')
    
    multi = MultiComponent (make_inner ('one', 'inner1', **config, propagate=True), 
                            make_inner ('two', 'inner2', **config, propagate=True), 
                            folder='__class__',
                            class_name='Higher',
                            name='higher',
                            **config)
    
    check_last_part (multi.inner1.first.data_io.get_path_result_file(), 
        'other_path/higher/one/folder_first/whole/first_result.pk')
    
    check_last_part (multi.inner1.second.data_io.get_path_result_file(), 
                     'other_path/higher/one/folder_second/whole/second_result.pk')
    
    check_last_part (multi.inner2.first.data_io.get_path_result_file(), 
                     'other_path/higher/two/folder_first/whole/first_result.pk')
    
    check_last_part (multi.inner2.second.data_io.get_path_result_file(), 
                     'other_path/higher/two/folder_second/whole/second_result.pk')
    
    check_last_part (multi.inner1.data_io.get_path_result_file(), 
                     'other_path/higher/one/whole/inner1_result.pk')
    
    check_last_part (multi.inner2.data_io.get_path_result_file(), 
                     'other_path/higher/two/whole/inner2_result.pk')
    
    check_last_part (multi.data_io.get_path_result_file(), 
                     'my_path/higher/whole/higher_result.pk')
    
    multi = MultiComponent (make_inner ('one', 'inner1', **config), 
                            make_inner ('two', 'inner2', **config), 
                            folder='__class__',
                            class_name='Higher',
                            name='higher',
                            propagate=True,
                            **config)
    
    check_last_part (multi.inner1.first.data_io.get_path_result_file(), 
                     'my_path/higher/one/folder_first/whole/first_result.pk')
    
    check_last_part (multi.inner1.second.data_io.get_path_result_file(), 
                     'my_path/higher/one/folder_second/whole/second_result.pk')
    
    check_last_part (multi.inner2.first.data_io.get_path_result_file(), 
                     'my_path/higher/two/folder_first/whole/first_result.pk')
    
    check_last_part (multi.inner2.second.data_io.get_path_result_file(), 
                     'my_path/higher/two/folder_second/whole/second_result.pk')
    
    check_last_part (multi.inner1.data_io.get_path_result_file(), 
                     'my_path/higher/one/whole/inner1_result.pk')
    
    check_last_part (multi.inner2.data_io.get_path_result_file(), 
                     'my_path/higher/two/whole/inner2_result.pk')
    
    check_last_part (multi.data_io.get_path_result_file(), 
                     'my_path/higher/whole/higher_result.pk')
    
    config = dict (path_results='my_path', Inner=dict(path_results='other_path', stop_propagation=True))
    multi = MultiComponent (make_inner ('one', 'inner1', **config, propagate=True), 
                            make_inner ('two', 'inner2', **config, propagate=True), 
                            folder='__class__',
                            class_name='Higher',
                            name='higher',
                            propagate=True,
                            **config)
    
    check_last_part (multi.inner1.first.data_io.get_path_result_file(), 
                     'other_path/higher/one/folder_first/whole/first_result.pk')
    
    check_last_part (multi.inner1.second.data_io.get_path_result_file(), 
                     'other_path/higher/one/folder_second/whole/second_result.pk')
    
    check_last_part (multi.inner2.first.data_io.get_path_result_file(), 
                     'other_path/higher/two/folder_first/whole/first_result.pk')
    
    check_last_part (multi.inner2.second.data_io.get_path_result_file(), 
                     'other_path/higher/two/folder_second/whole/second_result.pk')
    
    check_last_part (multi.inner1.data_io.get_path_result_file(), 
                     'other_path/higher/one/whole/inner1_result.pk')
    
    check_last_part (multi.inner2.data_io.get_path_result_file(), 
                     'other_path/higher/two/whole/inner2_result.pk')
    
    check_last_part (multi.data_io.get_path_result_file(), 
                     'my_path/higher/whole/higher_result.pk')


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 32
def test_chain_folders ():
    config = dict (path_results='my_path', Second=dict(path_results='other_path'))
    def first_level ():
        a0=Component (name='first', class_name='First', folder='folder_first', **config)
        b0=Component (name='second', class_name='Second', folder='folder_second', **config)
        return a0, b0
    
    a0, b0 = first_level()
    
    assert a0.data_io.folder=='folder_first'
    
    def second_level ():
        a0, b0 = first_level ()
        a1= MultiComponent (a0, b0, folder='one', class_name='Inner', name='inner1', **config)
        a0, b0 = first_level ()
        b1= MultiComponent (a0, b0, folder='two', class_name='Inner', name='inner2', **config)
        return a1, b1
    
    a1, b1 = second_level ()
    
    assert a1.first.data_io.folder=='one/folder_first'
    assert a1.second.data_io.folder=='one/folder_second'
    assert b1.first.data_io.folder=='two/folder_first'
    assert b1.second.data_io.folder=='two/folder_second'
    
    def third_level ():
        a1, b1 = second_level ()
        a2= MultiComponent (a1, b1, folder='third1', class_name='Higher', name='higher1', **config)
        a1, b1 = second_level ()
        b2= MultiComponent (a1, b1, folder='third2', class_name='Higher', name='higher2', **config)
        return a2, b2
    
    a2, b2 = third_level ()
    
    assert a2.inner1.first.data_io.folder=='third1/one/folder_first'
    assert a2.inner1.second.data_io.folder=='third1/one/folder_second'
    assert b2.inner1.first.data_io.folder=='third2/one/folder_first'
    assert b2.inner1.second.data_io.folder=='third2/one/folder_second'
    assert b2.inner2.first.data_io.folder=='third2/two/folder_first'
    assert b2.inner2.second.data_io.folder=='third2/two/folder_second'


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 33
def test_set_root ():
    config = dict (path_results='my_path', Second=dict(path_results='other_path'))
    def first_level ():
        a0=Component (name='first', class_name='First', folder='folder_first', **config)
        b0=Component (name='second', class_name='Second', folder='folder_second', **config)
        return a0, b0
    
    a0, b0 = first_level()
    
    def second_level ():
        a0, b0 = first_level ()
        a1= MultiComponent (a0, b0, folder='one', class_name='Inner', name='inner1', **config)
        a0, b0 = first_level ()
        b1= MultiComponent (a0, b0, folder='two', class_name='Inner', name='inner2', **config)
        return a1, b1
    
    a1, b1 = second_level ()
    
    def third_level ():
        a1, b1 = second_level ()
        a2= MultiComponent (a1, b1, folder='third1', class_name='Higher', name='higher1', **config)
        a1, b1 = second_level ()
        b2= MultiComponent (a1, b1, folder='third2', class_name='Higher', name='higher2', **config)
        return a2, b2
    
    a2, b2 = third_level ()
    c = MultiComponent (a2, b2)
    assert [x.name for x in c.higher1.inner1.components] == ['first', 'second']
    assert [x.name for x in c.higher1.inner2.components] == ['first', 'second']
    
    c = MultiComponent (a2, b2, root=True)
    assert [x.name for x in c.higher1.inner1.components] == ['first', 'second']
    assert [x.name for x in c.higher1.inner2.components] == ['first_1', 'second_1']
    assert [x.data_io.fitting_file_name for x in c.higher1.inner2.components] == ['first_1_estimator.pk', 'second_1_estimator.pk']
    assert [x.data_io.result_file_name for x in c.higher1.inner2.components] == ['first_1_result.pk', 'second_1_result.pk']


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 34
def test_set_root_2 ():
    class Higher (MultiComponent):
        def __init__ (self, x=2, y=3, **kwargs):
            self.first = Intermediate (name='first_intermediate', **kwargs)
            self.second = Intermediate (name='second_intermediate', **kwargs)
            super().__init__ (**kwargs)
    
    higher = Higher (Higher=dict(root=True))
    assert higher.second_intermediate.root is higher


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 35
def test_pass_functions_to_multi_component ():
    def myf (x): return x*2
    pipe = MultiComponent (Sum1 (), myf, lambda x: x+3, (lambda x: x-1, 'Minus'), DummyEstimator () )
    X = np.array ([1,2,3])
    r = X
    pipe.components[-1].fit (X)
    for c in pipe.components:
        r = c (r)
    assert (r== ( ((X+1)*2+3-1)*3+X.sum() )).all()
    class_names = ['Sum1', 'Myf', '<Lambda>', 'Minus', 'DummyEstimator']
    assert all([x.class_name==y for x, y in zip(pipe.components, class_names)])


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 36
def test_pass_tuples_classes ():
    def myf (x): return x*2
    multi_comp = MultiComponent (Sum1, 
                                 (MultiComponent, myf, Sum1), 
                                 (MultiComponent, (MultiComponent, Sum1, Sum1),
                                  myf),
                                 myf,
                                 path_results='test_pass_tuples_classes', load=False)
    
    multi_comp.show()
    assert [x.class_name for x in multi_comp.components]== ['Sum1', 'MultiComponent', 'MultiComponent', 'Myf']
    assert [x.class_name for x in multi_comp.components[1].components]==['Myf', 'Sum1']
    assert [x.class_name for x in multi_comp.components[2].components]==['MultiComponent', 'Myf']
    assert [x.class_name for x in multi_comp.components[2].components[0].components]==['Sum1', 'Sum1']
    multi_comp = MultiComponent ((MultiComponent, myf, Sum1, 'FuncSum'), Sum1, 
                                 name='top')
    multi_comp.show()
    assert multi_comp.class_name=='Top'
    assert [x.class_name for x in multi_comp.components]==['FuncSum', 'Sum1']
    assert [x.class_name for x in multi_comp.components[0].components]==['Myf', 'Sum1']


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 37
def test_set_suffix ():
    higher = Higher ()
    assert higher.name=='higher'
    assert higher.first_intermediate.name=='first_intermediate'
    assert higher.second_intermediate.name=='second_intermediate'
    assert higher.second_intermediate.first_component.name == 'first_component'
    assert higher.second_intermediate.second_component.name == 'second_component'
    
    higher.set_suffix ('a')
    assert higher.name=='higher_a'
    assert higher.first_intermediate.name=='first_intermediate_a'
    assert higher.second_intermediate.name=='second_intermediate_a'
    assert higher.second_intermediate.first_component.name == 'first_component_a'
    assert higher.second_intermediate.second_component.name == 'second_component_a'
    
    higher.set_suffix ('b')
    assert higher.name=='higher_b'
    assert higher.first_intermediate.name=='first_intermediate_b'
    assert higher.second_intermediate.name=='second_intermediate_b'
    assert higher.second_intermediate.first_component.name == 'first_component_b'
    assert higher.second_intermediate.second_component.name == 'second_component_b'
    
    higher.set_suffix ('c')
    assert higher.name=='higher_c'
    assert higher.first_intermediate.name=='first_intermediate_c'
    assert higher.second_intermediate.name=='second_intermediate_c'
    assert higher.second_intermediate.first_component.name == 'first_component_c'
    assert higher.second_intermediate.second_component.name == 'second_component_c'


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 38
def test_multi_comp_propagate_args ():
    first_intermediate = MultiComponent (Component (name='first_component'), Component (name='second_component'),
                                         name='first_intermediate')
    second_intermediate = MultiComponent (Component (name='first_component'), Component (name='second_component'),
                                         name='second_intermediate')
    higher = MultiComponent (first_intermediate, second_intermediate)
    display (higher.data_converter)
    
    higher = MultiComponent (first_intermediate, second_intermediate,
                             data_converter='PandasConverter')
    display (higher.data_converter)
    
    higher.first_intermediate.data_converter
    
    higher = MultiComponent (first_intermediate, second_intermediate,
                             data_converter='PandasConverter',
                             rebuild=True)
    display (higher.data_converter)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 39
def test_pass_no_component_class ():
    mc = MultiComponent (Sum1, DummyEstimator)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 40
def test_compare_kwargs_compose ():
    class Intermediate2 (MultiComponent):
        def __init__ (self, name=None, z=6, h=10, x=3, **kwargs):
            self.first = Component (name='first_component', **kwargs)
            self.second = Component (name='second_component', **kwargs)
            super().__init__ (name=name, **kwargs)
    
    class Higher2 (MultiComponent):
        def __init__ (self, x=2, y=3, compare_kwargs=dflt.compare_kwargs,
                      error_if_kwargs_mismatch=dflt.error_if_kwargs_mismatch, **kwargs):
            self.first = Intermediate2 (name='first_intermediate', **kwargs)
            self.second = Intermediate2 (name='second_intermediate', **kwargs)
            super().__init__ (compare_kwargs=compare_kwargs, error_if_kwargs_mismatch=error_if_kwargs_mismatch, 
                              **kwargs)
            
    h = Higher2 (y=4, z=2, h=40, verbose=2, error_if_kwargs_mismatch=True)
    with pytest.raises (ValueError):
        h = Higher2 (y=4, z=2, hs=40, verbose=2, error_if_kwargs_mismatch=True)
    h = Higher2 (y=4, z=2, hs=40, verbose=2, compare_kwargs=True)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 41
def test_propagate_attribute ():
    dummy = MultiComponent (
        MultiComponent ((lambda x:x+1, 'c1'), (lambda y:y+2, 'c2'), name='m1'),
        MultiComponent ((lambda z:z+3, 'c3'), (lambda w:w+4, 'c4'), name='m2'))
    
    assert dummy.logger.name=='no_logging' and dummy.logger.level==30
    
    new_logger=set_logger ('new_logger', verbose=2)
    assert new_logger.name=='new_logger' and new_logger.level==10
    
    assert os.path.exists ('log/new_logger.log')
    
    dummy.propagate_attribute ('logger', new_logger)
    
    assert dummy.logger.name=='new_logger' and dummy.logger.level==10
    
    for c in dummy.components:
        assert c.logger.name=='new_logger' and c.logger.level==10
        for c2 in c.components:
            assert c2.logger.name=='new_logger' and c2.logger.level==10
    dummy = MultiComponent (
        MultiComponent ((lambda x:x+1, 'c1'), (lambda y:y+2, 'c2'), name='m1'),
        MultiComponent ((lambda z:z+3, 'c3'), (lambda w:w+4, 'c4'), name='m2'))
    
    assert dummy.data_io.load_flag and dummy.m1.c1.data_io.load_flag
    
    dummy.propagate_sub_component_attribute ('data_io', 'load_flag', False)
    
    assert not dummy.data_io.load_flag and not dummy.m1.c1.data_io.load_flag


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 42
def test_separate_data ():
    def create_dummy (**kwargs):
        return MultiComponent (
                    MultiComponent ((lambda x:x+1, 'c1'), (lambda y:y+2, 'c2'), name='m1', **kwargs),
                    MultiComponent ((lambda z:z+3, 'c3'), (lambda w:w+4, 'c4'), name='m2', **kwargs),
                    **kwargs)
    
    path_results='test_set_separate_data_results'
    dummy = create_dummy (path_results=path_results)
    
    check_last_part(dummy.path_results,path_results)
    
    check_last_part(dummy.m2.c3.path_results,path_results)
    
    path_data='test_set_separate_data_data'
    dummy.set_separate_data (path_data)
    
    dummy.data_io.memory_io.path_results
    
    assert dummy.data_io.memory_io is not None
    assert dummy.m2.c3.data_io.memory_io is None
    assert dummy.m2.data_io.memory_io is None
    
    check_last_part(dummy.path_results,path_data)
    check_last_part(dummy.m2.c3.path_results,path_data)
    
    dummy = create_dummy (path_results=path_results, memory=True)
    
    assert dummy.data_io.memory_io is not None
    
    assert dummy.m2.data_io.memory_io is not None
    
    dummy.set_separate_data (path_data)
    
    assert dummy.data_io.memory_io is not None
    
    assert dummy.m2.data_io.memory_io is None
    
    check_last_part(dummy.data_io.memory_io.path_results, path_data)
    
    assert dummy.data_io.memory_io.component.components[0].name=='m1'


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 43
def test_pipeline_fit_apply ():
    # test `fit_apply` method
    pipeline = SimplePipeline()
    x = np.array([3,4,5])
    r1 = pipeline.fit_apply (x.reshape(-1,1))
    print (r1)
    
    x1 = x * 1000 + sum(x)
    x2 = x1 * 100 + max(x1)
    assert (r1.ravel()==x2).all()
    
    # *********************************
    # Another way of building a pipeline
    # *********************************
    pipeline = Sequential (Transform1(), 
                           Transform2())
    
    x = np.array([3,4,5])
    r1 = pipeline.fit_apply (x.reshape(-1,1))
    
    x1 = x * 1000 + sum(x)
    x2 = x1 * 100 + max(x1)
    assert (r1.ravel()==x2).all()


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 44
def test_pipeline_fit_apply_bis ():
    # test `fit_apply` method
    class _NewMulti (MultiComponent):
    
        def __init__ (self, **kwargs):
            super().__init__ (**kwargs)
    
            # custom transform
            self.tr1 = Transform1(**kwargs) 
    
            # slklearn transform
            self.tr2 = Transform2(**kwargs) 
    
        def _fit (self, X, y=None):
            self.tr1.fit (X)
            self.tr2.fit (X)
    
        def _apply (self, X, y=None):
            X1=self.tr1.apply (X)
            X2=self.tr2.apply (X)
            return X1+X2
    
    new_multi = _NewMulti()
    x = np.array([3,4,5])
    r2 = new_multi.fit_apply (x)
    print (r2)
    
    x2b = 100 * x + max(x)
    x1 = x * 1000 + sum(x)
    assert (r2.ravel()==(x1 + x2b)).all()


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 45
def test_pipeline_new_comp ():
    # test automatic creation of pipeline components
    
    # 1. by setting components as attributes:
    class NewPipeline_pipeline_new_comp (Pipeline):
        def __init__ (self, **kwargs):
            super().__init__(**kwargs)
            self.tr1 = Component(FunctionTransformer (lambda x: x+1))
            self.tr2 = Component(FunctionTransformer (lambda x: x*2))
    pipeline = NewPipeline_pipeline_new_comp ()
    result = pipeline.transform (3)
    print (result)
    assert result == 8


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 46
def test_pipeline_set_comp ():
    #2. by using `set_components`
    class NewPipeline_pipeline_set_comp (Pipeline):
        def __init__ (self, **kwargs):
            super().__init__(**kwargs)
            tr1 = Component(FunctionTransformer (lambda x: x+1))
            tr2 = Component(FunctionTransformer (lambda x: x*2))
            self.set_components (tr1, tr2)
    
            # the following transform is not added to the pipeline component list:
            self.tr3 = Component(FunctionTransformer (lambda x: x+1))
    
            # The reason is that once set_components is called, the component list 
            # is frozen and inmutable setting new components by attribute doesn't 
            # result in adding them to the component list
    
    pipeline = NewPipeline_pipeline_set_comp()
    result = pipeline.transform (3)
    
    assert result == 8
    assert len(pipeline.components) == 2
    print (result, len(pipeline.components))


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 47
def test_athena_pipeline_training ():
    #3. after calling `set_components()`, we can add new components with `add_component()`
    class NewPipeline_athena_pipeline_training (Pipeline):
        def __init__ (self, **kwargs):
            super().__init__(**kwargs)
            tr1 = Component(FunctionTransformer (lambda x: x+1))
            tr2 = Component(FunctionTransformer (lambda x: x*2))
            self.set_components (tr1, tr2)
    
            tr3 = Component(FunctionTransformer (lambda x: x+2))
            self.add_component(tr3)
    
    pipeline = NewPipeline_athena_pipeline_training ()
    result = pipeline.transform (3)
    
    assert result == 10
    assert len(pipeline.components) == 3
    print (result, len(pipeline.components))


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 48
def test_pipeline_load_estimator ():
    # test `load_estimator` method
    
    # Transform1: custom Transform
    class _Transform1 (PickleSaverComponent):
    
        def __init__ (self, **kwargs):
            super().__init__ (**kwargs)
            self.estimator= Bunch(inv_c = 1)
    
        def _fit (self, X, y=None):
            self.estimator.inv_c = X.ravel()[0]
    
        def _apply (self, x):
            return x / self.estimator.inv_c
    
    class _NewPipeline (Pipeline):
    
        def __init__ (self, **kwargs):
            super().__init__ (**kwargs)
    
            # custom transform
            self.tr1 = _Transform1(**kwargs) 
    
            # slklearn transform
            self.tr2 = PickleSaverComponent(StandardScaler(), **kwargs)
    
        def _fit (self, X, y=None):
            self.tr1.fit (X)
            self.tr2.fit (X)
    
    # remove any previously stored 
    path_results = 'pipeline_loading_saving'
    remove_previous_results (path_results=path_results)
    
    pipeline = _NewPipeline(path_results=path_results, save_test_result=False)
    pipeline.fit (np.array([3,4,5]).reshape(-1,1))
    result1 = pipeline.transform (np.array([300,400,500]).reshape(-1,1))
    print (pipeline.tr2.estimator.mean_)
    
    del pipeline 
    pipeline = _NewPipeline(path_results=path_results, save_test_result=False)
    pipeline.load_estimator ()
    print (pipeline.tr2.estimator.mean_)
    result2 = pipeline.transform (np.array([300,400,500]).reshape(-1,1))
    
    np.testing.assert_array_equal (result1, result2)
    
    # remove stored files resulting from running the current test
    remove_previous_results (path_results=path_results)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 49
def test_construct_diagram ():
    path_results = 'construct_diagram'
    remove_previous_results (path_results=path_results)
    
    # ********************************************
    # example without dimensionality of outputs
    # ********************************************
    pipeline = build_pipeline_construct_diagram_1 (path_results)
    diagram = pipeline.construct_diagram ()
    display (diagram)
    
    # ********************************************
    # example that shows dimensionality of outputs
    # ********************************************
    pipeline = build_pipeline_construct_diagram_2 (path_results)
    X = np.array([1,2,3])
    result = pipeline (X)
    
    diagram = pipeline.construct_diagram ()
    display (diagram)
    
    remove_previous_results (path_results=path_results)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 50
def test_show_summary ():
    path_results = 'show_summary'
    remove_previous_results (path_results=path_results)
    
    pipeline = build_pipeline_construct_diagram_2 (path_results)
    X = np.array([1,2,3])
    result = pipeline (X)
    
    pipeline.show_summary ()
    
    remove_previous_results (path_results=path_results)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 51
def test_multi_comp_profiling2 ():
    class SpendTime (Component):
        def __init__ (self, time_fit=1, time_apply=1, time_fit_apply=1, **kwargs):
            super().__init__(**kwargs)
    
    class HasFitApply(SpendTime):
        def __init__ (self, **kwargs):
            super().__init__(**kwargs)
    
        def _fit (self, X, y=None):
            time.sleep(self.time_fit)
    
        def _apply (self, X):
            time.sleep(self.time_apply)
            return 1
    
        def _fit_apply (self, X, y=None):
            time.sleep(self.time_fit_apply)
            return 1
    
    class HasFitAndApply(SpendTime):
        def __init__ (self, **kwargs):
            super().__init__(**kwargs)
    
        def _fit (self, X, y=None):
            time.sleep(self.time_fit)
    
        def _apply (self, X):
            time.sleep(self.time_apply)
            return 1
    
    class HasOnlyApply(SpendTime):
        def __init__ (self, **kwargs):
            super().__init__(**kwargs)
    
        def _apply (self, X):
            time.sleep(self.time_apply)
            return 1
    
    class AllCombined (Sequential):
        def __init__ (self, **kwargs):
            super().__init__(**kwargs)
            self.has_fit_and_apply = HasFitAndApply (time_fit=0.1, time_apply=0.15)
            self.has_fit_apply1 = HasFitApply (time_fit_apply=0.3)
            self.has_only_apply = HasOnlyApply (time_apply=0.2)
            self.has_fit_apply2 = HasFitApply (time_fit_apply=0.25)
    
    all_combined = AllCombined ()
    all_combined.fit_apply (1)
    dfd = all_combined.gather_times()
    
    display('sum', dfd.sum)
    display('component overhead', dfd.component_ovh)
    display('no_overhead_total', dfd.no_overhead_total)
    display('overhead_total', dfd.overhead_total)
    display ('sum - novh_sum', dfd.sum['whole']-dfd.novh_sum['whole'])
    display('overhead summary', dfd.overhead_summary)
    
    if False:
        all_combined.fit (1)
        dfd = all_combined.gather_times()
    
        display('sum', dfd.sum)
        display('no_overhead_total', dfd.no_overhead_total)
        display('overhead_total', dfd.overhead_total)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 52
def test_some_components_return_nothing ():
    def print_function ():
        print ('hello')
    
    sequential = Sequential (print_function, print_function,
                             some_components_return_nothing=True)
    sequential ()


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 53
def test_fit_apply_multi_exist_all ():
    def f1 (x):
        print ('f1')
        return x+1
    def f2 (x):
        print ('f2')
        return x+2
    def f3 (x):
        print ('f3')
        return x+3
    
    m = Sequential (f1, f2, f3, data_io='MemoryIO', path_results='memory')
    m.fit_apply (0)
    m.f1.data_io.storage={}
    m.f2.data_io.storage={}
    m.f3.data_io.storage={}
    m.f1.error_if_fit_apply=True
    m.f2.error_if_fit_apply=True
    m.f3.error_if_fit_apply=True
    assert m.fit_apply (0)==6
    
    class Estimator ():
        def __init__ (self):
            pass
        def fit (self, x, y=None):
            print ('fit e')
            self.par = 100
        def transform (self, x):
            print ('apply e')
            return x+self.par
    
    m = Sequential (f1, f2, Estimator, f3, data_io='MemoryIO', path_results='memory')
    m.fit_apply (0)
    m.estimator.estimator = None
    
    m.f1.error_if_fit_apply=True
    m.f2.error_if_fit_apply=True
    m.f3.error_if_fit_apply=True
    m.estimator.error_if_fit_apply=True
    assert m.fit_apply (0)==106
    assert m.estimator.estimator.par==100


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 54
def test_make_pipeline ():
    tr1 = Component(FunctionTransformer (lambda x: x+1))
    tr2 = Component(FunctionTransformer (lambda x: x*2))
    pipeline = make_pipeline (tr1, tr2)
    result = pipeline.transform (3)
    
    print (result)
    assert result == 8


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 55
def test_pipeline_factory ():
    path_results = 'pipeline_factory'
    remove_previous_results (path_results=path_results)
    
    pipeline1 = pipeline_factory (SimplePipeline, path_results=path_results)
    assert pipeline1.path_results==Path(path_results).resolve()
    #pipeline2 = pipeline_factory ('SimplePipeline', path_results=path_results)
    #assert pipeline2.path_results==Path(path_results).resolve()
    
    remove_previous_results (path_results=path_results)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 56
def test_pandas_pipeline ():
    path_results = 'pandas_pipeline'
    remove_previous_results (path_results=path_results)
    df = pd.DataFrame ({'a':[1,2,3,4], 'b': [5,6,7,8], 'label': [1,0,1,0]})
    
    pipe = SimplePandasPipelineNoPandasComponent ()
    with pytest.raises (KeyError):
        r = pipe.fit_transform (df)
        display (r)
    
    pipe = SimplePandasPipeline ()
    r = pipe.fit_transform (df)
    display (r)
    
    tr1 = PandasTransformWithLabels1 ()
    tr2 = PandasTransformWithLabels2 ()
    r1=tr1.fit_transform (df)
    r2=tr2.fit_transform (r1)
    assert (r==r2).all().all()
    
    df_equal = (r1==df*1000+df[df.label==1].sum(axis=0))[['a','b']]
    assert df_equal.all().all()
    
    df_equal = (r2==r1*100+r1[r1.label==0].max(axis=0))[['a','b']]
    assert df_equal.all().all()
    remove_previous_results (path_results=path_results)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 57
def test_sequential_with_tracking ():
    path_results='test_sequential_with_tracking'
    #s = SequentialWithTracking (get_data_frame, Sum1, DummyEstimator, Max10)
    #s = SequentialWithTracking (get_data_frame, Sum1, Max10)
    s = SequentialWithTracking (get_data_frame, Sum1, DummyEvaluator, path_results=path_results)
    #x = s.fit_apply()
    x1 = s ()
    x2 = s (factor=2)
    df = s.tracker.get_experiment_data()
    df
    assert df.scores.values.ravel().tolist() == [368, 720]
    remove_previous_results (path_results)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 58
def test_seq_with_track_memory ():
    path_results='test_seq_with_track_memory'
    path_data='test_data_pipeline'
    complete_pipeline = CompletePipelineSTM (path_results=path_results, path_data=path_data)
    assert complete_pipeline.cls.DataPipeline.memory
    assert not complete_pipeline.cls.Seq1.memory
    assert not complete_pipeline.memory
    
    complete_pipeline ()
    
    k = list(complete_pipeline.cls.DataPipeline.data_io.memory_io.storage.keys())[0]
    check_last_part (k, f'{path_data}/whole/data_pipeline_result.pk')
    assert complete_pipeline.cls.DataPipeline.data_io.memory_io.storage[k]==3.3
    assert Path (f'{path_data}/whole/data_pipeline_result.pk').exists()
    assert Path (f'{path_data}/whole/f1_result.pk').exists()
    
    assert Path (f'{path_results}/main/experiments/00000/0/whole/g1_result.pk').exists()
    assert not Path (f'{path_results}/main/experiments/00000/0/whole/f1_result.pk').exists()
    
    complete_pipeline.cls.DataPipeline.error_if_apply=True
    complete_pipeline ()
    with pytest.raises (RuntimeError):
        complete_pipeline (ga=1000)
    complete_pipeline.cls.DataPipeline.error_if_apply=False
    complete_pipeline.cls.Seq1.error_if_apply=True
    assert complete_pipeline (ga=1000)=={'result': 1203.3}
    
    complete_pipeline.cls.DataPipeline.data_io.memory_io.storage[k] = 10000
    assert complete_pipeline (ga=2000)=={'result': 12200}
    
    remove_previous_results (path_results)
    remove_previous_results (path_data)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 59
def test_seq_with_track_memory2 ():
    path_results='test_seq_with_track_memory2'
    path_data='test_data_pipeline2'
    complete_pipeline = CompletePipelineSTM2 (path_results=path_results, path_data=path_data)
    assert complete_pipeline.cls.F1.data_io.memory_io is None
    assert complete_pipeline.cls.DataPipeline.data_io.memory_io is not None
    
    complete_pipeline ()
    k = list(complete_pipeline.cls.DataPipeline.data_io.memory_io.storage.keys())[0]
    check_last_part (k, f'{path_data}/whole/data_pipeline_result.pk')
    assert complete_pipeline.cls.DataPipeline.data_io.memory_io.storage[k]==3.3
    assert Path (f'{path_data}/whole/data_pipeline_result.pk').exists()
    assert Path (f'{path_data}/whole/f1_result.pk').exists()
    
    assert Path (f'{path_results}/main/experiments/00000/0/whole/g1_result.pk').exists()
    assert not Path (f'{path_results}/main/experiments/00000/0/whole/f1_result.pk').exists()
    
    complete_pipeline.cls.DataPipeline.error_if_apply=True
    complete_pipeline ()
    with pytest.raises (RuntimeError):
        complete_pipeline (ga=1000)
    complete_pipeline.cls.DataPipeline.error_if_apply=False
    complete_pipeline.cls.Seq1.error_if_apply=True
    assert complete_pipeline (ga=1000)=={'result': 1203.3}
    
    complete_pipeline.cls.DataPipeline.data_io.memory_io.storage[k] = 10000
    assert complete_pipeline (ga=2000)=={'result': 12200}
    
    remove_previous_results (path_results)
    remove_previous_results (path_data)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 60
def test_parallel ():
    x = np.array([3,4,5]).reshape(-1,1)
    parallel = Parallel (Transform1 (),
                         Transform2 ())
    r1 = parallel.fit_apply (x)
    
    x1 = x * 1000 + x.sum(axis=0)
    x2 = x * 100 + x.max(axis=0)
    r_ref = [x1, x2]
    
    assert all([(x==y).all() for x, y in zip(r1, r_ref)])
    
    parallel = Parallel (Transform1 (),
                         Transform2 (),
                         initialize_result=lambda self: {},
                         join_result=lambda self, Xr, Xi_r, components, i: {**Xr, **{components[i].name:Xi_r}})
    
    r1 = parallel.fit_apply (x)
    assert list(r1.keys())==['transform1','transform2']
    assert (r1['transform1']==r_ref[0]).all()
    assert (r1['transform2']==r_ref[1]).all()


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 61
def test_pipeline_find_last_result ():
    path_results = 'test_pipeline_find_last_result'
    
    # pipelines
    pipe1 = make_pipe1 ()
    X = np.array([1,2,3]).reshape(-1,1)
    r1 = pipe1.apply (X)
    print (r1)
    pipe2 = make_pipe1 (path_results=path_results, verbose=2)
    a, b = pipe2.A, pipe2.B
    # second
    X2 = a.apply (X)
    X2 = b.apply (X2)
    
    is_source = pipe2.find_last_result ()
    assert is_source
    
    pipe2.A.raise_error = True
    pipe2.B.raise_error = False
    pipe2.C.raise_error = False
    pipe2.D.raise_error = False
    pipe2.E.raise_error = False
    with pytest.raises (RuntimeError):
        _ = a.apply (X)
    pipe2.C.raise_error = True
    with pytest.raises (RuntimeError):
        _ = pipe2.apply ()
    pipe2.C.raise_error = False
    r2 = pipe2.apply ()
    assert (r1==r2).all()
    print (r2)
    
    remove_previous_results (path_results=path_results)
    
    #| test pipeline_find_last_result_parallel1 
    path_results = 'test_pipeline_find_last_result_parallel1'
    
    remove_previous_results (path_results=path_results)
    # pipelines
    pipe1 = make_pipe2 ()
    X = np.array([1,2,3]).reshape(1,-1)
    r1 = pipe1.apply (X)
    print (r1)
    # second
    pipe2 = make_pipe2 (path_results=path_results, verbose=2, root=True)
    a = pipe2.obj.A (X)
    b1 = pipe2.obj.B1 (a)
    b2a = pipe2.obj.B2a (a)
    b3a = pipe2.obj.B3a (a)
    b3b = pipe2.obj.B3b (b3a)
    
    #pipe2.obj.A.raise_error = True
    #pipe2.obj.B1.raise_error = True
    #pipe2.obj.B2a.raise_error = True
    pipe2.obj.B3a.raise_error = True
    #pipe2.obj.B3b.raise_error = True
    
    is_source = pipe2.find_last_result ()
    print (is_source)
    r2 = pipe2.apply ()
    assert (r1==r2).all()
    print (r2)
    
    remove_previous_results (path_results=path_results)
    
    #| test pipeline_find_last_result_parallel2 
    path_results = 'test_pipeline_find_last_result_parallel2'
    
    remove_previous_results (path_results=path_results)
    # pipelines
    pipe1 = make_pipe2 ()
    X = np.array([1,2,3]).reshape(1,-1)
    r1 = pipe1.apply (X)
    print (r1)
    # second
    pipe2 = make_pipe2 (path_results=path_results, verbose=2, root=True)
    a = pipe2.obj.A (X)
    # B1
    b1 = pipe2.obj.B1 (a)
    # B2
    b2a = pipe2.obj.B2a (a)
    b2b = pipe2.obj.B2b (b2a)
    b2c = pipe2.obj.B2c (b2b)
    # B3
    b3a = pipe2.obj.B3a (a)
    b3b = pipe2.obj.B3b (b3a)
    b3c = pipe2.obj.B3c (b3b)
    # B4
    b4 = pipe2.obj.B4 (a)
    
    pipe2.obj.A.raise_error = True
    pipe2.obj.B2a.raise_error = True
    pipe2.obj.B2b.raise_error = True
    
    pipe2.obj.B3a.raise_error = True
    pipe2.obj.B3b.raise_error = True
    
    is_source = pipe2.find_last_result ()
    print (is_source)
    r2 = pipe2.apply ()
    assert (r1==r2).all()
    
    remove_previous_results (path_results=path_results)
    
    #| test pipeline_find_last_result_parallel3 
    path_results = 'test_pipeline_find_last_result_parallel3'
    
    remove_previous_results (path_results=path_results)
    # pipelines
    pipe1 = make_pipe2 ()
    X = np.array([1,2,3]).reshape(1,-1)
    r1 = pipe1.apply (X)
    print (r1)
    # second
    pipe2 = make_pipe2 (path_results=path_results, verbose=2, root=True, new_parallel=True)
    a = pipe2.obj.A (X)
    # B
    b = pipe2.new_parallel (a)
    # B2
    c = pipe2.obj.C (b)
    
    pipe2.obj.A.raise_error = True
    pipe2.new_parallel.raise_error = True
    
    is_source = pipe2.find_last_result ()
    print (is_source)
    r2 = pipe2.apply ()
    assert (r1==r2).all()
    
    remove_previous_results (path_results=path_results)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 62
def test_pipeline_find_last_fitted_model_seq ():
    path_results = 'test_pipeline_find_last_fitted_model_seq'
    remove_previous_results (path_results=path_results)
    
    # pipelines
    pipe1 = make_pipe_fit1 ()
    X = np.array([1,2,3]).reshape(-1,1)
    r1 = pipe1.fit_apply (X)
    print (r1)
    pipe2 = make_pipe_fit1 (path_results=path_results, verbose=2)
    # second
    r = pipe2.A.fit_apply (X)
    r = pipe2.B.fit_apply (r)
    r = pipe2.C.fit_apply (r)
    r = pipe2.D.fit_apply (r)
    
    all_fitted = pipe2.find_last_fitted_model ()
    assert not all_fitted
    
    pipe2.A.raise_error = True
    pipe2.B.raise_error = True
    pipe2.B.create_estimator()
    assert pipe2.B.estimator == Bunch()
    pipe2.C.raise_error = True
    pipe2.C.create_estimator()
    assert pipe2.C.estimator == Bunch()
    r2 = pipe2.fit_apply (None)
    assert (r1==r2).all()
    print (r2)
    
    remove_previous_results (path_results=path_results)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 63
def test_pipeline_find_last_fitted_model_parallel ():
    path_results = 'test_pipeline_find_last_fitted_model_parallel'
    remove_previous_results (path_results=path_results)
    
    # pipelines
    pipe1 = make_pipe_fit2 ()
    X = np.array([1,2,3]).reshape(-1,1)
    r1 = pipe1.fit_apply (X)
    print (r1)
    pipe2 = make_pipe_fit2 (path_results=path_results, verbose=2, root=True)
    # second
    r = pipe2.A0.fit_apply (X)
    r = pipe2.A1.fit_apply (r)
    b2 = pipe2.obj.B2.fit_apply (r)
    b3a = pipe2.obj.B3a.fit_apply (r)
    b3b = pipe2.obj.B3b.fit_apply (b3a)
    b4a = pipe2.obj.B4a.fit_apply (r)
    b4b = pipe2.obj.B4b.fit_apply (b4a)
    b4c = pipe2.obj.B4c.fit_apply (b4b)
    b4d = pipe2.obj.B4d.fit_apply (b4c)
    b4e = pipe2.obj.B4e.fit_apply (b4d)
    
    all_fitted = pipe2.find_last_fitted_model ()
    assert not all_fitted
    
    pipe2.A0.raise_error = True
    pipe2.obj.B3a.raise_error = True
    pipe2.obj.B4a.raise_error = True
    pipe2.obj.B4b.raise_error = True
    pipe2.obj.B4c.raise_error = True
    pipe2.obj.B4d.raise_error = True
    
    pipe2.A1.create_estimator()
    pipe2.obj.B2.create_estimator()
    pipe2.obj.B3a.create_estimator()
    pipe2.obj.B3b.create_estimator()
    pipe2.obj.B4b.create_estimator()
    pipe2.obj.B4c.create_estimator()
    pipe2.obj.B4e.create_estimator()
    
    pipe2.logger.info (f'\n{"-"*100}\n')
    r2 = pipe2.fit_apply (None)
    assert (r1==r2).all()
    
    remove_previous_results (path_results=path_results)
    
    #| test pipeline_find_last_fitted_model_parallel_remove 
    
    path_results = 'test_pipeline_find_last_fitted_model_parallel_remove'
    remove_previous_results (path_results=path_results)
    
    # ******************************************************
    # pipelines
    pipe1 = make_pipe_fit2 ()
    X = np.array([1,2,3]).reshape(-1,1)
    r1 = pipe1.fit_apply (X)
    
    # ******************************************************
    pipe2 = make_pipe_fit2 (path_results=path_results, verbose=2, root=True)
    
    r = pipe2.A0.fit_apply (X)
    r = pipe2.A1.fit_apply (r)
    
    b1 = pipe2.obj.B1.fit_apply (r)
    b2 = pipe2.obj.B2.fit_apply (r)
    
    b3a = pipe2.obj.B3a.fit_apply (r)
    b3b = pipe2.obj.B3b.fit_apply (b3a)
    b3c = pipe2.obj.B3c.fit_apply (b3b)
    b3d = pipe2.obj.B3d.fit_apply (b3c)
    
    b4a = pipe2.obj.B4a.fit_apply (r)
    b4b = pipe2.obj.B4b.fit_apply (b4a)
    b4c = pipe2.obj.B4c.fit_apply (b4b)
    b4d = pipe2.obj.B4d.fit_apply (b4c)
    b4e = pipe2.obj.B4e.fit_apply (b4d)
    
    b5 = pipe2.obj.B5.fit_apply (r)
    
    pipe2.obj.B4d.set_load_result (False)
    pipe2.obj.B4e.set_load_result (False)
    
    all_fitted = pipe2.find_last_fitted_model ()
    assert not all_fitted
    
    pipe2.A0.raise_error = True
    pipe2.A1.raise_error = True
    
    pipe2.obj.B1.applied = False
    pipe2.obj.B2.fit_applied = False
    
    pipe2.obj.B3a.raise_error = True
    pipe2.obj.B3b.raise_error = True
    pipe2.obj.B3c.raise_error = True
    pipe2.obj.B3d.fit_applied = False
    
    pipe2.obj.B4a.raise_error = True
    pipe2.obj.B4b.raise_error = True
    pipe2.obj.B4c.fit_applied = False
    pipe2.obj.B4d.applied = False
    pipe2.obj.B4e.fit_applied = False
    
    pipe2.obj.B5.fit_applied = False
    
    pipe2.A1.create_estimator()
    pipe2.obj.B2.create_estimator()
    pipe2.obj.B3a.create_estimator()
    pipe2.obj.B3b.create_estimator()
    pipe2.obj.B4b.create_estimator()
    pipe2.obj.B4c.create_estimator()
    pipe2.obj.B4e.create_estimator()
    pipe2.obj.B5.create_estimator()
    
    pipe2.logger.info (f'\n{"-"*100}\n')
    r2 = pipe2.fit_apply (None)
    assert (r1==r2).all()
    
    assert pipe2.obj.B1.applied
    assert pipe2.obj.B2.fit_applied
    assert pipe2.obj.B3d.fit_applied
    
    assert pipe2.obj.B4c.fit_applied
    assert pipe2.obj.B4d.applied
    assert pipe2.obj.B4e.fit_applied
    
    assert pipe2.obj.B5.fit_applied
    
    remove_previous_results (path_results=path_results)
    
    # ******************************************************
    pipe2.logger.info (f'\n{"*"*100}\n{"*"*100}\n')
    pipe2 = make_pipe_fit2 (path_results=path_results, verbose=2, root=True)
    # second
    r = pipe2.A0.fit_apply (X)
    r = pipe2.A1.fit_apply (r)
    
    b1 = pipe2.obj.B1.fit_apply (r)
    b2 = pipe2.obj.B2.fit_apply (r)
    
    b3a = pipe2.obj.B3a.fit_apply (r)
    b3b = pipe2.obj.B3b.fit_apply (b3a)
    b3c = pipe2.obj.B3c.fit_apply (b3b)
    b3d = pipe2.obj.B3d.fit_apply (b3c)
    
    b4a = pipe2.obj.B4a.fit_apply (r)
    b4b = pipe2.obj.B4b.fit_apply (b4a)
    b4c = pipe2.obj.B4c.fit_apply (b4b)
    b4d = pipe2.obj.B4d.fit_apply (b4c)
    b4e = pipe2.obj.B4e.fit_apply (b4d)
    
    b5 = pipe2.obj.B5.fit_apply (r)
    
    os.remove (pipe2.obj.B4d.data_io.get_path_result_file ())
    os.remove (pipe2.obj.B4e.data_io.get_path_result_file ())
    
    
    all_fitted = pipe2.find_last_fitted_model ()
    assert not all_fitted
    
    pipe2.A0.raise_error = True
    pipe2.A1.raise_error = True
    
    pipe2.obj.B1.applied = False
    pipe2.obj.B2.fit_applied = False
    
    pipe2.obj.B3a.raise_error = True
    pipe2.obj.B3b.raise_error = True
    pipe2.obj.B3c.raise_error = True
    pipe2.obj.B3d.fit_applied = False
    
    pipe2.obj.B4a.raise_error = True
    pipe2.obj.B4b.raise_error = True
    pipe2.obj.B4c.fit_applied = False
    pipe2.obj.B4d.applied = False
    pipe2.obj.B4e.fit_applied = False
    
    pipe2.obj.B5.fit_applied = False
    
    pipe2.A1.create_estimator()
    pipe2.obj.B2.create_estimator()
    pipe2.obj.B3a.create_estimator()
    pipe2.obj.B3b.create_estimator()
    pipe2.obj.B4b.create_estimator()
    pipe2.obj.B4c.create_estimator()
    pipe2.obj.B4e.create_estimator()
    pipe2.obj.B5.create_estimator()
    
    pipe2.logger.info (f'\n{"-"*100}\n')
    r2 = pipe2.fit_apply (None)
    assert (r1==r2).all()
    
    assert pipe2.obj.B1.applied
    assert pipe2.obj.B2.fit_applied
    assert pipe2.obj.B3d.fit_applied
    
    assert pipe2.obj.B4c.fit_applied
    assert pipe2.obj.B4d.applied
    assert pipe2.obj.B4e.fit_applied
    
    assert pipe2.obj.B5.fit_applied
    
    
    remove_previous_results (path_results=path_results)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 64
def test_multi_modality ():
    data = {'transform1': np.array([1,2,3]),
            'transform2': np.array([10,20,30])}
    
    parallel = ParallelChannels (Transform1 (),
                              Transform2 (),
                              use_name=True)
    r = parallel.fit_apply (data)
    
    x1 = data['transform1']
    x1 = x1 * 1000 + x1.sum(axis=0)
    x2 = data['transform2']
    x2 = x2 * 100 + x2.max(axis=0)
    assert list(r.keys())==['transform1','transform2']
    assert (r['transform1']==x1).all()
    assert (r['transform2']==x2).all()
    
    # with configs
    data = dict(modA=np.array([1,2,3]),
                modB=np.array([10,20,30]))
    configs = dict(modA=dict (modality='A', factor=2000),
                   modB=dict (modality='B', factor=3000))
    
    parallel = ParallelChannels (component_class=TransformM,
                              configs=configs)
    r = parallel.fit_apply (data)
    
    x1 = data['modA']
    x1 = x1 * 2000 + x1.sum(axis=0)
    x2 = data['modB']
    x2 = x2 * 3000 + x2.sum(axis=0)
    assert list(r.keys())==['modA','modB']
    assert (r['modA']==x1).all()
    assert (r['modB']==x2).all()
    
    assert parallel.transform_m_modA.modality == 'A'
    assert parallel.transform_m_modB.modality == 'B'


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 65
def test_column_selector ():
    df = pd.DataFrame ({'x1': list(range(5)),
                    'x2': list(range(5,10)),
                    'x3': list(range(15,20)),
                    'x4': list(range(25,30))
                   })
    dfr = ColumnSelector(columns=['x2','x4'], error_if_apply=True).transform(df)
    assert (dfr==df[['x2','x4']]).all().all()


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 66
def test_concat ():
    df = pd.DataFrame ({'x1': list(range(5)),
                    'x2': list(range(5,10)),
                    'x3': list(range(15,20)),
                    'x4': list(range(25,30))
                   })
    
    df2 = pd.DataFrame ({'x5': list(range(5)),
                    'x6': list(range(5,10)),
                    'x7': list(range(15,20))
                   })
    dfr = Concat (error_if_apply=True).transform(df, df2)
    assert (dfr==pd.concat ([df,df2], axis=1)).all().all()


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 67
def test_identity ():
    df = pd.DataFrame ({'x1': list(range(5)),
                    'x2': list(range(5,10)),
                    'x3': list(range(15,20)),
                    'x4': list(range(25,30))
                   })
    
    dfr = Identity (error_if_apply=True).transform(df)
    assert (dfr==df).all().all()


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 68
def test_make_column_transformer ():
    df, tr1 = column_transformer_data ()
    
    tr1 = Component(FunctionTransformer (lambda x: x+1), name='tr1')
    tr2 = PandasComponent(FunctionTransformer (lambda x: x*2), 
                          output_columns=['cont2_bis','cat_1'], 
                          name='tr2')
    
    column_transformer = make_column_transformer (
        (tr1, ['cont2', 'cont4']),
        (tr2, ['cont2', 'cat_1']),
        error_if_apply=True,
        verbose=2
    )
    print (f'{"-"*100}\n{column_transformer}')
    dfr = column_transformer.transform(df)
    
    # display and test
    display(dfr)
    assert (dfr[['cont2','cont4']] == tr1(df[['cont2','cont4']])).all().all()
    assert (dfr[['cont2_bis','cat_1']] == tr2(df[['cont2','cat_1']])).all().all()
    assert (dfr.columns == ['cont2','cont4', 'cont2_bis','cat_1']).all()
    assert (column_transformer.name, column_transformer.class_name) == ('__base_column_transformer', '_BaseColumnTransformer')
    
    # set name of column transformer
    column_transformer = make_column_transformer (
        (tr1, ['cont2', 'cont4']),
        (tr2, ['cont2', 'cat_1']),
        name='test_transformer',
        class_name='TestTransformer',
        error_if_apply=True,
        verbose=2
    )
    print (f'{"-"*100}\n{column_transformer}')
    assert (column_transformer.name, column_transformer.class_name) == ('test_transformer', 'TestTransformer')
    
    # set name of column transformer and parameters that are specific 
    # for the column_transformer: path_results
    column_transformer = make_column_transformer (
        (tr1, ['cont2', 'cont4']),
        (tr2, ['cont2', 'cat_1']),
        name='test_transformer',
        class_name='TestTransformer',
        TestTransformer=dict(path_results='mine'),
        path_results='other',
        error_if_apply=True,
        verbose=2
    )
    print (f'{"-"*100}\n{column_transformer}')
    assert column_transformer.path_results.name=='mine'
    assert column_transformer.components[0].path_results.name=='other'


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 69
def test_make_column_transformer_passthrough ():
    df, tr1 = column_transformer_data ()
    
    column_transformer = make_column_transformer (
        (tr1, ['cont1', 'cont4']),
        ('passthrough', ['cont2', 'cat_1']),
        error_if_apply=True,
        verbose=2
    )
    dfr = column_transformer.transform(df)
    
    # display and test
    display(dfr)
    assert (dfr[['cont1','cont4']] == tr1(df[['cont1','cont4']])).all().all()
    assert (dfr[['cont2','cat_1']] == df[['cont2','cat_1']]).all().all()
    assert (dfr.columns == ['cont1','cont4', 'cont2','cat_1']).all()


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 70
def test_make_column_transformer_remainder ():
    df, tr1 = column_transformer_data ()
    
    # remainder is new transformation
    tr3 = Component(FunctionTransformer (lambda x: x+100), name='tr3')
    column_transformer = make_column_transformer (
        (tr1, ['cont1', 'cont4']),
        ('passthrough', ['cont2', 'cat_1']),
        remainder=tr3,
        error_if_apply=True,
        verbose=2
    )
    dfr = column_transformer.transform(df)
    
    # display and test
    display('with tr3', dfr)
    assert (dfr[['cont1','cont4']] == tr1(df[['cont1','cont4']])).all().all()
    assert (dfr[['cont2','cat_1']] == df[['cont2','cat_1']]).all().all()
    assert (dfr[['cont3','cat_2']] == tr3(df[['cont3','cat_2']])).all().all()
    assert (dfr.columns == ['cont1','cont4', 'cont2','cat_1','cont3','cat_2']).all()
    
    # remainder is passthrough
    del tr1.nick_name
    del tr3.nick_name
    column_transformer = make_column_transformer (
        (tr1, ['cont1', 'cont4']),
        (tr3, ['cont2', 'cat_1']),
        remainder='passthrough',
        error_if_apply=True,
        verbose=2
    )
    dfr = column_transformer.transform(df)
    
    display('with passthrough', dfr)
    assert (dfr[['cont1','cont4']] == tr1(df[['cont1','cont4']])).all().all()
    assert (dfr[['cont2','cat_1']] == tr3(df[['cont2','cat_1']])).all().all()
    assert (dfr[['cont3','cat_2']] == df[['cont3','cat_2']]).all().all()
    assert (dfr.columns == ['cont1','cont4', 'cont2','cat_1','cont3','cat_2']).all()
    
    # remainder is tr3, and one of the transforms is drop
    del tr1.nick_name
    del tr3.nick_name
    column_transformer = make_column_transformer (
        (tr1, ['cont1', 'cont4']),
        ('drop', ['cont2', 'cat_1']),
        remainder=tr3,
        error_if_apply=True,
        verbose=2
    )
    dfr = column_transformer.transform(df)
    
    display('with drop one of the transforms - cont2, cat_1', dfr)
    assert (dfr[['cont1','cont4']] == tr1(df[['cont1','cont4']])).all().all()
    assert (dfr[['cont3','cat_2']] == tr3(df[['cont3','cat_2']])).all().all()
    assert (dfr.columns == ['cont1','cont4', 'cont3','cat_2']).all()
    
    # check gather_descendants
    column_transformer.gather_descendants()
    assert sorted(column_transformer.full_obj.keys())==['column_selector', 'concat', 'tr1', 'tr1_cc', 'tr3', 'tr3_rem']


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 71
def test_make_column_transformer_descendants ():
    df, tr1 = column_transformer_data ()
    
    tr3 = Component(FunctionTransformer (lambda x: x+100), name='tr3')
    
    column_transformer = make_column_transformer (
        (tr1, ['cont1', 'cont4']),
        ('drop', ['cont2', 'cat_1']),
        remainder=tr3
    )
    
    # check gather_descendants
    column_transformer.gather_descendants()
    assert sorted(column_transformer.full_obj.keys())==['column_selector', 'concat', 'tr1', 'tr1_cc', 'tr3', 'tr3_rem']


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 72
def test_make_column_transformer_fit_transform ():
    df, tr1 = column_transformer_data ()
    
    class SumTimes100 (Component):
        def _fit (self, X):
            self.sum = X.sum(axis=0)
        def _apply (self, X):
    
            dfr = pd.DataFrame ({'c1_times100': self.sum.values[0]*100 + X.iloc[:,0].values,
                                 'c2_times100': self.sum.values[1]*100 + X.iloc[:,1].values,
                                 'c2_times1000': self.sum.values[1]*1000 + X.iloc[:,1].values})
            return dfr
    
    tr1 = SumTimes100 ()
    tr2 = PandasComponent(FunctionTransformer (lambda x: x*2), name='tr2')
    
    column_transformer = make_column_transformer (
        (tr1, ['cont2', 'cont4']),
        (tr2, ['cont2', 'cat_1']),
        error_if_apply=True,
        verbose=2
    )
    dfr = column_transformer.fit_transform(df)
    
    # display & test
    display(dfr)
    assert (dfr.columns == ['c1_times100','c2_times100', 'c2_times1000','cont2', 'cat_1']).all()
    assert (dfr['c1_times100'] == sum(df.cont2)*100+df.cont2).all()
    assert (dfr['c2_times100'] == sum(df.cont4)*100+df.cont4).all()
    assert (dfr['c2_times1000'] == sum(df.cont4)*1000+df.cont4).all()


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 73
def test_make_column_transformer_different_indexes ():
    df, tr1 = column_transformer_data ()
    def my_transform (df):
        df = df.copy()
        df = df + 1
        df.index=list(range(5,10))
        return df
    tr1 = Component (apply=my_transform)
    tr2 = PandasComponent(FunctionTransformer (lambda x: x*2), name='tr2')
    
    column_transformer = make_column_transformer (
        (tr1, ['cont2', 'cont4']),
        (tr2, ['cont2', 'cat_1']),
        error_if_apply=True,
        verbose=2
    )
    dfr = column_transformer.fit_transform(df)
    
    # display & test
    display(dfr)
    
    assert not dfr.isna().any().any()


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 74
def test_multi_split_transform ():
    # example 1: apply transform on multiple splits
    data, multi_transform1, multi_transform2, tr2 = multi_split_data ()
    
    result = multi_transform1.fit_transform (data)
    
    assert type(result) is dict
    assert result.keys() == data.keys()
    for split in result.keys():
        assert (result[split]==sum(data['training'].ravel())+data[split]*1000).all()
    
    # check that automatic name given is based on component
    assert multi_transform1.name=='transform1_multi_split'
    assert multi_transform1.class_name=='Transform1MultiSplit'
    
    # check that we can assign a different name
    multi_transform1 = MultiSplitDict (component = Transform1(), name='different', class_name='Yes')
    assert multi_transform1.name=='different'
    assert multi_transform1.class_name=='Yes'
    # check that this new name is given only to MultiSplitDict, 
    # not to the component that it's wrapping
    assert multi_transform1.component.name=='transform1'
    assert multi_transform1.component.class_name=='Transform1'


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 75
def test_multi_split_fit ():
    # example 2: fit method gets training, validation and test
    data, multi_transform1, multi_transform2, tr2 = multi_split_data ()
    # we apply the transform to only test
    
    
    # we apply the transform to only test data
    result = multi_transform2.fit_transform (data, apply_to='test')
    
    assert type(result) is dict
    assert list(result.keys()) == ['test']
    for split in result.keys():
        assert (result[split]==max(data['training'].ravel())+data[split]*100).all()
    
    for split in ['validation', 'test']:
        assert (tr2.data[split] == data[split]).all()


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 76
def test_multi_split_chain ():
    data, multi_transform1, multi_transform2, tr2 = multi_split_data ()
    
    # test that we can chain transformations
    
    result = multi_transform1.fit_transform (data)
    result = multi_transform2.fit_transform (result, apply_to='test')
    
    
    #check that we have no error if split does not exist
    result = multi_transform1.fit_transform (data, apply_to=['training', 'validation'])
    result = multi_transform2.fit_transform (result, apply_to=['test'])
    assert len(result)==0
    
    #check that we have an error if we set the flag `raise_error_if_split_doesnot_exist=True`
    multi_transform2.raise_error_if_split_doesnot_exist = True
    result = multi_transform1.fit_transform (data, apply_to=['training', 'validation'])
    with pytest.raises (RuntimeError):
        result = multi_transform2.fit_transform (result, apply_to=['test'])


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 77
def test_multi_split_io ():
    data, multi_transform1, multi_transform2, tr2 = multi_split_data ()
    
    # check loading / saving
    
    path_results = 'results_multi_split'
    
    remove_previous_results (path_results=path_results)
    
    tr = PickleSaverComponent (FunctionTransformer (lambda x: x*2),
                    name='times2',
                    path_results=path_results)
    
    multi_transform = MultiSplitDict (component=tr,
                                      apply_to=['validation', 'test'],
                                      path_results=path_results,
                                      data_io=PickleIO (path_results=path_results))
    
    result = multi_transform (data)
    
    multi_transform2 = MultiSplitDict (component=tr,
                                       data_io=PickleIO (path_results=path_results), 
                                       name='times2_multi_split')
    
    result2 = multi_transform2.load_result ()
    
    for k in result.keys():
        assert (result[k] == result2[k]).all()
    
    assert result.keys()==result2.keys()
    
    assert sorted(os.listdir(path_results))==['test', 'validation', 'whole']
    
    assert (tr.data_io.load_result(split='test') == result['test']).all()
    
    assert (tr.data_io.load_result(split='validation') == result['validation']).all()
    
    assert os.listdir(f'{path_results}/validation')==['times2_result.pk']
    
    assert os.listdir(f'{path_results}/test')==['times2_result.pk']
    
    assert os.listdir(f'{path_results}/whole')==['times2_multi_split_result.pk']
    
    remove_previous_results (path_results=path_results)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 78
def test_multi_split_non_dict ():
    # check loading / saving
    tr = Component (FunctionTransformer (lambda x: x*2))
    
    multi_transform = MultiSplitDict (tr, apply_to = ['test'])
    
    data = np.array([100,200,300]).reshape(-1,1)
    result = multi_transform (data)
    
    assert type(result)==np.ndarray
    assert (result==data*2).all()


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 79
def test_multi_split_non_dict_bis ():
    tr = Component (FunctionTransformer (lambda x: x*2))
    
    multi_transform = MultiSplitDict (tr, apply_to = ['test'])
    
    # output applied to single split, converted to non-dictionary
    data = dict(training = np.array([1,2,3]).reshape(-1,1),
                validation = np.array([10,20,30]).reshape(-1,1),
                test = np.array([100,200,300]).reshape(-1,1))
    result = multi_transform (data, output_not_dict=True)
    
    assert type(result)==np.ndarray
    assert (result==data['test']*2).all()


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 80
def test_multi_split_with_function ():
    def f (x, a=1): return x+a
    d = {'training': 10, 'test': 100}
    multi = MultiSplitDict (f)
    assert multi (d)=={'training': 11, 'test': 101}
    
    multi = MultiSplitDict (f, a=10)
    assert multi (d)=={'training': 20, 'test': 110}
    
    assert multi.name=='f_multi_split' and multi.class_name=='FMultiSplit'


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 81
def test_multi_split_root ():
    """ Have MultiSplitComponent be root"""
    data, *_ = multi_split_data ()
    print ('original data')
    display (data)
    component = MultiSplitDict (Component (apply=lambda x:x+1), root=True)
    result = component.apply (data)
    print ('transformed data')
    display (result)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 82
def test_multi_split_tuples ():
    """ Data fed to MultiSplitDict is tuples for each split"""
    data = dict (train=(np.array([1,2,3]), np.array([1,0,1])),
                 test=(np.array([10,20,30]), np.array([0,1,1])))
    
    component = MultiSplitDict (Component (apply=lambda x:x+1))
    if False:
        result = component.fit_apply (data)
        print ('transformed data')
        display (result)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 83
def test_multi_split_df_column_transform_whole_df ():
    """Example: apply transform on whole dataframe."""
    df, data, tr2 = multi_split_data_df_column ()
    
    # Wrap Transform1 object as `MultiSplitDFColumn`
    multi_transform_whole = MultiSplitDFColumn (component=Transform1(), drop_split=True)
    
    # call `fit_transform` on wrapped object
    result = multi_transform_whole.fit_transform (df)
    
    # check correctness of result
    assert type(result) is pd.DataFrame
    assert (result.split.unique() == list(data.keys())).all()
    for split in data.keys():
        assert (result[result.split==split].drop(columns='split').values==sum(data['training'].ravel())+data[split]*1000).all()
    # check that automatic name given is based on component
    assert multi_transform_whole.name=='transform1_multi_split'
    assert multi_transform_whole.class_name=='Transform1MultiSplit'


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 84
def test_multi_split_df_column_transform ():
    """Example: apply transform on multiple splits."""
    # get input data and transform to be wrapped
    df, data, tr2 = multi_split_data_df_column ()
    
    # Wrap Transform1 object as `MultiSplitDFColumn`
    multi_transform1 = MultiSplitDFColumn (component=Transform1(), drop_split=True,
                                           apply_to=list(data.keys()))
    
    # call `fit_transform` on wrapped object
    result = multi_transform1.fit_transform (df)
    
    # check correctness of result
    assert type(result) is pd.DataFrame
    assert (result.split.unique() == list(data.keys())).all()
    for split in data.keys():
        assert (result[result.split==split].drop(columns='split').values==sum(data['training'].ravel())+data[split]*1000).all()
    """Example: apply transform on multiple splits."""
    # get input data and transform to be wrapped
    df, data, tr2 = multi_split_data_df_column ()
    
    # Wrap Transform1 object as `MultiSplitDFColumn`
    multi_transform1 = MultiSplitDFColumn (component=Transform1(), drop_split=True,
                                           apply_to=['training', 'validation', 'test'])
    
    # call `fit_transform` on wrapped object
    result = multi_transform1.fit_transform (df)
    
    # check correctness of result
    assert type(result) is pd.DataFrame
    assert (result.split.unique() == list(data.keys())).all()
    for split in data.keys():
        assert (result[result.split==split].drop(columns='split').values==sum(data['training'].ravel())+data[split]*1000).all()
    # check that automatic name given is based on component
    assert multi_transform1.name=='transform1_multi_split'
    assert multi_transform1.class_name=='Transform1MultiSplit'


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 85
def test_multi_split_df_column_fit ():
    """Example: fit method gets training, validation and test."""
    # get input data and transform to be wrapped
    df, data, tr2 = multi_split_data_df_column ()
    
    # Wrap transform as `MultiSplitDFColumn`
    multi_transform2 = MultiSplitDFColumn (component=tr2, drop_split=True, 
                                           fit_additional = ['validation', 'test'],
                                           apply_to=['training', 'validation', 'test'])
    
    # apply the transform to only test data
    result = multi_transform2.fit_transform (df, apply_to='test')
    
    # check correctness of result
    assert type(result) is pd.DataFrame
    assert list(result.split.unique()) == ['test']
    for split in result.split.unique():
        assert (result[result.split==split].drop(columns='split').values==max(data['training'].ravel())+data[split]*100).all()
    for split in ['validation', 'test']:
        assert (tr2.data[split] == df[df.split==split]).all().all()


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 86
def test_cross_validator_1 ():
    # setup
    df = get_cross_validator_input_data ()
    splitter = SkSplitGenerator (KFold (n_splits=5), label_col='label', split_col='split')
    classifier = DummyClassifier (data_converter=PandasConverter (metadata=['split']),
                                  project_op='min', statistic='min')
    dfclass = MultiSplitDFColumn (classifier)
    
    # using `CrossValidator` 
    cv = CrossValidator (dfclass, splitter=splitter)
    result = cv.fit_apply (df)
    
    # using reference where MultiSplitDFColumn uses all splits
    splitter.reset ()
    dfclass = MultiSplitDFColumn (classifier, apply_to=['training','validation','test'])
    cv2 = CrossValidator (dfclass, splitter=splitter)
    result2 = cv2.fit_apply (df)
    
    # check results
    assert len(result)==5
    for r, r2 in zip (result, result2): pd.testing.assert_frame_equal (r, r2.sort_index())
    
    assert (result[0].columns == [0,'label','split']).all()
    
    statistics = [-3000, -5000, -6000, -5000, -5000]
    i=0
    assert (result2[i][0].values == np.r_[statistics[i]+df['a'].values[2:],statistics[i]+df['a'].values[:2]]).all()
    for i in range(1,4):
        assert (result2[i][0].values == np.r_[statistics[i]+df['a'].values[0:2*i],statistics[i]+df['a'].values[2*(i+1):], statistics[i]+df['a'].values[2*i:2*(i+1)]]).all()
    i=4
    assert (result2[i][0].values == statistics[i]+df['a'].values).all()


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 87
def test_cross_validator_2 ():
    # set-up
    def classifier_func (X):
        return pd.DataFrame ({'label': X.label, 
                              'classification': np.floor(X['a'].values / 2) % 2})
    df = get_cross_validator_input_data ()
    classifier_comp = Component (apply=classifier_func)
    classifier = MultiSplitDFColumn (classifier_comp)
    splitter = SkSplitGenerator (KFold (n_splits=5), label_col='label', split_col='split')
    evaluator = MultiSplitDFColumn(PandasEvaluator(convert_after=lambda x: pd.DataFrame (x, index=[0])))
    
    # `CrossValidator` usage 
    cv = CrossValidator (classifier, splitter=splitter, evaluator=evaluator, add_evaluation=False)
    result = cv.fit_apply (df)
    
    # check results
    results = pd.concat(result)
    assert (results.loc[results.split=='test', 'accuracy_score'] == [1, 0, 0.5, 1, 0]).all()
    assert (results.loc[results.split=='training', 'accuracy_score'] == [0.375, 0.625, 0.5, 0.375, 0.625]).all()


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 88
def test_cross_validator_3 ():
    classifier_comp = DummyHistoryClassifier ()
    classifier = MultiSplitDFColumn (classifier_comp)
    df = get_cross_validator_input_data ()
    splitter = SkSplitGenerator (KFold (n_splits=5), label_col='label', split_col='split')
    
    # example usage
    cv = CrossValidator (classifier, splitter=splitter, score_method='history')
    result = cv.fit_apply (df)
    
    assert (list(result.keys()) == ['score'] 
            and (result['score'] == np.array([3.6, 2.6, 1.2, 1. , 1.2, 2.2, 3.6, 4.6])).all())
    
    splitter.reset ()
    cv = CrossValidator (classifier, splitter=splitter, score_method='history', select_epoch=True)
    result = cv.fit_apply (df)
    
    assert result=={'last_score': 4.6, 'n_iterations': 5, 'argmax_score': 7, 'max_score': 4.6, 'argmin_score': 3, 'min_score': 1.0}
    
    # ************************************************
    # ************************************************
    path_results = 'test_cross_validator_3'
    splitter.reset ()
    cv = CrossValidator (classifier, splitter=splitter, score_method='history', select_epoch=True,
                         path_results=path_results)
    result = cv.fit_apply (df)
    
    assert result=={'last_score': 4.6, 'n_iterations': 5, 'argmax_score': 7, 'max_score': 4.6, 'argmin_score': 3, 'min_score': 1.0}
    
    result = joblib.load (f'{path_results}/whole/cross_validation_final_metrics.pk')
    assert result=={'last_score': 4.6, 'n_iterations': 5, 'argmax_score': 7, 'max_score': 4.6, 'argmin_score': 3, 'min_score': 1.0}
    
    result = joblib.load (f'{path_results}/whole/cross_validation_metrics.pk')
    assert list(result.keys())==['score'] and (result['score']==[3.6, 2.6, 1.2, 1. , 1.2, 2.2, 3.6, 4.6]).all()
    
    assert sorted (os.listdir(f'{path_results}/whole'))==['cross_validation_final_metrics.pk',
                                                     'cross_validation_metrics.pk',
                                                     'cross_validator_result.pk',
                                                     'pipeline_0_result.pk',
                                                     'pipeline_1_result.pk',
                                                     'pipeline_2_result.pk',
                                                     'pipeline_3_result.pk',
                                                     'pipeline_4_result.pk']
    
    remove_previous_results (path_results)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 89
def test_cross_validator_4 ():
    # set up
    classifier_comp = DummyHistoryClassifier ()
    classifier = MultiSplitDFColumn (classifier_comp)
    df = get_cross_validator_input_data ()   
    splitter = SkSplitGenerator (KFold (n_splits=5), label_col='label', split_col='split')
    
    # usage
    cv = CrossValidator (classifier, splitter=splitter, score_method='history', select_epoch=True,
                         optimization_mode='max')
    result = cv.fit_apply (df)
    
    assert result=={'score': 4.6, 'last_score': 4.6, 'n_iterations': 5, 'argmax_score': 7}
    
    # ************************************************
    # ************************************************
    path_results = 'test_cross_validator_4'
    splitter.reset ()
    cv = CrossValidator (classifier, splitter=splitter, score_method='history', select_epoch=True,
                         optimization_mode='max', path_results=path_results)
    result = cv.fit_apply (df)
    
    # check
    assert result=={'score': 4.6, 'last_score': 4.6, 'n_iterations': 5, 'argmax_score': 7}
    
    result = joblib.load (f'{path_results}/whole/cross_validation_final_metrics.pk')
    assert result=={'score': 4.6, 'last_score': 4.6, 'n_iterations': 5, 'argmax_score': 7}
    
    result = joblib.load (f'{path_results}/whole/cross_validation_metrics.pk')
    assert list(result.keys())==['score'] and (result['score']==[3.6, 2.6, 1.2, 1. , 1.2, 2.2, 3.6, 4.6]).all()
    
    assert sorted (os.listdir(f'{path_results}/whole'))==['cross_validation_final_metrics.pk',
                                                     'cross_validation_metrics.pk',
                                                     'cross_validator_result.pk',
                                                     'pipeline_0_result.pk',
                                                     'pipeline_1_result.pk',
                                                     'pipeline_2_result.pk',
                                                     'pipeline_3_result.pk',
                                                     'pipeline_4_result.pk']
    
    remove_previous_results (path_results)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 90
def test_optuna_pruner ():
    run_multiple_studies (study_suffix='')


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 91
def test_optuna_pruner_2 ():
    run_multiple_studies (study_suffix='_2', nfolds=10, trial_outlier=4, fold_outlier='all', 
                          indicate_same_step=True, same_value_per_step=True)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 92
def test_optuna_pruner_3 ():
    run_multiple_studies (study_suffix='_3', nfolds=10, trial_outlier=4, fold_outlier='all', 
                          indicate_same_step=True, same_value_per_step=False)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 93
def test_optuna_pruner_4 ():
    run_multiple_studies (study_suffix='_4', nfolds=10, trial_outlier=4, fold_outlier=3, 
                          indicate_same_step=True, same_value_per_step=False)
    #run_study (n_startup_trials=2, n_warmup_steps=0, study_suffix='_4', n_trials=100,
    #       nfolds=100, trial_outlier=90, fold_outlier=90, indicate_same_step=True, 
    #       same_value_per_step=False)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 94
def test_optuna_pruner_5 ():
    run_multiple_studies (study_suffix='_5', nfolds=10, trial_outlier=4, fold_outlier=3, 
                          indicate_same_step=False, same_value_per_step=False)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 95
def test_cross_validator_pruner ():
    def check_rounded_result (result, reference):
        if type(reference.get('score')) == np.ndarray:
            assert list(result.keys())==['score']
            assert (np.floor(result['score'])==reference['score']).all()
        else:
            rounded_result = {k: np.floor(result[k]) for k in result}
            assert rounded_result==reference
    
    def check_dict_of_array (result, reference):
        assert list(result.keys())==['score']
        assert type(result['score'])==np.ndarray
        assert (result['score']==reference['score']).all()
    
    
    class PrunerClassifier (Component):
        def __init__ (self, **kwargs):
            super ().__init__ (**kwargs)
            self.experiment_number = None
        def _apply (self, X, **kwargs):
            return X
        def _fit (self, X, y=None, **kwargs):
            score = 1.0 if self.experiment_number == 4 else 1000.0
            self.logger.debug (f'base score due to experiment parameters: {score}')
    
            experiment_addition = self.experiment_number/10.0
            score += experiment_addition
            self.logger.debug (f'experiment_addition: {experiment_addition} => score: {score}')
    
            training_data_addition = np.mean(X.a.values)*10
            score += training_data_addition
            self.logger.debug (f'training_data_addition: {training_data_addition} => score: {score}')
    
            if self.experiment_number == 4: scores = [score-1.0]*2 + [score] + [score-1.0]*4
            else: scores = [score-1.0, score] + [score-1.0]*5
            self.logger.info (f'final scores across epochs: {scores}')
            self.dict_results = dict (score=scores)
        def history (self):
            return self.dict_results
    
    path_results = 'test_cross_validator_pruner'
    study_name='test_pruner'
    pruner = optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=0)
    os.makedirs (path_results, exist_ok=True)
    
    # set-up
    df = get_cross_validator_input_data ()
    classifier_comp = PrunerClassifier (verbose=2)
    classifier = MultiSplitDFColumn (classifier_comp)
    splitter = SkSplitGenerator (KFold (n_splits=5), label_col='label', split_col='split')
    def create_cv (trial):
        splitter.reset ()
        classifier_comp.experiment_number = trial.number
        cv = CrossValidator (classifier, splitter=splitter, score_method='history', select_epoch=True,
                             optimization_mode='max', path_results=f'{path_results}/{trial.number}', 
                             key_score='score', pruner_optimization_mode='max', verbose=2, trial=trial)
        return cv
    
    df = get_cross_validator_input_data ()   
    
    def objective (trial):
        v = 0
        if trial.number == 4:
            v = 0
        else:
            v = 5
        cv = create_cv (trial)
    
        cv.logger.info (f'{"*"*100}\n')
        cv.logger.info (f'experiment number {trial.number}')
        result = cv.fit_apply (df)
        should_prune = trial.should_prune()
        dict_results = cv.load_result (result_file_name='cross_validation_metrics.pk')
        final_results = cv.load_result (result_file_name='cross_validation_final_metrics.pk')
        cross_validator_result = cv.load_result (result_file_name='cross_validator_result.pk')
        npipelines = len(glob.glob (f'{path_results}/{trial.number}/whole/pipeline_*_result.pk'))
    
        cv.logger.info (f'experiment number {trial.number}, should_prune: {should_prune}, '
               f'npipelines: {npipelines}, n_iterations: {cv.n_iterations}\n'
               f'dict_results: {cv.dict_results}\nstored: {dict_results}\n'
               f'final_results: {final_results}\ncross_validator_result: {cross_validator_result}')
    
        if trial.number==4: 
            assert should_prune and npipelines==1 and cv.n_iterations==1
            assert cv.dict_results=={'score': 56.4, 'last_score': 55.4, 'n_iterations': 1.0, 'argmax_score': 2}
            check_dict_of_array (dict_results,
                                 {'score': np.array([55.4, 55.4, 56.4, 55.4, 55.4, 55.4, 55.4])})
            assert final_results=={'score': 56.4, 'last_score': 55.4, 'n_iterations': 1.0, 'argmax_score': 2}
            assert cross_validator_result=={'score': 56.4, 'last_score': 55.4, 'n_iterations': 1.0, 'argmax_score': 2}
    
        else: 
            assert not should_prune and npipelines==5 and cv.n_iterations==5
            check_rounded_result (cv.dict_results, 
                                  {'score': 1045.0, 'last_score': 1044.0, 'n_iterations': 5.0, 'argmax_score': 1.0})
            check_rounded_result (dict_results,
                                  {'score': np.array([1044, 1045, 1044, 1044, 1044, 1044, 1044])})
            check_rounded_result (final_results,
                                  {'score': 1045.0, 'last_score': 1044.0, 'n_iterations': 5.0, 'argmax_score': 1.0})
            check_rounded_result (cross_validator_result,
                                  {'score': 1045.0, 'last_score': 1044.0, 'n_iterations': 5.0, 'argmax_score': 1.0})
    
        return 5.0
    
    study = optuna.create_study(direction='maximize',
                                study_name=study_name,
                                storage=f'sqlite:///{path_results}/{study_name}.db',
                                pruner=pruner, load_if_exists=True)
    
    study.optimize(objective, n_trials=10, n_jobs=1)
    
    classifier_comp.logger.info ('\n\nFinal results across epochs for the different experiments')
    for experiment_number in range(10):
        dict_results = joblib.load (f'{path_results}/{experiment_number}/whole/cross_validation_metrics.pk')
        classifier_comp.logger.info (dict_results)
    
    classifier_comp.logger.info ('\nFinal results for the different experiments')
    for experiment_number in range(10):
        final_results = joblib.load (f'{path_results}/{experiment_number}/whole/'
                                     'cross_validation_final_metrics.pk')
        classifier_comp.logger.info (final_results)
    
    remove_previous_results (path_results)  


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 96
def test_instances_ensembler_1 ():
    classifier = DummyClassifierInstance ()
    ensembler = make_ensembler (classifier, n_models=5)
    result = ensembler.fit_apply(df_ensemble)
    result.tolist()
    
    assert result.tolist()==[0.0, 0.2, 0.4, 0.6000000000000001, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0]


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 97
def test_model_ensembler_1 ():
    ensembler = make_ensembler (DummyClassifierInstance, n_models=5)
    result = ensembler.fit_apply(df_ensemble)
    assert result.tolist()==[0.0, 0.2, 0.4, 0.6000000000000001, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0]


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 98
def test_ensemble_different_models ():
    ensembler = make_ensembler (SumColumn(), MultiplyColumn())
    result = ensembler.fit_apply(df_ensemble)
    c0=SumColumn ()
    c1=MultiplyColumn ()
    x0=c0.fit_apply (df_ensemble)
    x1=c1.fit_apply (df_ensemble)
    assert (result==(x0+x1)/2).all()
    ensembler = make_ensembler (SumColumn(), MultiplyColumn(), 
                                final_classifier=WeightedClassifier(weights=np.array([0.1,0.9])))
    result = ensembler.fit_apply(df_ensemble)
    
    # we compare against weighted result
    assert (result==(x0*0.1+x1*0.9)).all()


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 99
def test_ensembler_separate_paths ():
    path_results='test_ensembler_whole'
    path_models='test_ensembler_models_whole'
    ensembler = make_ensembler (DummyClassifierInstance, n_models=5, 
                                path_results=path_results, path_models=path_models)
    
    result = ensembler.fit_apply(df_ensemble)
    
    for i in range (5):
        assert ensembler.components[0].components[i].estimator == {'threshold': i}
    
    assert sorted(os.listdir (f'{path_results}/whole'))==['dummy_classifier_instance_0_result.pk',
     'dummy_classifier_instance_1_result.pk',
     'dummy_classifier_instance_2_result.pk',
     'dummy_classifier_instance_3_result.pk',
     'dummy_classifier_instance_4_result.pk',
     'ensembler_result.pk',
     'parallel_models_result.pk',
     'weighted_classifier_result.pk']
    
    assert sorted(os.listdir (f'{path_models}/models'))==['dummy_classifier_instance_0_estimator.pk',
         'dummy_classifier_instance_1_estimator.pk',
         'dummy_classifier_instance_2_estimator.pk',
         'dummy_classifier_instance_3_estimator.pk',
         'dummy_classifier_instance_4_estimator.pk']
    
    for i in range (5):
        assert joblib.load (f'{path_models}/models/dummy_classifier_instance_{i}_estimator.pk')=={'threshold': i}    
    
    base_separate = 'test_ensemble_separate' 
    separate_model_paths = []
    separate_model_paths.append(f'{base_separate}/folder_a/aaa/first.pk')
    separate_model_paths.append(f'{base_separate}/folder_b/bbb/second.pk')
    separate_model_paths.append(f'{base_separate}/folder_c/ccc/third.pk')
    separate_model_paths.append(f'{base_separate}/folder_d/ddd/fourth.pk')
    separate_model_paths.append(f'{base_separate}/folder_e/eee/fifth.pk')
    for i in range (len(separate_model_paths)):
        Path(separate_model_paths[i]).parent.mkdir (parents=True, exist_ok=True)
        shutil.move (f'{path_models}/models/dummy_classifier_instance_{i}_estimator.pk', separate_model_paths[i])
    
    remove_previous_results (path_models)
    remove_previous_results (path_results)
    
    classifier = DummyClassifierInstance (path_results=path_results, path_models=path_models)
    ensembler = make_ensembler (DummyClassifierInstance, separate_model_paths=separate_model_paths, 
                                path_results=path_results)
    
    classifier.load_estimator()
    assert ensembler.parallel_models.components[0].estimator is None
    assert len(ensembler.parallel_models.components) == len(separate_model_paths)
    
    result = ensembler.fit_apply(df_ensemble)
    print (result)
    assert result.tolist()==[0.0, 0.2, 0.4, 0.6000000000000001, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0]
    
    assert os.path.exists(path_results)
    assert sorted(os.listdir(f'{path_results}/whole'))==[
        'dummy_classifier_instance_0_result.pk', 'dummy_classifier_instance_1_result.pk', 
        'dummy_classifier_instance_2_result.pk', 'dummy_classifier_instance_3_result.pk', 
        'dummy_classifier_instance_4_result.pk', 'ensembler_result.pk', 'parallel_models_result.pk', 
        'weighted_classifier_result.pk'
    ]
    assert not os.path.exists(path_models)
    
    remove_previous_results (path_results)
    remove_previous_results (path_models)
    remove_previous_results (base_separate)


# %% ../../../nbs/00_tests/core/tst.compose.ipynb 100
def test_instances_ensembler_2 ():
    path_results='test_ensembler'
    path_models='test_ensembler_models'
    
    classifier = DummyClassifierInstance (path_results=path_results, path_models=path_models)
    ensembler = make_ensembler (classifier, n_models=5)
    result = ensembler.fit_apply(df_ensemble)
    
    assert sorted(os.listdir (f'{path_results}/whole'))==[
        'dummy_classifier_instance_0_result.pk',
        'dummy_classifier_instance_1_result.pk',
        'dummy_classifier_instance_2_result.pk',
        'dummy_classifier_instance_3_result.pk',
        'dummy_classifier_instance_4_result.pk']
    
    assert sorted(os.listdir (f'{path_models}/models'))==['dummy_classifier_instance_0_estimator.pk',
         'dummy_classifier_instance_1_estimator.pk',
         'dummy_classifier_instance_2_estimator.pk',
         'dummy_classifier_instance_3_estimator.pk',
         'dummy_classifier_instance_4_estimator.pk']
    
    for i in range (5):
        assert joblib.load (f'{path_models}/models/dummy_classifier_instance_{i}_estimator.pk')=={'threshold': i}    
    
    base_separate = 'test_ensemble_separate' 
    separate_model_paths = []
    separate_model_paths.append(f'{base_separate}/folder_a/aaa/first.pk')
    separate_model_paths.append(f'{base_separate}/folder_b/bbb/second.pk')
    separate_model_paths.append(f'{base_separate}/folder_c/ccc/third.pk')
    separate_model_paths.append(f'{base_separate}/folder_d/ddd/fourth.pk')
    separate_model_paths.append(f'{base_separate}/folder_e/eee/fifth.pk')
    for i in range (len(separate_model_paths)):
        Path(separate_model_paths[i]).parent.mkdir (parents=True, exist_ok=True)
        shutil.move (f'{path_models}/models/dummy_classifier_instance_{i}_estimator.pk', separate_model_paths[i])
    
    remove_previous_results (path_models)
    remove_previous_results (path_results)
    
    classifier = DummyClassifierInstance (path_results=path_results, path_models=path_models)
    ensembler = make_ensembler (classifier, n_models=5, separate_model_paths=separate_model_paths)
    
    classifier.load_estimator()
    assert ensembler.components[0].component.estimator is None
    
    result = ensembler.fit_apply(df_ensemble)
    print (result)
    assert result.tolist()==[0.0, 0.2, 0.4, 0.6000000000000001, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0]
    
    assert os.path.exists(path_results)
    os.listdir(path_results)
    assert not os.path.exists(path_models)
    
    remove_previous_results (path_results)
    remove_previous_results (path_models)
    remove_previous_results (base_separate)

