# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/core/utils.ipynb.

# %% auto 0
__all__ = ['estimator2io', 'result2io', 'SklearnIO', 'save_csv', 'save_parquet', 'save_multi_index_parquet', 'save_keras_model',
           'save_csv_gz', 'save_json', 'MemorySaver', 'read_csv', 'read_csv_gz', 'load_keras_model', 'MemoryLoader',
           'DataIO', 'PandasIO', 'PickleIO', 'NoSaverIO', 'MemoryIO', 'data_io_factory', 'ModelPlotter', 'Profiler',
           'Comparator', 'compare', 'camel_to_snake', 'snake_to_camel', 'get_parent_class', 'get_component_hierarchy',
           'get_component_class_defaults', 'get_component_func_defaults', 'get_tree_defaults',
           'get_ds_experiment_manager']

# %% ../../nbs/core/utils.ipynb 3
__author__ = "Jaume Amores"
__copyright__ = "Copyright 2021, Johnson Controls"
__license__ = "MIT"

# %% ../../nbs/core/utils.ipynb 4
from pathlib import Path
import re
from functools import partialmethod
import time
import pickle
from IPython.display import display
from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin
from sklearn.utils import Bunch
import numpy as np
import pandas as pd
import pyarrow.parquet as pq
import pyarrow as pa
import joblib
import copy
import json
import inspect
from typing import Optional, List, Union

try:
    from graphviz import *
    imported_graphviz = True
except:
    imported_graphviz = False
    
from fastcore.meta import delegates

# dsblocks
from .data_conversion import DataConverter, PandasConverter
from ..config import bt_defaults as dflt
from ..utils.utils import set_logger, get_logging_level, replace_attr_and_store
from .callback import BaseCallback

# %% ../../nbs/core/utils.ipynb 7
def save_csv (df, path, **kwargs):
    """
    Save DataFrame `df` to csv file indicated in `path`.

    Convenience function that uses the same signature
    as joblib.dump, which is needed by `DataIO`.
    """
    df.to_csv (path, **kwargs)

# %% ../../nbs/core/utils.ipynb 8
def save_parquet (df, path, **kwargs):
    """
    Save DataFrame `df` to parquet file indicated in `path`.

    Convenience function that uses the same signature
    as joblib.dump, which is needed by `DataIO`.
    """
    df.to_parquet (path, **kwargs)

# %% ../../nbs/core/utils.ipynb 9
def save_multi_index_parquet (df, path, **kwargs):
    """
    Save DataFrame `df` to multi-index parquet file indicated in `path`.

    Convenience function that uses the same signature
    as joblib.dump, which is needed by `DataIO`.
    """
    table = pa.Table.from_pandas(df)
    pq.write_table(table, path)

# %% ../../nbs/core/utils.ipynb 10
def save_keras_model (model, path, **kwargs):
    """
    Save keras `model` to file indicated in `path`.

    Convenience function that uses the same signature
    as joblib.dump, which is needed by `DataIO`.
    """
    model.save (path, **kwargs)

# %% ../../nbs/core/utils.ipynb 11
def save_csv_gz (df, path, **kwargs):
    """
    Save DataFrame `df` to csv.gz file indicated in `path`.

    Convenience function that uses the same signature
    as joblib.dump, which is needed by `DataIO`.
    """
    df.to_csv (path, compression='gzip', **kwargs)

# %% ../../nbs/core/utils.ipynb 12
def save_json (data_dict, path, **kwargs):
    """
    Save data `data_dict` to json file indicated in `path`.

    Convenience function that uses the same signature
    as joblib.dump, which is needed by `DataIO`.
    """
    with open (path, 'wt') as f: json.dump (data_dict, f, indent=4)

# %% ../../nbs/core/utils.ipynb 13
class MemorySaver ():
    def __init__ (self, storage):
        self.storage = storage
    def __call__ (self, data, path, **kwargs):
        path = str(Path(path).resolve())
        self.storage[path] = data

# %% ../../nbs/core/utils.ipynb 14
def read_csv (path, **kwargs):
    """
    Read DataFrame from csv file indicated in `path`.

    Convenience function that uses the same signature
    as joblib.load, which is needed by `DataIO`.
    """
    return pd.read_csv (path, index_col=0, parse_dates=True, **kwargs)

# %% ../../nbs/core/utils.ipynb 15
def read_csv_gz (path, **kwargs):
    """
    Read DataFrame from csv.gz file indicated in `path`.

    Convenience function that uses the same signature
    as joblib.load, which is needed by `DataIO`.
    """
    return pd.read_csv (path, index_col=0, parse_dates=True, compression='gzip', **kwargs)

# %% ../../nbs/core/utils.ipynb 17
def load_keras_model (path, **kwargs):
    return keras.models.load_model(path)

# %% ../../nbs/core/utils.ipynb 18
class MemoryLoader ():
    def __init__ (self, storage):
        self.storage = storage
    def __call__ (self, path, **kwargs):
        path = str(Path(path).resolve())
        return self.storage[path]

# %% ../../nbs/core/utils.ipynb 20
estimator2io = dict(
    keras=dict(fitting_load_func=load_keras_model,
               fitting_save_func=save_keras_model,
               fitting_file_extension=''),
    pickle=dict(fitting_load_func=joblib.load,
                 fitting_save_func=joblib.dump,
                 fitting_file_extension='.pk'),
    default=dict(fitting_load_func=joblib.load,
                 fitting_save_func=joblib.dump,
                 fitting_file_extension='.pk'))

result2io = dict(
    pandas=dict(result_load_func=pd.read_parquet,
                result_save_func=save_multi_index_parquet,
                result_file_extension='.parquet'),
    pickle=dict(result_load_func=joblib.load,
                 result_save_func=joblib.dump,
                 result_file_extension='.pk'),
    csv=dict(result_load_func=read_csv,
                result_save_func=save_csv,
                result_file_extension='.csv'),
    default=dict(result_load_func=joblib.load,
                 result_save_func=joblib.dump,
                 result_file_extension='.pk')
)

# %% ../../nbs/core/utils.ipynb 23
#@delegates ()
class DataIO (BaseCallback):
    """
    Manages serialization and deserialization of component's result and model.
    
    Parameters
    ----------
    component : Component or None, optional
        reference to component that uses the DataIO object.
        By storing this reference, the DataIO class behaves very much
        like a callback object that keeps the state and internal parameters
        of the caller for greater flexibility.
        This reference is currently used mainly for accessing the estimator
        that is part of the component, since sometimes this estimator is
        initialized after the component's construction. The reference is
        also used for getting access to the name and class of the
        Component,  for determining the names of the files where
        results are saved.
    path_models: string, optional.
        Path to folder where trained models are stored.
    fitting_file_name: string or None, optional
        Name of the file used for storing the estimator's parameters.
        If not provided, the name of the component is used. This name
        is usually the name of the class of the component converted to
        snake case.
    fitting_file_extension: string or None, optional
        Extension of the file where the estimator's parameters are
        saved. By default, no extension is used.
    models_folder: string, optional.
        Name of models folder, appended to path_models. It is useful when 
        path_results and path_models are the same and we want to distinguish
        the folder for storing each. The results folder is given by the `split` 
        argument (see below), and the models folder by the `models_folder` 
        argument.
    fitting_load_func : function or None, optional
        Function used for loading the stored parameters of the estimator.
        If None is given, the parameters are not loaded. This function
        must have the following signature:
        estimator_parameters = fitting_load_func (path_to_file)
    fitting_save_func : function or None, optional
        Function used for saving the stored parameters of the estimator.
        If None is given, the parameters are not saved. This function
        must have the following signature:
        fitting_save_func (estimator_parameters, path_to_file)
    estimator_io: string, optional.
        Type of data loading / saving to be used for this particular estimator.
        For instance, if the estimator is a keras model, one can indicate
        estimator_io='keras', and the DataIO object will use the appropriate
        keras loading and saving functions for this estimator. For current 
        possibilities, see the definition of the dictionary `estimator_io` above.
        Other more granular ways of indicating the functions to be used for 
        loading and saving are `fitting_load_func` and `fitting_save_func`.
    load_model: bool, optional.
        Whether or not to load a pre-existent model if found in path_models.
        If loaded, the model is not fitted/trained again.
    save_model: bool, optional.
        Whether or not to save the trained model to path_models.
    path_results : string or Path or None, optional
        Path to results folder where estimator's parameters and transformed
        data are stored.
    result_file_extension : string or None, optional
        Extension of the file where the transformed data is saved.
        By default, no extension is used.
    result_file_name : string or None, optional
        Name of the file used for storing the transformed data.
        If not provided, it constructed based on the name of the component.
    result_load_func : function or None, optional
        Function used for loading the transformed data. By default, this
        function is `pd.read_parquet`. The provided function must have the
        following signature:
        transformed_data = result_load_func (path_to_file)
    result_save_func : function, optional
        Function used for saving the transformed data. By default, the
        function used is `save_multi_index_parquet` (see above). The
        provided function must have the following signature:
        result_save_func (transformed_data, path_to_file)
    result_io: string, optional.
        Type of data loading / saving to be used for the results obtained.
        For instance, result_io='pandas' will make the DataIO use pandas 
        functions for loading and saving the result, which is currently 
        expected to be a DataFrame. For current possibilities, see the 
        definition of the dictionary `result_io` above. Other more granular 
        ways of indicating the functions to be used for loading and saving are 
        `result_load_func` and `result_save_func`.
    save_splits : dict, optional
        Dictionary mapping split names to booleans. Usual split names are 'test', 
        'validation', 'training'. For each split name, if True the transformed data 
        is saved for that split.
    load_result: bool, optional.
        Whether or not to load pre-existent results found in path_results.
        If loaded, the results are not computed again.
    save_result: bool, optional.
        Whether or not to save the results to path_results.
    load: bool, optional
        If False, neither transformed data nor estimated parameters are loaded,
        regardless of what `load_result` and `load_model` indicate.
    save : bool, optional
        If False, neither transformed data nor estimated parameters are saved,
        regardless of  what `save_result` and `save_model` indicate.
    split: string, optional.
        Name of the split on which the results are obtained, typically one of `train`, 
        `validation`, `test`, or `whole`, if the results are extracted from the entire
        dataset and not from a split of it. The name of the split is appended to the 
        path_results in order to load / save results obtained for that 
        split.
    use_split_folder: bool, optional.
        Whether or not to append the string in `split` to path_results. Useful when the 
        original data is found in a path that does not include split information.
    folder: string, optional.
        Folder to be appended to path_results. Useful when we have multiple copies of the 
        same component and we want each copy to store the results in a different folder.
    stop_propagation: bool, optional.
        If True, I/O parameters propagated from the root MultiComponent are not propagated
        through the subtree whose root is the component associated with this DataIO
        object.
    memory: bool, optional.
        Whether or not to also store the results and models in memory.
    """

    specific_params = ['path_results', 'load_result', 'save_result', 'path_models', 'load_model', 'save_model',
                       'load', 'save']

    def __init__ (self,
                  component=None,
                  memory=dflt.memory,
                 **kwargs):

        self.component = component
        self.memory_io = None

        if component is not None:
            config = self.component.obtain_config_params (**kwargs)
            self._init (**config)
            self.setup (component)
        else:
            self._init (**kwargs)
            self._initial_kwargs = kwargs
            
        if memory: self.memory_io = MemoryIO (component=component, **kwargs)

    def _init (self,
               path_models=dflt.path_models,
               fitting_file_name=None,
               fitting_file_extension=None,
               models_folder=dflt.models_folder,
               model_suffix=dflt.model_suffix,
               fitting_load_func=None,
               fitting_save_func=None,
               estimator_io=dflt.estimator_io,
               load_model=dflt.load_model,
               save_model=dflt.save_model,

               path_results=dflt.path_results,
               result_file_extension=None,
               result_file_name=None,
               result_suffix=dflt.result_suffix,
               result_load_func=None,
               result_save_func=None,
               result_io=dflt.result_io,
               save_splits=dflt.save_splits,
               load_result=dflt.load_result,
               save_result=dflt.save_result,

               load=dflt.load,
               save=dflt.save,
               split=dflt.split,
               use_split_folder=dflt.use_split_folder,
               folder='',
               stop_propagation=False,
               init_names={'_init','__init__'},
               **kwargs):
        
        super().__init__ (init_names=init_names, **kwargs)

        self.path_models = path_models
        self.path_results = path_results

        # saving / loading estimator parameters
        self.fitting_file_name = fitting_file_name
        self.fitting_file_extension = fitting_file_extension
        self.models_folder = models_folder
        self.model_suffix = model_suffix
        self.fitting_load_func = fitting_load_func
        self.fitting_save_func = fitting_save_func
        self.estimator_io = estimator_io

        # saving / loading transformed data
        self.result_file_extension = result_file_extension
        self.result_suffix = result_suffix
        self.result_load_func = result_load_func
        self.result_save_func = result_save_func
        self.result_io = result_io

        # saving / loading transformed training data
        self.set_save_splits (save_splits)

        # saving / loading transformed test data
        self.result_file_name = result_file_name

        # whether the transformation has been applied to training data (i.e., to be saved in training path)
        # or to test data (i.e,. to be saved in test path)
        self.split = split
        self.use_split_folder = use_split_folder

        # global saving and loading
        self.set_save (save)
        self.set_load (load)
        self.set_save_model (save_model)
        self.set_load_model (load_model)
        self.set_save_result (save_result)
        self.set_load_result (load_result)

        self.set_folder (folder)
        self.stop_propagation = stop_propagation

    def setup (self, component=None):
        """
        Initialize remaining fields given `component` from which data is saved/loaded.

        We use the name of `component` for inferring the name of the files
        where the data is saved / loaded. The reason why we need to do this in a
        separate setup method is because the component might be unknown when
        constructing the DataIO object in the subclasses below (see for instance
        SklearnIO subclass)
        """
        if self.memory_io is not None:  self.memory_io.setup (component=component)
        self.component = component

        if hasattr(self, '_initial_kwargs'):
            config = self.component.obtain_config_params (**self._initial_kwargs)
            self._init (**config)
            del self._initial_kwargs

        if self.fitting_file_extension is None:
            self.fitting_file_extension = estimator2io[self.estimator_io]['fitting_file_extension']
        if self.fitting_load_func is None:
            self.fitting_load_func = estimator2io[self.estimator_io]['fitting_load_func']
        if self.fitting_save_func is None:
            self.fitting_save_func = estimator2io[self.estimator_io]['fitting_save_func']

        if self.result_file_extension is None:
            self.result_file_extension = result2io[self.result_io]['result_file_extension']
        if self.result_load_func is None:
            self.result_load_func = result2io[self.result_io]['result_load_func']
        if self.result_save_func is None:
            self.result_save_func = result2io[self.result_io]['result_save_func']

        # configuration for saving / loading fitted estimator
        if self.fitting_file_name is None:
            self.set_fitting_file_name (self.component.name)

        # configuration for saving / loading result of transforming training data
        if self.result_file_name is None:
            self.set_result_file_name (self.component.name)

        if self.path_results is not None:
            self.path_results = Path(self.path_results).resolve()

        if self.path_models is None:
            self.path_models = self.path_results
        else:
            self.path_models = Path(self.path_models).resolve()

    def get_path_model_file (self, path_models=None, fitting_file_name=None):
        path_models = self.path_models if path_models is None else Path(path_models).resolve()
        fitting_file_name = self.fitting_file_name if fitting_file_name is None else fitting_file_name
        if path_models is not None:
            path_model_file = path_models / self.folder / self.models_folder / fitting_file_name
        else:
            path_model_file = None
        return path_model_file

    def _load_estimator (self, *, path_models=None, fitting_file_name=None):
        """Load estimator parameters."""
        path_model_file = self.get_path_model_file (path_models=path_models,
                                                    fitting_file_name=fitting_file_name)
        estimator, _ = self._load (path=path_model_file,
                                load_func=self.fitting_load_func)
        return estimator
    def load_estimator (self, **kwargs):
        """
        Load estimator parameters.
        
        Parameters
        ----------
            path_models: str
            fitting_file_name: str
        Returns
        -------
            Loaded estimator.
        """
        estimator = None
        if self.memory_io is not None: 
            estimator = self.memory_io.load_estimator (**kwargs)
        return self._load_estimator (**kwargs) if estimator is None else estimator

    def load_estimators (self, **kwargs):
        all_exist = True
        for component in self.component.components:
            estimator = component.data_io.load_estimator (**kwargs)
            all_exist = all_exist and (estimator is not None)
        return all_exist

    def _save_estimator (self, *, path_models=None, fitting_file_name=None):
        if self.component.estimator is not None:
            path_model_file = self.get_path_model_file (path_models=path_models,
                                                        fitting_file_name=fitting_file_name)
            self._save (path_model_file, self.fitting_save_func, self.component.estimator)

    def save_estimator (self, **kwargs):
        """
        Save estimator parameters.
        
        Parameters
        ----------
            path_models: str
            fitting_file_name: str
        """
        if self.memory_io is not None: 
            self.memory_io.save_estimator (**kwargs)
        self._save_estimator (**kwargs)

    def get_path_result_file (self, split=None, path_results=None, result_file_name=None):
        self_split = self.split if self.use_split_folder else ''
        split = self_split if split is None else split if self.use_split_folder else ''
        path_results = self.path_results if path_results is None else Path(path_results).resolve()
        result_file_name = self.result_file_name if result_file_name is None else result_file_name
        if path_results is not None:
            path_result_file = path_results / self.folder / split / result_file_name
        else:
            path_result_file = None
        return path_result_file

    def _load_result (self, split:Optional[str]=None, path_results:Optional[str]=None, 
                      result_file_name:Optional[str]=None, return_found:bool=False):
        path_result_file = self.get_path_result_file (split=split, path_results=path_results,
                                                      result_file_name=result_file_name)
        result, found = self._load (path_result_file, self.result_load_func)
        return (result, found) if return_found else result
    
    def load_result (self, *, return_found=False, **kwargs):
        """
        Load transformed data.

        Transformed training data is loaded if self.training_data_flag=True,
        otherwise transformed test data is loaded.
        
        Parameters
        ---------
            split: str or None
            path_results: str or None
            result_file_name: str or None
            return_found: bool
        Returns
        -------
            loaded data item
        """
        found = False
        if self.memory_io is not None: 
            (result, found) = self.memory_io.load_result (return_found=True, **kwargs)
        if not found:
            return self._load_result (return_found=return_found, **kwargs)
        else:
            return (result, found) if return_found else result

    def _save_result (self, result, *, split=None, path_results=None, result_file_name=None):
        """
        Save transformed data.

        Transformed training data is saved if self.training_data_flag=True,
        otherwise transformed test data is saved.
        """
        path_result_file = self.get_path_result_file (split=split, path_results=path_results,
                                                      result_file_name=result_file_name)
        self._save (path_result_file, self.result_save_func,
                    result)
    
    def save_result (self, result, **kwargs):
        """
        Save transformed data.

        Transformed training data is saved if self.training_data_flag=True,
        otherwise transformed test data is saved.
        
        Parameters
        ----------
        result
        split: str or None
        path_results: str or None
        result_file_name: str or None
        """
        if self.memory_io is not None: 
            self.memory_io.save_result (result, **kwargs)
        self._save_result (result, **kwargs)

    def can_load_model (self, load=None):
        return load if load is not None else (self.load_flag and self.load_model_flag)

    def can_load_result (self, load=None):
        return load if load is not None else (self.load_flag and self.load_result_flag)

    def can_save_model (self, save=None):
        return save if save is not None else (self.save_flag and self.save_model_flag)

    def can_save_result (self, save=None, split=None):
        split = self.split if split is None else split
        return save if save is not None else (self.save_flag and
                                              self.save_result_flag and
                                              self.save_splits.get(split, True))

    def _exists (self, path):
        return path.exists ()
    
    def _load (self, path, load_func):
        if (path is not None) and self._exists (path):
            self.component.logger.info (f'loading from {path}')
            return load_func (path), True
        else:
            return None, False

    def _save (self, path, save_func, item):
        if (path is not None) and (save_func is not None):
            self.component.logger.info (f'saving to {path}')
            # create parent directory if it does not exist
            if not isinstance (self, MemoryIO):
                path.parent.mkdir(parents=True, exist_ok=True)
            # save data using save_func
            try:
                save_func (item, path)
            except Exception as e:
                self.component.logger.warning (f'could not write to {path}, exception: {e}')

    def write_git_hash_and_root_path (self, path_results_set, root_path, current_hash, type_path='results'):
        path_results =  self.path_results if type_path=='results' else self.path_models
        file_paths = []
        if (path_results is not None) and (str(path_results) not in path_results_set) and (path_results not in path_results_set):
            path_results_set.add (path_results)
            path_results_set.add (str(path_results))
            if type_path=='results':
                path_metadata = self.get_path_result_file (result_file_name='config/root_metadata.json')
            else:
                path_metadata = self.get_path_model_file (fitting_file_name='config/root_metadata.json')
            metadata = dict (root_path=str(root_path), current_hash=current_hash)
            if ((type_path=='results' and self.can_save_result ()) or
                (type_path=='models' and self.can_save_model ())):
                self._save (path_metadata, save_json,  metadata)
                if path_metadata.exists (): file_paths = [path_metadata]
        if type_path=='results':
            file_paths.extend (self.write_git_hash_and_root_path (path_results_set, root_path, current_hash,
                type_path='models'))
        return file_paths

    # ********************************
    # setters
    # ********************************
    def set_split (self, split):
        self.split = split

    def set_save_splits (self, save_splits):
        self.save_splits = save_splits

    def set_full_path_results (self, path_results):
        path_results = Path(path_results).resolve()
        self.result_file_name = path_results.name
        self.path_results = path_results.parent
        if self.folder is not None and self.folder != '':
            folder = self.folder
            self.folder = ''
            self.path_models = self.path_models / folder
        self.use_split_folder = False
        self.component.path_results = self.path_results

    def set_full_path_models (self, path_models):
        path_models = Path(path_models).resolve()
        self.fitting_file_name = path_models.name
        self.path_models = path_models.parent
        if self.folder is not None and self.folder != '':
            folder = self.folder
            self.folder = ''
            self.path_results = self.path_results / folder
        self.models_folder = ''
        self.component.path_models = self.path_models

    def set_path_results (self, path_results):
        self.path_results = path_results
        if self.memory_io is not None:
            self.memory_io.path_results = path_results
        if self.path_results is not None:
            self.path_results = Path(self.path_results).resolve()
            if self.memory_io is not None:
                self.memory_io.path_results = Path(self.path_results).resolve()
            if self.path_models is None:
                self.set_path_models (path_results)
        self.component.path_results = self.path_results
        self.component.path_models = self.path_models

    def set_path_models (self, path_models):
        self.path_models = path_models
        if self.memory_io is not None:
            self.memory_io.path_models = path_models
        if self.path_models is not None:
            self.path_models = Path(self.path_models).resolve()
            if self.memory_io is not None:
                self.memory_io.path_models = Path(self.path_models).resolve()
        else:
            self.path_models = self.path_results
            if self.memory_io is not None:
                self.memory_io.path_models = path_models
        self.component.path_results = self.path_results
        self.component.path_models = self.path_models


    # global saving and loading
    def set_save (self, save):
        self.save_flag = save
        if not save:
            self.set_save_model (False)
            self.set_save_result (False)
        if self.memory_io is not None: 
            self.memory_io.set_save (save)

    def set_load (self, load):
        self.load_flag = load
        if not load:
            self.set_load_model (False)
            self.set_load_result (False)
        if self.memory_io is not None: 
            self.memory_io.set_load (load)

    def set_save_model (self, save):
        self.save_model_flag = save if self.save_flag else False
        if self.memory_io is not None: 
            self.memory_io.set_save_model (save)

    def set_load_model (self, load):
        self.load_model_flag = load if self.load_flag else False
        if self.memory_io is not None: 
            self.memory_io.set_load_model (load)

    def set_save_result (self, save):
        self.save_result_flag = save if self.save_flag else False
        if self.memory_io is not None: 
            self.memory_io.set_save_result (save)

    def set_load_result (self, load):
        self.load_result_flag = load if self.load_flag else False
        if self.memory_io is not None: 
            self.memory_io.set_load_result (load)

    def set_fitting_file_name (self, name):
        self.fitting_file_name = f'{name}{self.model_suffix}{self.fitting_file_extension}'

    def set_result_file_name (self, name):
        self.result_file_name = f'{name}{self.result_suffix}{self.result_file_extension}'

    def set_file_names (self, name):
        self.set_fitting_file_name (name)
        self.set_result_file_name (name)

    def _get_folder_name (self, folder):
        if folder == '__class__':
            if self.component is not None:
                return self.component.name
            else:
                self.logger.warning (f'folder {folder} set on data_io without associated component')
                return ''
        else:
            return folder

    def set_folder (self, folder):
        self.folder = self._get_folder_name (folder)
        
    def set_memory (self, memory):
        if memory and self.memory_io is None: 
            if True:
                keys = set(self.__stored_args__)
                #keys |= {'path_models','path_results', 'load','load_result','save','save_result'}
                keys = keys - {'component'}
                memory_kwargs = {k:getattr(self, k) for k in keys if hasattr (self, k)}
                self.memory_io = MemoryIO (component=self.component,
                                           **memory_kwargs)
            else:
                self.memory_io = MemoryIO (component=self.component,
                                           **self.__stored_args__)
        elif not memory:
            self.memory_io = None

    def chain_folders (self, folder):
        folder = self._get_folder_name (folder)
        if folder=='':
            return
        if self.folder == '':
            self.folder = folder
        else:
            self.folder = f'{folder}/{self.folder}'

    def exists_result (self, split=None, path_results=None, result_file_name=None,
                       include_memory=False):
        path_result_file = self.get_path_result_file (split=split, path_results=path_results,
                                                      result_file_name=result_file_name)
        return (path_result_file is not None and 
                (self._exists (path_result_file)  or 
                 (include_memory and self.memory_io is not None and self.memory_io._exists(path_result_file))))

    def exists_estimator (self, path_models=None, fitting_file_name=None, include_memory=False):
        """Load estimator parameters."""
        path_model_file = self.get_path_model_file (path_models=path_models,
                                                    fitting_file_name=fitting_file_name)
        return (path_model_file is not None and 
                (self._exists(path_model_file) or
                (include_memory and self.memory_io is not None and self.memory_io._exists(path_model_file))))

# %% ../../nbs/core/utils.ipynb 33
#@delegates ()
class PandasIO (DataIO):
    """
    Saves results in parquet format by default.

    Results are supposed to be in Pandas DataFrame format.
    """
    def __init__ (self,
                  result_file_extension='.parquet',
                  result_load_func=pd.read_parquet,
                  result_save_func=save_multi_index_parquet,
                  **kwargs):

        super().__init__ (result_file_extension=result_file_extension,
                          result_load_func=result_load_func,
                          result_save_func=result_save_func,
                          **kwargs)

# %% ../../nbs/core/utils.ipynb 35
#@delegates ()
class PickleIO (DataIO):
    """
    DataIO that uses pickle format for saving / loading the estimator.

    It does not restrict the format for saving / loading the result of
    the transformation.
    """

    def __init__ (self,
                  fitting_file_extension='.pk',
                  fitting_load_func=joblib.load,
                  fitting_save_func=joblib.dump,

                  result_file_extension='.pk',
                  result_load_func=joblib.load,
                  result_save_func=joblib.dump,
                  **kwargs):

        super().__init__ (fitting_file_extension=fitting_file_extension,
                          fitting_load_func=fitting_load_func,
                          fitting_save_func=fitting_save_func,

                          result_file_extension=result_file_extension,
                          result_load_func=result_load_func,
                          result_save_func=result_save_func,
                          **kwargs)

SklearnIO = PickleIO

# %% ../../nbs/core/utils.ipynb 37
#@delegates ()
class NoSaverIO (DataIO):
    """DataIO that does not load or save anything."""
    def __init__ (self,
                  load=False,
                  save=False,
                  force_load=False,
                  force_save=False,
                  **kwargs):
        super().__init__ (load=force_load if force_load else False,
                          save=force_save if force_save else False,
                          **kwargs)

# %% ../../nbs/core/utils.ipynb 39
#@delegates ()
class MemoryIO (DataIO):
    """DataIO that retrieves from and stores in memory."""
    def __init__ (self, include_memory_io=False, **kwargs):
        super().__init__ (include_memory_io=False, **kwargs)
        self.storage = {}
        self.memory_loader = MemoryLoader (self.storage)
        self.memory_saver = MemorySaver (self.storage)
        
        self.fitting_load_func = self.memory_loader
        self.result_load_func = self.memory_loader
        self.fitting_save_func = self.memory_saver
        self.result_save_func = self.memory_saver
    def _exists (self, path):
        path = str(Path(path).resolve())
        return path in self.storage

# %% ../../nbs/core/utils.ipynb 53
def data_io_factory (data_io, component=None, **kwargs):
    if type(data_io) is str:
        cls = eval(data_io)
    elif type(data_io) is type:
        cls = data_io
    elif isinstance (data_io, DataIO):
        data_io = copy.copy(data_io)
        data_io.setup (component=component)
        return data_io
    else:
        raise ValueError (f'invalid converter {data_io}, must be str, '
                           'class or object instance of DataIO')
    data_io = cls(component=component, **kwargs)
    return data_io

# %% ../../nbs/core/utils.ipynb 57
#@delegates ()
class ModelPlotter (BaseCallback):
    """Helper class that provides information about the component used for plotting the pipeline diagram."""
    def __init__ (self,
                  component=None,
                  # diagram options
                  diagram_node_name = None,
                  diagram_edge_name = '',
                  diagram_module_path = '',
                  **kwargs):

        super().__init__ (**kwargs)
        self.set_component (component)

        # diagram options
        if diagram_node_name is None:
            diagram_node_name = self.component.class_name
        self.diagram_node_name = diagram_node_name
        self.diagram_edge_name = diagram_edge_name
        self.diagram_module_path = diagram_module_path
        self.result_shape = {}

    def set_component (self, component=None):
        self.component = component

    def get_node_name (self):
        return self.diagram_node_name

    def get_edge_name (self, split=None, load_data=True):
        split = self.component.data_io.split if split is None else split
        result_shape = self.result_shape[split] if split in self.result_shape else None
        if result_shape is None:
            if load_data:
                df = self.component.data_io.load_result(split=split)
                if (df is not None) and hasattr(df, 'shape'):
                    result_shape = self.result_shape[split] = df.shape

        if result_shape is None:
            return self.diagram_edge_name
        else:
            return f'{self.diagram_edge_name} {result_shape}'

    def get_module_path (self):
        return self.diagram_module_path

# %% ../../nbs/core/utils.ipynb 59
#@delegates ()
class Profiler (BaseCallback):
    def __init__ (self, component, do_profiling=True, **kwargs):
        super().__init__ (**kwargs)
        self.component = component
        self.name = component.name
        if hasattr(component, 'hierarchy_level'):
            self.hierarchy_level = component.hierarchy_level
        else:
            self.hierarchy_level = 0
        self.do_profiling=do_profiling
        self.times = Bunch(sum=pd.DataFrame (),
                          max=pd.DataFrame (),
                          min=pd.DataFrame (),
                          number=pd.DataFrame ())
        keys = list(self.times.keys()).copy()
        for k in keys:
            self.times[f'novh_{k}']=pd.DataFrame ()

    def start_timer (self):
        self.time = time.time()

    def start_no_overhead_timer (self):
        self.no_overhead_time = time.time()

    def finish_timer (self, method, split):
        self._finish_timer (method, split, suffix='', measured_time=self.time)

    def finish_no_overhead_timer (self, method, split):
        self._finish_timer (method, split, suffix='novh_', measured_time=self.no_overhead_time)

    def _finish_timer (self, method, split, suffix='', measured_time=None):
        if method.startswith('_'):
            method = method[1:]
        total_time = time.time() - measured_time
        df=self.times[f'{suffix}sum']
        if method in df.index and split in df.columns:
            df.loc[method, split] += total_time
            self.times[f'{suffix}number'].loc[method, split] += 1
            self.times[f'{suffix}max'].loc[method, split] = max(self.times[f'{suffix}max'].loc[method, split], total_time)
            self.times[f'{suffix}min'].loc[method, split] = min(self.times[f'{suffix}min'].loc[method, split], total_time)
        else:
            df.loc[method, split] = total_time
            self.times[f'{suffix}number'].loc[method, split] = 1
            self.times[f'{suffix}max'].loc[method, split] = total_time
            self.times[f'{suffix}min'].loc[method, split] = total_time

    def _compute_avg (self, df_sum, df_number):
        df_avg = df_sum.copy()
        columns = [c for c in df_avg.columns if c != ('leaf', '')]
        df_avg[columns] = df_avg[columns] / df_number[columns]
        return df_avg

    def retrieve_times (self, is_leaf=False):
        retrieved_times = Bunch()
        for k in self.times:
            df = self.times[k]
            columns = pd.MultiIndex.from_product([list(df.columns),list(df.index)])
            index = pd.MultiIndex.from_product([[self.hierarchy_level], [self.name]])
            df = pd.DataFrame (index=index,
                               columns = columns, data=df.values.reshape(1,-1))
            df['leaf']=is_leaf
            retrieved_times[k] = df

        retrieved_times['avg']= self._compute_avg (retrieved_times['sum'],
                                                   retrieved_times['number'])
        retrieved_times['novh_avg']= self._compute_avg (retrieved_times['novh_sum'],
                                                        retrieved_times['novh_number'])
        return retrieved_times

    def combine_times (self, df_list):
        df_dict = Bunch()
        for k in df_list[0].keys():
            df_dict[k] = pd.concat([x[k] for x in df_list])
            df_dict[k] = df_dict[k].sort_index()
        df_novh_avg = df_dict['novh_avg']
        df_dict['no_overhead_total'] = df_novh_avg[df_novh_avg.leaf].sum(axis=0)

        df_avg = self.retrieve_times ()['avg']
        df_dict['overhead_total'] = df_avg -  df_dict['no_overhead_total']
        df_dict['no_overhead_total'] = df_dict['no_overhead_total'].to_frame().T
        return df_dict

    def analyze_overhead (self, df_dict):
        def data (df):
            df = df[[c for c in df if c[0] != 'leaf']]
            return df.values
        component_ovh = data (df_dict.avg)-data(df_dict.novh_avg)

        df_dict['component_ovh'] = pd.DataFrame (component_ovh,
                                                 columns=[c for c in df_dict.avg if c[0] != 'leaf'],
                                                 index=df_dict.avg.index)
        is_leaf=df_dict.avg[('leaf','')]

        total_non_leaf_time = np.nansum(data(df_dict.avg[~is_leaf]).ravel())
        total_leaf_time = np.nansum(data(df_dict.avg[is_leaf]).ravel())
        total_non_leaf_ovh = total_non_leaf_time - total_leaf_time

        total_ovh=np.nansum(data(df_dict.overhead_total).ravel())

        total_leaf_ovh = np.nansum(component_ovh[is_leaf.values].ravel())

        df_dict['overhead_summary'] = pd.DataFrame ({'total': [total_ovh],
                                                     'total leaf': [total_leaf_ovh],
                                                     'total non-leaf': [total_non_leaf_ovh],
                                                     'remaining / unexplained': [
                                                         total_ovh-total_leaf_ovh-total_non_leaf_ovh]},
                                                    index=[0])
        return df_dict

# %% ../../nbs/core/utils.ipynb 64
#@delegates ()
class Comparator (BaseCallback):
    def __init__ (self, component=None, data_io=None, name='comparator', **kwargs):
        super().__init__ (**kwargs)
        if component is not None:
            self.component = component
            self.logger = component.logger
            self.name = component.name
            self.data_io = component.data_io
        else:
            self.component = None
            self.logger = set_logger ('comparator', filename=None)
            self.name = name
            if data_io is not None:
                self.data_io = data_io_factory (data_io)

    def compare_objects (self, left, right, message='', **kwargs):
        if left != right:
            return message + f'{left}!={right}'
        else:
            return ''

    def compare (self, left, right, message='', rtol=1e-07, atol=0, **kwargs):
        if not type(left)==type(right):
            return f'{message}{type(left)}!={type(right)}'
        if isinstance(left, tuple) or isinstance(left, list):
            if len(left) != len(right):
                return f'{message}different length: {len(left)}!={len(right)}:\nleft: {left}\nright: {right}'
            try:
                left = np.array(left, dtype=float)
                right = np.array(right, dtype=float)
            except:
                for i, (x, y) in enumerate(zip(left, right)):
                    result = self.compare (x, y, message=message+f'[{i}] ', rtol=rtol, atol=atol, **kwargs)
                    if len(result) > 0:
                        return result
                return ''
        elif isinstance (left, dict):
            if sorted(left.keys()) != sorted(right.keys()):
                return f'{message}{sorted(left.keys())}!={sorted(right.keys())}'
            for k in left:
                result = self.compare (left[k], right[k], message=message+f'[{k}] ', rtol=rtol, atol=atol,
                                       **kwargs)
                if len(result) > 0:
                    return result
            return ''

        if isinstance (left, np.ndarray):
            if (left.dtype == np.object) and (right.dtype == np.object):
                if left.tolist() != right.tolist():
                    return message + f'{left}!={right}'
                else:
                    return ''
            else:
                try:
                    np.testing.assert_allclose (left, right, rtol=rtol, atol=atol)
                    return ''
                except AssertionError as e:
                    return message + str(e)
        elif isinstance (left, pd.DataFrame):
            try:
                pd.testing.assert_frame_equal (left, right)
                return ''
            except AssertionError as e:
                return message + str(e)
        elif isinstance (left, str):
            if left != right:
                return message + f'{left}!={right}'
            else:
                return ''
        elif np.isscalar (left) and np.isreal (left):
            if np.isclose (left, right, rtol=rtol, atol=atol):
                return ''
            else:
                return message + f'{left} not close to {right}'
        else:
            return self.compare_objects (left, right, message=message, rtol=1e-07, atol=0, **kwargs)
        return ''

    def assert_equal (self, item1, item2=None, split=None, raise_error=True, verbose=None,
                      **kwargs):
        """
        Check whether the transformed data is the same as the reference data stored in given path.

        Parameters
        ----------
        path_reference_results: str
            Path where reference results are stored. The path does not include the
            file name, since this is stored as a field of data_io.
        assert_equal_func: function, optional
            Function used to check whether the values are the same. By defaut,
            `pd.testing.assert_frame_equal` is used, which assumes the data type is
            DataFrame.

        """
        if verbose is not None:
            self.logger.setLevel(get_logging_level (verbose))
        self.logger.info (f'comparing results for {self.name}')
        if item2 is None:
            item2 = item1
            self.logger.info (f'loading our results...')
            item1 = self.data_io.load_result (split=split)
        elif type(item1) is str:
            self.logger.info (f'loading others results...')
            item1 = self.data_io.load_result (split=split, path_results=item1)
        if type(item2) is str:
            item2 = self.data_io.load_result (split=split, path_results=item2)
        difference = self.compare (item1, item2, **kwargs)
        if len(difference) == 0:
            self.logger.info (f'Results are equal.\n')
            if not raise_error:
                if verbose is not None:
                    self.logger.setLevel(get_logging_level (self.component.verbose))
                return True
        else:
            if raise_error:
                raise AssertionError (f'Component {self.name} => results are different: {difference}')
            else:
                self.logger.warning (f'Component {self.name} => results are different: {difference}')
                if verbose is not None:
                    self.logger.setLevel(get_logging_level (self.component.verbose))
                return False

# %% ../../nbs/core/utils.ipynb 69
def compare (left, right, **kwargs):
    c = Comparator ()
    return c.compare (left, right, **kwargs)

# %% ../../nbs/core/utils.ipynb 74
def camel_to_snake (name):
    """
    Convert CamelCase to snake_case.

    Used for converting classes names to file names where
    the corresponding computation is stored:
        https://stackoverflow.com/questions/1175208/elegant-python-function-to-convert-camelcase-to-snake-case
    """
    s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', name)
    return re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()

def snake_to_camel (name):
    return ''.join(word.title() for word in name.split('_'))

# %% ../../nbs/core/utils.ipynb 77
def get_parent_class (classes):
    if len(classes)==0: return None
    if type(classes) is set: classes = list(classes)
    parent = classes[0]
    parent_obj = parent ()
    for c in classes[1:]:
        if isinstance(parent_obj, c): 
            parent = c
            parent_obj = parent ()
    return parent

# %% ../../nbs/core/utils.ipynb 79
def get_component_hierarchy (component):
    from dsblocks.core.components import __all__ as all_components, Component
    from dsblocks.core.compose import __all__ as all_compose, MultiComponent
    all_internal_classes = set(all_components).union (all_compose)
    
    def add_classes (tuples, internal_classes, external_classes):
        if tuples[0].__name__ in all_internal_classes:
            internal_classes.add (tuples[0])
        else:
            external_classes.add (tuples[0])
        other_internal_classes = {x for x in tuples[1] if x.__name__ in all_internal_classes}
        other_external_classes = set(tuples[1]).difference (other_internal_classes)
        internal_classes |= other_internal_classes
        external_classes |= other_external_classes
        return internal_classes, external_classes
    
    component_class = component if type(component) is type else component.__class__
    hierarchy = inspect.getclasstree ([component_class])
    internal_classes, external_classes = set(), set()
    for tuples in hierarchy:
        if type(tuples) is tuple:
            internal_classes, external_classes = add_classes (tuples, internal_classes, external_classes)
        elif type(tuples) is list:
            for tuples_from_list in tuples:
                internal_classes, external_classes = add_classes (tuples_from_list, internal_classes, 
                                                                  external_classes)
    if Component not in internal_classes:
        if MultiComponent in internal_classes:
            internal_classes_component, external_classes_component = get_component_hierarchy (MultiComponent)
        else:
            if internal_classes==set():
                internal_classes_component=set()
                external_classes_component=set()
            else:
                internal_classes_component, external_classes_component = get_component_hierarchy (
                    get_parent_class (internal_classes))
        internal_classes |= internal_classes_component
        external_classes |= external_classes_component
    return internal_classes, external_classes

# %% ../../nbs/core/utils.ipynb 86
def get_component_class_defaults (component, exclude_internals=False, include_internals=False):
    internal_classes, external_classes = get_component_hierarchy (component)
    if exclude_internals or include_internals:
        internal_classes |= set ([c.__class__ for c in component.callbacks])
    if exclude_internals:
        internal_args = set()
        for c in internal_classes:
            internal_args |= set(inspect.signature (c.__init__).parameters.keys())
    if include_internals:
        external_classes |= internal_classes
    defaults = {}
    for c in external_classes:
        pars = inspect.signature (c.__init__).parameters
        defaults.update ({k: pars.get(k).default 
                          for k in pars.keys() if pars.get(k).default is not inspect._empty})
    if exclude_internals:
        defaults = {k:defaults[k] for k in set(defaults)-internal_args}
    return defaults

# %% ../../nbs/core/utils.ipynb 88
def get_component_func_defaults (component):
    functions = [f for f in [getattr(component,'fit_func',None), 
                             getattr(component,'result_func',None), 
                             getattr(component,'fit_apply_func',None)]
                 if f is not None]
    defaults = {}
    for f in functions:
        pars = inspect.signature (f).parameters
        defaults.update ({k: pars.get(k).default 
                          for k in pars.keys() if pars.get(k).default is not inspect._empty})
    return defaults

# %% ../../nbs/core/utils.ipynb 90
def get_tree_defaults (component, **kwargs):
    from dsblocks.core.compose import MultiComponent
    defaults = get_component_class_defaults (component, **kwargs)
    defaults.update (get_component_func_defaults (component))
    if isinstance (component, MultiComponent):
        for c in component.components:
            defaults.update (get_tree_defaults (c, **kwargs))
    return defaults

# %% ../../nbs/core/utils.ipynb 94
def get_ds_experiment_manager (*args_function, **kwargs_function):
    # define inside function to avoid circular dependency
    from hpsearch.experiment_manager import ExperimentManager
    #@delegates ()
    class DSExperimentManager (ExperimentManager):
        def __init__ (self, 
                      component, 
                      path_experiments=None, 
                      folder=None, 
                      include=None,
                      exclude=None,
                      run_number=0,
                      construction_args={},
                      non_pickable_fields=None,
                      include_function_name=True,
                      **kwargs):
            
            # ------------------
            # Callback code
            replace_attr_and_store (base_class=DSExperimentManager, replace_generic_attr=False,
                                    but='path_experiments, folder')
            self.parent = component
            if not hasattr(self.parent, 'callbacks'):
                self.parent.callbacks = []
            self.parent.callbacks += [self]
            
            if path_experiments is None: path_experiments = component.path_results/component.name
            if folder is None: folder = component.name
            init_non_pickable_fields = ['component', 'init_other_parameters', 'root', 'parent']
            non_pickable_fields = (init_non_pickable_fields if non_pickable_fields is None 
                                   else non_pickable_fields + init_non_pickable_fields)
            super().__init__(folder=folder, path_experiments=path_experiments, 
                             non_pickable_fields=non_pickable_fields, **kwargs)
            self.component = component
            self.include = set(include) if include is not None else None
            self.exclude = set(exclude) if exclude is not None else None
            self.run_number = run_number
            if include is None:
                if self.exclude is None: self.exclude = set()
                self.exclude |= set(inspect.signature (self.component.data_io._init).parameters.keys())
                self.exclude |= set(inspect.signature (self.component.data_converter.__init__).parameters.keys())
                self.exclude |= set(inspect.signature (DataConverter.__init__).parameters.keys())
                self.exclude |= set(inspect.signature (self.__init__).parameters.keys())
                internal_classes, external_classes = get_component_hierarchy (self.component)
                for c in internal_classes:
                    self.exclude |= set(inspect.signature (c.__init__).parameters.keys())
                self.exclude |= {x.__name__ for x in external_classes}
            self_args = {**kwargs, **self.component.__stored_args__, **construction_args}
            self.init_parameters, self.init_other_parameters = self.extract_parameters (**self_args)
            self.init_other_parameters = {k:self.init_other_parameters[k] 
                                          for k in set(self.init_other_parameters).difference(non_pickable_fields)
                                          }
            self.include_function_name = include_function_name
            
            self._get_default_parameters_from_tree ()
        
        def _get_default_parameters_from_tree (self):
            if not hasattr(self, 'defaults'): self.defaults = {}
            defaults = get_tree_defaults (self.component)
            defaults.update (self.defaults)
            self.defaults = defaults
        
        def get_default_parameters (self, parameters):
            return self.defaults
        
        def run_experiment (self, parameters={}, path_results=None):
            func = parameters.get ('function', 'apply')
            if hasattr (self.component, 'set_path_results'):
                self.component.set_path_results (path_results)
                self.component.set_path_models (path_results)
            else:
                self.component.data_io.set_path_results (path_results)
                self.component.data_io.set_path_models (path_results)
            X = parameters.get('_X_', ())
            if parameters.get ('find_data_source', False):
                self.component.find_last_result ()
            if func == 'apply':
                result = self.component.apply (*X, **parameters)
            elif func == 'fit':
                self.component.fit (*X, **parameters)
                result = None
            elif func == 'fit_apply':
                result = self.component.fit_apply (*X, **parameters)
            if type(result) is dict:
                result2 = {}
                for k in result.keys():
                    if type(result[k]) is dict:
                        for k2 in result[k].keys():
                            result2[f'{k}_{k2}'] = result[k][k2]
                    else:
                        result2[k] = result[k]
                result = result2
                for k in result.keys():
                    assert np.isscalar (result[k]) and np.isreal (result[k]), 'returned dictionary must be flat and have numeric values'
                return result
            else:
                if np.isscalar (result) and np.isreal (result):
                    return {self.key_score: result}
                else:
                    return {self.key_score: None}
            
        def extract_parameters (self, **kwargs):
            if self.include is not None:
                parameters = {k: kwargs[k] for k in self.include.intersection (kwargs)}
            elif self.exclude is not None:
                parameters = {k: kwargs[k] for k in set(kwargs).difference (self.exclude)}
            else:
                parameters = kwargs
            other_parameters = {k: kwargs[k] for k in set(kwargs).difference (parameters)}
            return parameters, other_parameters
            
        def run (self, run_number=-1, _X_=(), y=None, function='apply', find_data_source=False, **kwargs):
            if run_number==-1: run_number = self.run_number
            if self.include_function_name: 
                parameters, other_parameters = self.extract_parameters (function=function, **kwargs)
            else:
                parameters, other_parameters = self.extract_parameters (**kwargs)
            parameters.update (self.init_parameters)
            other_parameters.update (self.init_other_parameters)
            other_parameters.update (_X_=_X_)
            other_parameters.update (find_data_source=find_data_source)
            if function != 'apply':  other_parameters.update (y=y)
            result, dict_results = self.create_experiment_and_run (parameters=parameters, 
                                                                   other_parameters=other_parameters, 
                                                                   run_number=run_number)
            return dict_results
        
        def apply (self, *X, **kwargs):
            return self.run (function='apply', _X_=X, **kwargs)
            
        def fit (self, *X, y=None, **kwargs):
            self.run (function='fit', _X_=X, y=y, **kwargs)
        
        def fit_apply (self, *X, y=None, **kwargs):
            return self.run (function='fit_apply', _X_=X, y=y, **kwargs)
    
    return DSExperimentManager (*args_function, **kwargs_function)
