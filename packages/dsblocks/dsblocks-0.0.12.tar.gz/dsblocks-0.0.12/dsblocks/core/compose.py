# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/core/compose.ipynb.

# %% auto 0
__all__ = ['Sequential', 'Pass', 'MultiComponent', 'Pipeline', 'make_pipeline', 'pipeline_factory', 'PandasPipeline',
           'TrackingComponent', 'SequentialWithTracking', 'ParallelCallback', 'Parallel', 'ParallelChannelsCallback',
           'ParallelChannels', 'ColumnSelector', 'Concat', 'ColumnTransformer', 'Identity',
           'make_column_transformer_pipelines', 'make_column_transformer', 'MultiSplitComponent', 'MultiSplitDict',
           'MultiSplitDFColumn', 'ParallelInstancesCallback', 'ParallelInstances', 'CrossValidatorCallback',
           'CrossValidator', 'finalize_result_parallel_models', 'ParallelModelInstancesCallback',
           'ParallelModelInstances', 'WeightedClassifier', 'make_parallel_models', 'make_ensembler']

# %% ../../nbs/core/compose.ipynb 3
__author__ = "Jaume Amores"
__copyright__ = "Copyright 2021, Johnson Controls"
__license__ = "MIT"

# %% ../../nbs/core/compose.ipynb 4
import abc # comment # comment
import warnings 
import sys
from pathlib import Path
import shutil
import copy
import warnings
try:
    import sh
    sh_imported = True
except ImportError:
    sh_imported = False
from functools import partial
import re
import inspect
from typing import Optional, Union, Callable, List

import joblib
import json
from sklearn.utils import Bunch
import pandas as pd
import numpy as np

try:
    from graphviz import *
    imported_graphviz = True
except:
    imported_graphviz = False
    
from fastcore.meta import delegates

from .components import (Component,
                                          PandasComponent,
                                          SamplingComponent,
                                          NoSaverComponent)
from .data_conversion import PandasConverter
from .utils import PandasIO
import dsblocks.config.bt_defaults as dflt
from ..blocks.blocks import SkSplitGenerator
from ..utils.utils import (get_logging_level, set_empty_logger,
                                  get_calling_modules, add_commit_files, 
                                  get_existing_kwargs, json_load, json_dump)
from .callback import BaseCallback

# %% ../../nbs/core/compose.ipynb 8
#@delegates ()
class MultiComponent (SamplingComponent):
    """
    Component containing a list of components inside.
    
    Parameters
    ----------
    *components: list of components, optional.
        List of components that compose the MultiComponent object.
    name: string or None, optional.
        Name of MultiComponent object. If not provided, it is 
        inferred from one of the following: `class_name` converted to 
        camel case, name of the subclass inheriting from MultiComponent, 
        converted to camel case.
    class_name: string, optional.
        Class name given to MultiComponent object. If not provided, it is 
        inferred from one of the following: `name` converted to snake case, 
        or name of the subclass inheriting from MultiComponent.
    group: string, optional.
        Name of the group to which this component belongs. Can be used for 
        assigning the same parameters to all the components of a given 
        group. If we pass group='MyGroup', and then pass a parameter
        named MyGroup which is a dictionary, any parameters in this dictionary
        will overwrite those in the general list of parameters for those 
        components that have group='MyGroup'. For instance, if we pass
        (parameter1='value1A', parameter2='value2A', group='MyGroup', 
        MyGroup=dict(parameter1='value1B', parameter2='value2B'))
        then parameter1 will have 'value1B' and parameter2 will have 'value2B'. 
        This is useful for parameters that are propagated down the tree through 
        kwargs.
    tag: string, optional.
        Name of the tag assigned to this component. Similar to 'group',it
        can be used for  assigning the same parameters to all the components 
        of a given tag. If we pass 
        (parameter1='value1A', parameter2='value2A', tag='MyTag', 
        parameter1_MyTag='value2', parameter2_MyTag='value2B'), then
        parameter1 will have 'value1B' and parameter2 will have 'value2B'. 
        This is useful for parameters that are propagated down the tree through 
        kwargs.
    separate_labels: bool, optional.
        Whether or not the fit method receives the labels in a separate `y` 
        variable or in the same input `X` variable where the  
        data is contained.
    propagate: bool, optional.
        If True, properties such as path_results and path_models are propagated 
        down the tree of components whose root is the current MultiComponent.
    path_results : string or Path or None, optional
        Path to folder where results are stored.
    path_models: string, optional.
        Path to folder where trained models are stored.
    memory: bool, optional.
        Whether or not to store the results and models in memory, in addition to
        storing them in disk.
    root: Component, bool, or None, optional.
        If True, the current MultiComponent is signaled as the root of the 
        whole pipeline. If a component is object is passed instead of a 
        boolean, it then indicates that the object being passed is the 
        root of the pipeline. Used for having all the components in the 
        pipeline store a reference to the root. Useful for navigating across other
        components of the pipeline when debugging, jumping to other branches 
        of the pipeline or sibling components.
    automatic_root: bool, optional.
        If True, we make every the MultiComponent make itself a root of the
        subtree below, which makes the pipeline automatically discover the 
        actual root in highest position. This is computationally costly
        and is not recommended to set it to True.
    track: bool, optional.
        If True, the MultiComponent keeps track of all its results.
    track_results: bool, optional.
        If True, and the current MultiComponent is root, the MultiComponent keeps track 
        of the results of the whole subtree underneath.
    unique_names: bool, optional.
        If True, the component names are forced to be unique in the tree under this 
        MultiComponent. Names that were originally the same have a numeric suffix appended 
        to them.
    rebuild: bool, optional.
        If True, the whole tree is rebuilt, with all objects re-constructed, using the 
        kwargs passed to this MultiComponent, except for those arguments in the 
        `not_propagate` list (see below)
    not_propagate: bool, optional.
        When rebuilding the tree of components under the current MultiComponent (see argument
        above), arguments in kwargs are used except for those in `not_propagate`
    compare_kwargs: bool, optional.
        If True, the arguments passed in kwargs are matched against those actually used by 
        the tree of components under this MultiComponent. If there is any argument that is not
        used, either a warning or an exception is raised (see argument below).
    error_if_kwargs_mismatch: bool, optional.
        If True, an exception is raised if any of the kwargs is not used (see
        previous argument for further explanation)
    warning_if_nick_name_exists: bool, optional.
        If True, a warning is issued if two components share the same nick_name. A nick_name is 
        a second name assigned to a component, and it is based on the name given to the attribute
        used for storing this component in a parent component. The nick_name is normally not used,
        except for debugging purposes.
    root_checks_kwargs: bool, optional.
        If True, kwargs is analyzed and compared against arguments used across whole pipeline.
    """
    def __init__ (self,
                  *components,
                  name: Optional[str]=None,
                  class_name: Optional[str]=None,
                  group: Optional[str]=None,
                  tag: Optional[str]=None,
                  separate_labels: bool=dflt.separate_labels,
                  propagate: bool=dflt.propagate,
                  path_results: str=dflt.path_results,
                  path_models: str=dflt.path_models,
                  memory: bool=dflt.memory,
                  root: Union[Component, bool, None]=None,
                  automatic_root: bool=False,
                  track: bool=dflt.track,
                  track_results: bool=dflt.track,
                  unique_names: bool=True,
                  rebuild: bool=False,
                  not_propagate: Optional[bool]=None,
                  compare_kwargs: bool=dflt.compare_kwargs,
                  error_if_kwargs_mismatch: bool=dflt.error_if_kwargs_mismatch,
                  warning_if_nick_name_exists: bool=dflt.warning_if_nick_name_exists,
                  root_checks_kwargs: bool=False,
                  **kwargs):
        
        self.warning_if_nick_name_exists = warning_if_nick_name_exists

        if len(components) > 0:
            self.set_components (*components, path_results=path_results, path_models=path_models, 
                                 **kwargs)
        elif not hasattr (self, 'components'):
            self.components = []
        if not hasattr (self, 'finalized_component_list'):
            self.finalized_component_list = False

        # we need to call super().__init__() *after* having creating the `components` field,
        # since the constructor of Component calls a method that is overriden in Pipeline,
        # and this method makes use of the mentioned `components` field
        if root == True: root = self
        elif automatic_root: root = self if root is None else root
        if root and track: track_results=True
        super().__init__ (name=name, class_name=class_name, group=group, tag=tag, 
                          separate_labels=separate_labels, path_results=path_results, 
                          path_models=path_models, memory=memory,
                          root=root, track_results=track_results, **kwargs)
        if 'estimator' in kwargs:
            self.logger.warning ('estimator passed as key-word argument in MultiComponent')
        if self.root == True: root = self

        self.set_split ('whole')

        self.chain_folders (self.data_io.folder)
        if self.propagate:
            self.set_path_results (self.path_results)
            self.set_path_models (self.path_models)

        self.start_idx = dict (apply = dict (training=0, validation=0, test=0, whole=0),
                               fit = dict (training=0, validation=0, test=0, whole=0))
        self.is_data_source = dict (apply = dict (training=False, validation=False, test=False, whole=False),
                               fit = dict (training=False, validation=False, test=False, whole=False))
        self.all_components_fitted = False
        self.load_all_estimators = False

        self.set_root_info (root, unique_names=unique_names, root_checks_kwargs=root_checks_kwargs, **kwargs)
        if (compare_kwargs or error_if_kwargs_mismatch) and (root is not self):
            self.compare_kwargs_against_actual_args (error_if_kwargs_mismatch=error_if_kwargs_mismatch, **kwargs)
        if rebuild: self.rebuild_with_args (kwargs, not_propagate)

    def set_root_info (self, root, unique_names=True, root_checks_kwargs=False, **kwargs):
        if root is not None: self.set_root (root)
        if root is self:
            self.num_names = {}
            self.names = {}
            if unique_names: self.set_unique_names ()
            #self.propagate_attribute ('logger', self.logger)
            self.gather_and_save_info (compare_kwargs=root_checks_kwargs, **kwargs)

    def __repr__ (self):
        return self._repr ('MultiComponent')

    def gather_and_save_info (self, path_results=None, path_session=None, split=None,
                              remove_non_pickable=False, add_metadata=True, 
                              compare_kwargs=False, **kwargs):
        self.gather_descendants ()
        self.find_last_result (split=split)
        self.find_last_fitted_model (split=split)
        self.save_object (path_results=path_results, path_session=path_session,
                          remove_non_pickable=remove_non_pickable)
        if compare_kwargs:
            self.compare_kwargs_against_actual_args (**kwargs)
        if self.path_results is not None:
            stack_paths = self.save_call_stack (**kwargs)
            if add_metadata: self.add_metadata_to_repo (stack_paths=stack_paths, **kwargs)

    def compare_kwargs_against_actual_args (self, error_if_kwargs_mismatch=dflt.error_if_kwargs_mismatch, 
                                            used_kwargs=None, **kwargs):
        if used_kwargs is None: used_kwargs = set ()
        if not hasattr (self, 'functions_with_kwargs'): self.functions_with_kwargs = []
        self.functions_with_kwargs += [self.compare_kwargs_against_actual_args, 
                                      self.gather_and_save_info, self.save_call_stack, 
                                      self.add_metadata_to_repo, get_calling_modules]
        # inspect kwargs
        for func in self.functions_with_kwargs:
            used_kwargs |= set(inspect.signature (func).parameters.keys())
        super().compare_kwargs_against_actual_args (error_if_kwargs_mismatch=error_if_kwargs_mismatch,
                                                    used_kwargs=used_kwargs, **kwargs)
        
    def get_kwargs (self):
        used_kwargs = super().get_kwargs ()
        for component in self.components:
            used_kwargs |= component.get_kwargs ()
        return used_kwargs
            
    def save_call_stack (self, folder='all', **kwargs):
        call_stack = get_calling_modules (folder=folder, **kwargs)
        call_stack = [dict (zip (('path', 'lineno', 'function', 'context', 'flag'), x[1:])) for x in call_stack]
        config_results = self.path_results / 'config'
        config_results.mkdir (parents=True, exist_ok=True)
        with open (config_results / 'call_stack.json', 'wt') as f: json.dump (call_stack, f, indent=4)
        stack_paths = [x['path'] for x in call_stack]
        return stack_paths

    def add_metadata_to_repo (self, file_paths=None, stack_paths=None, additional_file_paths=None,
                              add_pipeline=False, track_files=False, copy_no_repo_files=False,
                              recursive=True, commit_message=None, out_of_repo_path=None,
                              **kwargs):
        use_git = sh_imported and Path ('.git').exists()
        if not Path ('.git').exists():
            self.logger.debug ('.git folder not found')
        if self.path_results is None:
            raise ValueError ('self.path_results must be set before calling add_metadata_to_repo')
        if use_git:
            git = sh.git.bake ()
            current_hash = str(git ('rev-parse', 'HEAD'))
        else:
            current_hash = None
        config_results = self.path_results / 'config'
        config_results.mkdir (parents=True, exist_ok=True)
        metadata = dict (root_path=str(self.path_results), current_hash=current_hash, new_hash=None)
        metadata_file = config_results / 'root_metadata.json'
        if metadata_file.exists ():
            prev_metadata = json_load (metadata_file)
            if type(prev_metadata) is not list: prev_metadata = [prev_metadata] 
            if (prev_metadata[-1]['root_path'] != metadata['root_path']
                or prev_metadata[-1]['current_hash'] != metadata['current_hash']):
                    metadata = prev_metadata + [metadata]
                
        json_dump (metadata, metadata_file, indent=4)
        if file_paths is None:
            if stack_paths is None: stack_paths = []
            if additional_file_paths is None: additional_file_paths = []
            metadata_paths = [config_results / 'call_stack.json', config_results / 'root_metadata.json']
            file_paths = (stack_paths + additional_file_paths + metadata_paths + 
                          [self.path_results / f'{dflt.name_logger}.log'])
            if add_pipeline:
                file_paths.append (self.path_results / 'pipeline.pk')
        if recursive:
            path_results_set = {str(self.path_results)}
            other_file_paths = self.write_git_hash_and_root_path (path_results_set, self.path_results,
                    current_hash)
        else:
            other_file_paths = []
        if track_files and use_git:
            if commit_message is None: commit_message = f'new metadata files in {self.path_results.name}'
            add_commit_files (file_paths+other_file_paths, message=commit_message, 
                              copy_no_repo_files=copy_no_repo_files, 
                              out_of_repo_path=out_of_repo_path)
            current_hash = str(git ('rev-parse', 'HEAD'))
            if type(metadata) is list: metadata[-1]['current_hash'] = current_hash
            else: metadata['current_hash'] = current_hash
            with open (config_results / 'next_hash.json', 'wt') as f: json.dump (metadata, f, indent=4)

    def write_git_hash_and_root_path (self, path_results_set, root_path, current_hash):
        file_paths = self.data_io.write_git_hash_and_root_path (path_results_set, root_path, current_hash)
        for component in self.components:
            if isinstance (component, MultiComponent):
                file_paths.extend (component.write_git_hash_and_root_path (path_results_set, root_path,
                    current_hash))
            else:
                file_paths.extend (component.data_io.write_git_hash_and_root_path (path_results_set, root_path,
                    current_hash))
        return file_paths

    def list (self, hierarchy_level=np.inf, horizontal=False):
        comps = [(k, self.cls[k]) if not isinstance(self.cls[k], list) else (k, self.cls[k][0]) for k in self.cls]
        comps = [x[0] for x in comps if x[1].hierarchy_level < hierarchy_level]
        comps = sorted(comps)
        if horizontal:
            print (comps)
        else:
            for k in comps:
                print (k)

    def number_levels (self):
        max_levels = 0
        for i, component in enumerate(self.components):
            component_levels = (component.number_levels () if isinstance (component, MultiComponent)
                                else 1)
            max_levels = max (max_levels, component_levels)
        return max_levels+1
                
    def show (self, max_level=15, vline=True, find_levels=True, same_horizontal_line=True, 
              color_lines=True, max_width=50):
        from rich import print
        char = ['*','=','+', '-', '.']*100 if not same_horizontal_line else ['_']*100
        if color_lines: 
            colors = ['white', 'bright_yellow', 'bright_white', 'bright_cyan']*100
        def print_color (text, level):
            if not color_lines or (level < 0): 
                print (text)
            else:
                n = int(len(re.findall('\|',text))/2)
                numbers = list(range(n))
                numbers = numbers + numbers[-1::-1]
                color=[f'[{colors[int(i)]}]|[/{colors[int(i)]}]' for i in numbers]
                split = text.split('|')
                text = split+color
                text[0::2]=split
                text[1::2]=color
                text = ''.join(text)
                
                m=re.search ('(___+)',text)
                if m is not None:
                    text = text.replace (m.group(), f'[{colors[level]}]{m.group()}[/{colors[level]}]')
                
                print (text)
                
        def hierarchy (self, level=0, before='', i=0, offset=0, space_before=''):
            if vline:
                line = (char[level-1+offset]*max_width if ((level+offset)<=len(char) and level>0) 
                        else '')
                #line_symbol = line[0] if line[0] not in {'.', '-','='} else '|'
                line_symbol = '|'
                print_line = line != ''
                line = list((space_before + line)[:max_width])
                line[-len(space_before):] = space_before[-1::-1]
                line = ''.join(line)
                idx_level = level-1
                if print_line and i==0: print_color (line, idx_level)
                space_before = (space_before + line_symbol + ' ') if print_line else (space_before + ' ')
            else:
                idx_level = -1
            empty_before = (before=='') or before.isspace()
            suffix = space_before+before if empty_before else f'{space_before}{before}: '
            line_class_name = f'{suffix}{self.class_name}{" "*max_width}'
            line_class_name = list(line_class_name[:max_width])
            line_class_name[-len(space_before):] = space_before[-1::-1]
            line_class_name = ''.join(line_class_name)
            print_color (line_class_name, idx_level)
            if (level < max_level) and isinstance (self, MultiComponent):
                space = space_before + ' '
                for i, c in enumerate(self.components):
                    new_before = f'{before}.{i+1}' if not empty_before else f'{i+1}'
                    hierarchy (c, level=level+1, before=new_before, i=i, offset=offset, space_before=space)
            if vline and print_line: print_color (line, idx_level)
        offset = max(len(char)+2-self.number_levels (),0) if find_levels else 0
        hierarchy (self, offset=offset)

    def register_components (self, *components):
        """
        Registering component in `self.components` list.

        Every time that a new component is set as an attribute of the pipeline,
        this component is added to the list `self.components`. Same
        mechanism as the one used by pytorch's `nn.Module`
        """
        if not hasattr(self, 'components'):
            self.components = []
            self.finalized_component_list = False
        if not self.finalized_component_list:
            self.components += components

    def _add_named_attribute (self, component, nick_name):
        if not hasattr(self, 'finalized_component_list'):
            self.finalized_component_list = False
        if not self.finalized_component_list:
            if not hasattr(self, component.name):
                super().__setattr__(component.name, component)
            if hasattr(component, 'nick_name') and self.warning_if_nick_name_exists:
                self.logger.warning (f'{component} already has a nick_name: {component.nick_name}')
                warnings.warn (f'{component} already has a nick_name: {component.nick_name}')
            component.nick_name = nick_name

    def __setattr__(self, k, v):
        """
        See register_components
        """
        super().__setattr__(k, v)

        if isinstance(v, Component):
            self.register_components(v)
            self._add_named_attribute (v, k)

    def set_but_not_add_component_attr (self, k, v):
        super().__setattr__(k, v)

    def add_component (self, component):
        if not hasattr(self, 'finalized_component_list'):
            self.finalized_component_list = False
        finalized_component_list = self.finalized_component_list
        self.finalized_component_list = False
        self.register_components(component)
        self._add_named_attribute (component, component.name)
        self.finalized_component_list = finalized_component_list

    def set_components (self, *components, **kwargs):
        self.components = components
        for i, component in enumerate(components):
            component, changed = self.obtain_component (component, **kwargs)
            self._add_named_attribute (component, component.name)
            if changed:
                self.components = list(self.components)
                self.components[i] = component
        self.finalized_component_list = True

    def obtain_component (self, component, use_whole_kwargs_if_function=False, **kwargs):
        if isinstance (component, Component):
            return component, False
        elif component.__class__.__name__ == 'function':
            function = component
            parameters = get_existing_kwargs (function, kwargs, 
                                              return_whole_kwargs=use_whole_kwargs_if_function)
            if parameters is not None: function = partial (function, **parameters)
            return Component (apply=function, **kwargs), True
        elif isinstance(component, tuple):
            assert len(component)>=2
            if (len(component)==2 and isinstance(component[1], str)
                    and component[0].__class__.__name__=='function'):
                function = component[0]
                parameters = get_existing_kwargs (function, kwargs, 
                                                  return_whole_kwargs=use_whole_kwargs_if_function)
                if parameters is not None: function = partial (function, **parameters)
                return Component (apply=function, class_name=component[1], **kwargs), True
            else:
                CompCls = component[0]
                assert type(CompCls) is type
                parameters = get_existing_kwargs (CompCls.__init__, kwargs)
                if type(component[-1]) is str:
                    return CompCls (*component[1:-1], class_name=component[-1], **parameters), True
                else:
                    return CompCls (*component[1:], **parameters), True
        elif type(component) is type:
            try:
                component = component (**kwargs)
            except TypeError:
                parameters = get_existing_kwargs (component.__init__, kwargs)
                component = component (**parameters)
            if not isinstance (component, Component): component = Component (estimator=component, **kwargs)
            return component, True
        else:
            return Component (estimator=component, **kwargs), True

    def clear_descendants (self):
        self.cls = Bunch ()
        self.obj = Bunch ()
        self.full_obj = Bunch ()
        self.full_cls = Bunch ()
        for component in self.components:
            if isinstance(component, MultiComponent):
                component.clear_descendants ()

    def gather_descendants (self, root='', nick_name=True):
        self.cls = Bunch ()
        self.obj = Bunch ()
        self.full_obj = Bunch ()
        self.full_cls = Bunch ()

        if hasattr(self, 'nick_name'):
            name = self.nick_name if nick_name else self.name
        else:
            name = self.name
        self.hierarchy_path = f'{root}{name}'
        for component in self.components:
            self._insert_descendant (self.cls, component, component.class_name)
            self._insert_descendant (self.obj, component, component.name)

            name = component.nick_name if nick_name else component.name
            component_hierarchy_path = f'{self.hierarchy_path}.{name}'
            self._insert_descendant (self.full_cls, component_hierarchy_path, component.class_name)
            self._insert_descendant (self.full_obj, component_hierarchy_path, component.name)
            if isinstance(component, MultiComponent):
                component.gather_descendants (root=f'{self.hierarchy_path}.',
                                              nick_name=nick_name)
                for name in component.cls:
                    self._insert_descendant (self.cls, component.cls[name], name)
                    self._insert_descendant (self.full_cls, component.full_cls[name], name)
                for name in component.obj:
                    self._insert_descendant (self.obj, component.obj[name], name)
                    self._insert_descendant (self.full_obj, component.full_obj[name], name)

    def _insert_descendant (self, cmp_dict, component, name):
        if name in cmp_dict:
            if not isinstance(cmp_dict[name], list):
                cmp_dict[name] = [cmp_dict[name]]
            if isinstance(component, list):
                cmp_dict[name].extend(component)
            else:
                cmp_dict[name].append(component)
        else:
            if isinstance(component, list):
                cmp_dict[name] = component.copy()
            else:
                cmp_dict[name] = component

    def gather_times (self, root=True):
        times = self.profiler.retrieve_times ()
        multi_comp_ovh = times.avg.drop(columns='leaf')
        index = multi_comp_ovh.index
        multi_comp_ovh = multi_comp_ovh.reset_index(drop=True)
        multi_comp_ovh_aggregated = multi_comp_ovh.sum(axis=1)
        times['multi_comp_ovh'] = multi_comp_ovh
        times['multi_comp_ovh_aggregated'] = multi_comp_ovh_aggregated
        dfs = [times]
        for component in self.components:
            c_times = component.profiler.retrieve_times ()
            c_multi_comp_ovh = c_times.avg.drop(columns='leaf').reset_index(drop=True)
            c_multi_comp_ovh_aggregated = c_multi_comp_ovh.sum(axis=1)
            times['multi_comp_ovh'] -= c_multi_comp_ovh
            times['multi_comp_ovh_aggregated'] -= c_multi_comp_ovh_aggregated
            if isinstance(component, MultiComponent):
                dfs.extend(component.gather_times (root=False))
            else:
                c_times = component.profiler.retrieve_times (is_leaf=True)
                c_multi_comp_ovh = c_times.avg.drop(columns='leaf')
                c_multi_comp_ovh.loc[:] = None
                c_multi_comp_ovh_aggregated = c_multi_comp_ovh.sum(axis=1)
                c_multi_comp_ovh_aggregated.loc[:] = None
                c_times['multi_comp_ovh'] = c_multi_comp_ovh
                c_times['multi_comp_ovh_aggregated'] = c_multi_comp_ovh_aggregated
                dfs.append(c_times)
        multi_comp_ovh.index = index
        multi_comp_ovh_aggregated.index = index
        if root:
            dfs = self.profiler.combine_times (dfs)
            dfs = self.profiler.analyze_overhead (dfs)
        return dfs

    def construct_diagram (self, split=None, include_url=False, port=4000, project='dsblocks'):
        """
        Construct diagram of the pipeline components, data flow and dimensionality.

        By default, we use test data to show the number of observations
        in the output of each component. This can be changed passing
        `split='train'`
        """
        split = self.get_split (split)

        if include_url:
            base_url = f'http://localhost:{port}/{project}'
        else:
            URL = ''

        node_name = 'data'
        output = 'train / test'

        f = Digraph('G', filename='fsm2.svg')
        f.attr('node', shape='circle')

        f.node(node_name)

        f.attr('node', shape='box')
        for component in self.components:
            last_node_name = node_name
            last_output = output
            node_name = component.model_plotter.get_node_name()
            if include_url:
                URL = f'{base_url}/{component.model_plotter.get_module_path()}.html#{node_name}'
            f.node(node_name, URL=URL)
            f.edge(last_node_name, node_name, label=last_output)
            output = component.model_plotter.get_edge_name(split=split)

        last_node_name = node_name
        node_name = 'output'
        f.attr('node', shape='circle')
        f.edge(last_node_name, node_name, label=output)

        return f

    def show_result_statistics (self, split=None):
        """
        Show statistics about results obtained by each component.

        By default, this is shown on test data, although this can change setting
        `split='train'`
        """
        split = self.get_split (split)

        for component in self.components:
            component.show_result_statistics(split=split)

    def show_summary (self, split=None, file=sys.stdout):
        """
        Show list of pipeline components, data flow and dimensionality.

        By default, we use test data to show the number of observations
        in the output of each component. This can be changed passing
        `split='train'`
        """
        split = self.get_split (split)

        node_name = 'data'
        output = 'train / test'
        if isinstance (file, str) or isinstance (file, Path): file = open (file, 'wt')

        for i, component in enumerate(self.components):
            node_name = component.model_plotter.get_node_name()
            output = component.model_plotter.get_edge_name(split=split)
            print (f'{"-"*100}', file=file)
            print (f'{i}: {node_name} => {output}', file=file)


    def get_split (self, split=None):
        if split is None:
            if self.data_io.split is not None:
                split = self.data_io.split
            else:
                split = 'whole'

        return split

    def assert_all_equal (self, path_reference_results, raise_error=False, recursive=True,
                          max_recursion=None, current_recursion=0, verbose=None, **kwargs):
        """Compare results stored in current run against reference results stored in given path."""
        if verbose is not None:
            self.logger.setLevel(get_logging_level (verbose))
        is_equal = True
        non_equal_components = []
        end_recursion = max_recursion is not None and current_recursion >= max_recursion
        components = self.components if not end_recursion else [self]
        for component in components:
            if isinstance(component, MultiComponent) and recursive and not end_recursion:
                this_equal = component.assert_all_equal (path_reference_results,
                                                         raise_error=raise_error,
                                                         recursive=recursive,
                                                         max_recursion=max_recursion,
                                                         current_recursion=current_recursion+1,
                                                         verbose=verbose,
                                                         **kwargs)
            else:
                this_equal = component.assert_equal (path_reference_results,
                                                     raise_error=raise_error,
                                                     verbose=verbose,
                                                     **kwargs)
            if not this_equal:
                non_equal_components.append(component.name)
            is_equal = this_equal and is_equal

        if not is_equal:
            self.logger.warning (f'Results are different in components {non_equal_components}')
        else:
            self.logger.info ('both pipelines give the same results')

        self.logger.setLevel(get_logging_level (self.verbose))

        return is_equal

    def load_estimator (self, skip_from=None):
        for component in self.components[:skip_from]:
            component.load_estimator ()

    def save_result (self, result, split=None, path_results=None, result_file_name=None):
        raise NotImplementedError ()
        self.data_io.save_result (result, split=split, path_results=path_results,
                                  result_file_name=result_file_name)
        for component in self.components:
            if isinstance (component, MultiComponent):
                component.save_result (result, split=split, path_results=path_results,
                                       result_file_name=result_file_name)
            else:
                component.data_io.save_result (result, split=split, path_results=path_results,
                                               result_file_name=result_file_name)

    def save_object (self, save_run=True, path_results=None, path_session=None,
                     remove_non_pickable=False):
        if remove_non_pickable:
            pipe = copy.deepcopy (self)
            pipe.remove_non_pickable_fields ()
        else:
            pipe = self
        path_results = pipe.data_io.path_results if path_results is None else Path(path_results)
        if path_results is not None:
            path_results.mkdir (parents=True, exist_ok=True)
            try:
                joblib.dump (pipe, path_results / 'pipeline.pk')
            except Exception as e:
                self.logger.warning (f'could not pickle object: {e}')
        if save_run:
            if path_session is None: path_session = dflt.path_session_folder
            path_named_session = Path (path_session) / f'last_run/{self.name}.pk'
            path_session = Path (path_session) / f'last_run/{dflt.session_filename}'
            path_session.parent.mkdir (parents=True, exist_ok=True)
            try:
                joblib.dump (pipe, path_session)
                joblib.dump (pipe, path_named_session)
            except Exception as e:
                self.logger.warning (f'could not pickle object: {e}')

    def save_logs (self, save_run=True, path_results=None, path_session=None):
        path_log_file = f'{dflt.path_logger_folder}/{dflt.logger_filename}'
        if Path(path_log_file).exists():
            if path_results is None:  path_results = self.data_io.path_results
            if path_results is not None:
                path_results.mkdir (parents=True, exist_ok=True)
                shutil.copy (path_log_file, path_results)
            if save_run:
                if path_session is None: path_session = dflt.path_session_folder
                path_session_log_file = Path (path_session) / f'last_run/{dflt.logger_filename}'
                path_session_log_file.parent.mkdir (parents=True, exist_ok=True)
                shutil.copy (path_log_file, path_session_log_file)


    def remove_non_pickable_fields (self):
        for component in self.components:
            component.remove_non_pickable_fields ()

    def find_last_result (self, split=None):
        return False

    def find_last_fitted_model (self, split=None):
        return False

    def find_method (self, method):
        if callable(getattr (self, method, None)):
            return getattr (self, method, None)
        elif (isinstance (self, Sequential) or isinstance (self, MultiSplitComponent) or
             isinstance (self, ParallelInstances)):
            if isinstance (self.components[-1], MultiComponent):
                return self.components[-1].find_method (method)
            elif callable(getattr (self.components[-1], method, None)):
                return getattr (self.components[-1], method, None)
        return None
    
    def rebuild_with_args (self, kwargs, not_propagate):
        if not_propagate is not None:
            parameters = {k:kwargs[k] for k in set(kwargs).difference(not_propagate)}
        else:
            parameters = kwargs
        self.components = self.recursive_rebuild_with_args (parameters, root=True)
    
    def recursive_rebuild_with_args (self, kwargs, root=False):
        def create_component (component, args, kwargs):
            parameters = component.__stored_args__.copy()
            parameters.update (kwargs)
            return component.__class__ (*args, **parameters)
        new_components = list(self.components)
        for i, component in enumerate(self.components):
            if isinstance (component, MultiComponent):
                new_components[i] = component.recursive_rebuild_with_args (kwargs)
            else:
                new_components[i] = create_component (component, (), kwargs)
        return (create_component (self, new_components, kwargs) if not root
                else tuple(new_components))
    
    # *************************
    # setters
    # *************************
    def set_split (self, split):
        super().set_split (split)
        for component in self.components:
            component.set_split (split)

    def set_save_splits (self, save_splits):
        super().set_save_splits (save_splits)
        for component in self.components:
            component.set_save_splits (save_splits)

    def set_load_model (self, load_model):
        super().set_load_model (load_model)
        for component in self.components:
            component.set_load_model (load_model)

    def set_save_model (self, save_model):
        super().set_save_model (save_model)
        for component in self.components:
            component.set_save_model (save_model)

    def set_save_result (self, save_result):
        super().set_save_result (save_result)
        for component in self.components:
            component.set_save_result (save_result)

    def set_load_result (self, load_result):
        super().set_load_result (load_result)
        for component in self.components:
            component.set_load_result (load_result)

    def set_path_results (self, path_results):
        self.data_io.set_path_results (path_results)
        for component in self.components:
            if not component.data_io.stop_propagation:
                if isinstance (component, MultiComponent):
                    component.set_path_results (path_results)
                else:
                    component.data_io.set_path_results (path_results)
    def set_path_models (self, path_models):
        self.data_io.set_path_models (path_models)
        for component in self.components:
            if isinstance (component, MultiComponent):
                component.set_path_models (path_models)
            else:
                component.data_io.set_path_models (path_models)
                
    def set_separate_data (self, path_data, use_memory=True, only_root_has_memory=True):
        self.data_io.stop_propagation = True
        self.set_path_results (path_data)
        memory_io = self.data_io.memory_io
        if only_root_has_memory:
            self.propagate_sub_component_attribute ('data_io', 'memory_io', None)
        if memory_io is not None:
            self.data_io.memory_io=memory_io
        elif use_memory:
            self.data_io.set_memory (True)
                
    def propagate_attribute (self, name, value):
        setattr (self, name, value)
        for component in self.components:
            if isinstance (component, MultiComponent):
                component.propagate_attribute (name, value)
            else:
                setattr (component, name, value)
                
    def propagate_sub_component_attribute (self, sub_component_name, name, value):
        setattr (getattr(self, sub_component_name), name, value)
        for component in self.components:
            if isinstance (component, MultiComponent):
                component.propagate_sub_component_attribute (sub_component_name, name, value)
            else:
                setattr (getattr(component, sub_component_name), name, value)

    def chain_folders (self, folder, root=True):
        if folder == '':
            return
        if root:
            self.data_io.chain_folders ('')
        else:
            self.data_io.chain_folders (folder)
        for component in self.components:
            if isinstance (component, MultiComponent):
                component.chain_folders (folder, root=False)
            else:
                component.data_io.chain_folders (folder)

    def set_root (self, root):
        super().__setattr__ ('root', root)
        for component in self.components:
            if isinstance (component, MultiComponent):
                component.set_root (root)
            else:
                component.root = root

    def register_global_name (self, component):
        if component.name in self.root.names:
            if component.name not in self.root.num_names:
                self.root.num_names[component.name] = 0
            self.root.num_names[component.name] += 1
            component.set_name (f'{component.name}_{self.root.num_names[component.name]}')
        self.root.names[component.name] = component

    def set_unique_names (self):
        self.register_global_name (self)
        for component in self.components:
            if isinstance (component, MultiComponent):
                component.set_unique_names ()
            else:
                self.register_global_name (component)

    def set_suffix (self, suffix):
        super().set_suffix (suffix)
        for component in self.components:
            component.set_suffix (suffix)
            
    def exist_all_estimators (self):
        exist_all = True
        for component in self.components:
            if isinstance (component, MultiComponent):
                exist_all = exist_all and component.exist_all_estimators ()
            elif component.is_model:
                exist_all = exist_all and component.data_io.exists_estimator (include_memory=True)
            if not exist_all:
                break
        return exist_all

# %% ../../nbs/core/compose.ipynb 79
#@delegates ()
class Pipeline (MultiComponent):
    """
    Sequential pipeline composed of a list of components that run sequentially.

    It can be used both for sequential processing of steps, for sequentially
    training estimators, and a mix of both.

    Parameters
    ----------
    components: list of components, optional.
        List of components to be run sequentially.
    some_components_return_nothing: bool or None, optional.
        If False, all the components are expected to return some value, and if one 
        component returns None, this value is passed on to the next component. If 
        True, returning None means that the component does not return anything, and 
        the next component in the pipeline gets no input. 
    """
    def __init__ (self, *components, some_components_return_nothing:Optional[bool]=None, **kwargs):
        if some_components_return_nothing is not None: 
            kwargs['component_returns_nothing'] = some_components_return_nothing
        super().__init__ (*components, **kwargs)

    def __repr__ (self):
        return self._repr ('Sequential')

    def _fit (self, *X, **kwargs):
        """
        Fit components of the pipeline, given data X and labels y.

        By default, y will be None, and the labels are part of `X`, as a variable.
        """
        assert len(self.components) > 1, 'Sequential class needs to have more than one component'
        X = self._fit_apply (*X, last=-1, **kwargs)
        self.components[-1].fit (X, **kwargs)

    def _fit_apply (self, *X, split=None, last=None, **kwargs):
        split = self.data_io.split if split is None else split
        first = self.start_idx['fit'][split]
        if first >= len(self.components):
            return X
        if first > 0:
            self.load_estimator (skip_from=first)

        if first < len(self.components):
            component = self.components[first]
            X = component.fit_apply (*X, sequential_fit_apply=True, split=split, **kwargs)
            first += 1
        last = len(self.components) if last is None else last
        for i, component in enumerate(self.components[first:last], start=first):
            X = component.fit_apply (X, sequential_fit_apply=i<(last-1), split=split, **kwargs)
        return X

    def _apply (self, *X, split=None, **kwargs):
        """Transform data with components of pipeline, and predict labels with last component.

        In the current implementation, we consider prediction a form of mapping,
        and therefore a special type of transformation."""
        split = self.data_io.split if split is None else split
        first = self.start_idx['apply'][split]

        if first < len(self.components):
            component = self.components[first]
            X = component.apply (*X, split=split, **kwargs)
            first += 1
        for component in self.components[first:]:
            X = component.apply (X, split=split, **kwargs)
        return X

    def find_last_result (self, split=None, func='apply', first=-1):

        idx = None
        for i, component in enumerate(self.components[first::-1]):
            if (component.data_io.can_load_result () and 
                component.data_io.exists_result (split=split,include_memory=True)):
                idx = i
                break
            elif isinstance (component, MultiComponent):
                starting_point = component.find_last_result (split=split)
                if starting_point:
                    idx = i
                    break
        split = self.data_io.split if split is None else split
        if idx is not None:
            first = (len(self.components) + first) if (first < 0) else first
            self.start_idx[func][split] = first - idx
            self.is_data_source[func][split] = True
        else:
            self.start_idx[func][split] = 0
            self.is_data_source[func][split] = False
        return self.is_data_source[func][split]

    def find_last_fitted_model (self, split=None):
        idx = len(self.components)-1
        all_components_fitted = True
        self.load_all_estimators = False
        for i, component in enumerate(self.components):
            if isinstance (component, MultiComponent):
                if not component.find_last_fitted_model (split=split):
                    idx = i-1
                    all_components_fitted = False
                    break
            elif (component.is_model and
                  not (component.data_io.can_load_model () and 
                       component.data_io.exists_estimator (include_memory=True))):
                    idx = i-1
                    all_components_fitted = False
                    break

        if idx >= 0:
            _ = self.find_last_result (split=split, func='fit', first=idx)
        if all_components_fitted and self.data_io.exists_result (split=split):
            self.data_io.load_estimator = self.data_io.load_estimators
            self.load_all_estimators = True
        self.all_components_fitted = all_components_fitted
        return all_components_fitted

# Sequential is an alias of Pipeline
Sequential = Pipeline

# %% ../../nbs/core/compose.ipynb 104
def make_pipeline(*components, cls=Pipeline, **kwargs):
    """Create `Pipeline` object of class `cls`, given `components` list."""
    pipeline = cls (**kwargs)
    pipeline.set_components(*components)
    return pipeline

# %% ../../nbs/core/compose.ipynb 108
def pipeline_factory (pipeline_class, **kwargs):
    """Creates a pipeline object given its class `pipeline_class`

    Parameters
    ----------
    pipeline_class : class or str
        Name of the pipeline class used for creating the object.
        This can be either of type string or class.
    """
    if type(pipeline_class) is str:
        Pipeline = eval(pipeline_class)
    elif type(pipeline_class) is type:
        Pipeline = pipeline_class
    else:
        raise ValueError (f'pipeline_class needs to be either string or class, we got {pipeline_class}')

    return Pipeline (**kwargs)

# %% ../../nbs/core/compose.ipynb 112
#@delegates ()
class PandasPipeline (Pipeline):
    """Pipeline that saves results in parquet format, and preserves DataFrame format."""
    def __init__ (self,
                  data_converter='PandasConverter',
                  data_io='PandasIO',
                  separate_labels=False,
                  **kwargs):
        super().__init__ (data_converter=data_converter,
                          data_io=data_io,
                          separate_labels=separate_labels,
                          **kwargs)

# %% ../../nbs/core/compose.ipynb 117
#@delegates ()
class TrackingComponent (Sequential):
    """Performs experiment tracking."""
    def __init__ (self, component, root=True, track=True, track_results=True, path_results='.dsblocks', 
                  name='root', save_result=dflt.save_result, save_model=dflt.save_model, **kwargs):
        super().__init__ (component, root=root, track=False, track_results=False, path_results=path_results, 
                          save_result=False, save_model=False, **kwargs)
        self.component = self.components[0]
        self.component.data_io.set_path_results (path_results)
        self.component.set_track_results (True)
        self.component.set_save_result (save_result)
        self.component.set_save_model (save_model)
        self.tracker = self.component.tracker
    def _apply (self, *X, **kwargs):
        return self.tracker.apply (*X, **kwargs)
    def _fit (self, *X, y=None, **kwargs):
        self.tracker.fit (*X, y=y, **kwargs)
    def _fit_apply (self, *X, y=None, **kwargs):
        return self.tracker.fit_apply (*X, y=y, **kwargs)
    def get_experiment_manager (self):
        return self.tracker

# %% ../../nbs/core/compose.ipynb 119
#@delegates ()
class SequentialWithTracking (TrackingComponent):
    """Performs experiment tracking."""
    def __init__ (self, *components, root=True, track=True, track_results=True, path_results='.dsblocks', 
                  name='root', main_name=None, save=dflt.save, **kwargs):
        if main_name is None: main_name = 'main'
        component = Sequential (*components, root=False, track=False, track_results=track, 
                                path_results=path_results, name=main_name, save=save, **kwargs)
        super().__init__ (component, root=root, track=False, track_results=False, path_results=path_results, 
                          name=name, save=False, **kwargs)
        self.component = component
        self.tracker = self.component.tracker

# %% ../../nbs/core/compose.ipynb 132
#@delegates ()
class ParallelCallback (BaseCallback):
    def __init__ (self, pcomponent, select_input_to_fit=None, select_input=None,
                  initialize_result=None, join_result=None, finalize_result=None, 
                  set_component_info=None, store_component_fit_info=None, 
                  store_component_apply_info=None, store_component_fit_apply_info=None, 
                  store_component_find_last_result_info=None, 
                  store_component_find_last_fitted_model_info=None, store_fit_info=None, 
                  store_fit_apply_info=None, store_apply_info=None, config=None,
                  **kwargs):
        super().__init__ (**kwargs)
        self.pcomponent = pcomponent

        select_input_to_fit = (self.select_input_to_fit if select_input_to_fit is None
                                      else partial(select_input_to_fit, self))
        self.select_input_to_fit = select_input_to_fit
        select_input = (self.select_input if select_input is None else partial(select_input, self))
        self.select_input = select_input
        initialize_result = (self.initialize_result if initialize_result is None
                                      else partial(initialize_result, self))
        self.initialize_result = initialize_result
        join_result = (self.join_result if join_result is None
                                      else partial(join_result, self))
        self.join_result = join_result
        finalize_result = (self.finalize_result if finalize_result is None
                                      else partial(finalize_result, self))
        self.finalize_result = finalize_result
        
        #**************************
        self.set_component_info = (self.set_component_info if set_component_info is None
                                   else partial(set_component_info, self))
        self.store_component_fit_info = (self.store_component_fit_info if store_component_fit_info is None
                                   else partial(store_component_fit_info, self))
        self.store_component_apply_info = (self.store_component_apply_info if store_component_apply_info is None
                                   else partial(store_component_apply_info, self))
        self.store_component_fit_apply_info = (self.store_component_fit_apply_info if store_component_fit_apply_info is None
                                   else partial(store_component_fit_apply_info, self))
        self.store_component_find_last_result_info = (self.store_component_find_last_result_info if store_component_find_last_result_info is None
                                   else partial(store_component_find_last_result_info, self))
        self.store_component_find_last_fitted_model_info = (self.store_component_find_last_fitted_model_info if store_component_find_last_fitted_model_info is None
                                   else partial(store_component_find_last_fitted_model_info, self))
        self.store_fit_info = (self.store_fit_info if store_fit_info is None
                                   else partial(store_fit_info, self))
        self.store_fit_apply_info = (self.store_fit_apply_info if store_fit_apply_info is None
                                   else partial(store_fit_apply_info, self))
        self.store_apply_info = (self.store_apply_info if store_apply_info is None
                                   else partial(store_apply_info, self))
        #**************************
        
        self.config = config

    def __getattr__ (self, k): 
        return getattr (self.pcomponent, k) if (k != 'pcomponent' and k!= 'parent') else None

    # *****************************************
    # hook functions for results handling
    # *****************************************
    def initialize_result (self):
        return []

    def select_input (self, components, i, *X):
        return X

    def select_input_to_fit (self, components, i, *X):
        return X

    def join_result (self, Xr, Xi_r, components, i):
        Xr.append (Xi_r)
        return Xr

    def finalize_result (self, Xr, components=None):
        if type(Xr) is list: Xr = tuple(Xr)
        return Xr

    # **********************************************************************************
    # hook functions setting and storing information at the beginning of each method,
    # at each iteration, and at the end of each method
    # **********************************************************************************
    def set_component_info (self, component, i):
        pass
    def store_component_fit_info (self, component, i):
        pass
    def store_component_apply_info (self, component, i):
        pass
    def store_component_fit_apply_info (self, component, i):
        pass
    def store_component_find_last_result_info (self, component, i):
        pass
    def store_component_find_last_fitted_model_info (self, component, i):
        pass
    def store_fit_info (self):
        pass
    def store_fit_apply_info (self):
        pass
    def store_apply_info (self):
        pass

# %% ../../nbs/core/compose.ipynb 134
#@delegates ()
class Parallel (MultiComponent):
    """
    Pipeline whose components don't have sequential dependencies.

    By default, all the components receive the same input data, and 
    the output of the Parallel pipeline is a tuple where the i-th
    element is the output of the i-th component, in the order given in t
    the components argument (see description below). This behavior, the 
    way the input is handled and the output is obtained, can be changed,
    along with other details, by providing a modified ParallelCallback
    object. See `ParallelCallback` class for more details, or see examples
    of how to obtain different Parallel classes in `ParallelChannels`, 
    `CrossValidator` or `ParallelInstances` classes.

    Parameters
    ----------
    components: list of components, optional.
        List of components.
    callback: ParallelCallback, optional.
        Callback called at different steps of each method in the
        Parallel component. Useful for modifying the behaviour of the 
        Parallel component, and converting it into different 
        subclasses, see for instance the `CrossValidator` class.
    """
    def __init__ (self, 
                  *components, 
                  callback: ParallelCallback=None, 
                  **kwargs):
        self.cb = ParallelCallback (self, **kwargs) if callback is None else callback
        self.force_end = False

        super().__init__ (*components, **kwargs)

    def __repr__ (self):
        return self._repr ('Parallel')

    def _fit (self, *X, **kwargs):
        """
        Fit components of the pipeline, given data X and labels y.

        By default, y will be None, and the labels are part of `X`, as a variable.
        """
        for i, component in enumerate(self.components):
            self.cb.set_component_info (component, i)
            Xi = self.cb.select_input_to_fit (self.components, i, *X)
            component.fit (*Xi, **kwargs)
            self.cb.store_component_fit_info (component, i)
            if self.force_end:
                self._finish_loop (i)
                break
        self.cb.store_fit_info ()

    def _finish_loop (self, i):
        message = f"finishing fit's loop at iteration {i}"
        #print (message)
        self.logger.info (message)
        self.force_end = False

    def _apply (self, *X, **kwargs):
        """Transform data with components of pipeline, and predict labels with last component.

        In the current implementation, we consider prediction a form of mapping,
        and therefore a special type of transformation."""
        Xr = self.cb.initialize_result ()
        for i, component in enumerate(self.components):
            self.cb.set_component_info (component, i)
            Xi = self.cb.select_input (self.components, i, *X)
            Xi_r = component.apply (*Xi, **kwargs)
            Xr = self.cb.join_result (Xr, Xi_r, self.components, i)
            self.cb.store_component_apply_info (component, i)

        Xr = self.cb.finalize_result (Xr)
        self.cb.store_apply_info ()

        return Xr

    def _fit_apply (self, *X, **kwargs):
        Xr = self.cb.initialize_result ()
        for i, component in enumerate(self.components):
            self.cb.set_component_info (component, i)
            Xi = self.cb.select_input_to_fit (self.components, i, *X)
            Xi_r = component.fit_apply (*Xi, **kwargs)
            Xr = self.cb.join_result (Xr, Xi_r, self.components, i)
            self.cb.store_component_fit_apply_info (component, i)
            if self.force_end:
                self._finish_loop (i)
                break

        Xr = self.cb.finalize_result (Xr)
        self.cb.store_fit_apply_info ()

        return Xr

    def find_last_result (self, split=None, func='apply'):
        is_data_source = True
        for i, component in enumerate(self.components):
            self.cb.set_component_info (component, i)
            if not (component.data_io.can_load_result () and 
                    component.data_io.exists_result (split=split, include_memory=True)):
                if isinstance (component, MultiComponent):
                    is_data_source = is_data_source and component.find_last_result (split=split)
                else:
                    is_data_source = False
            self.cb.store_component_find_last_result_info (component, i)
        self.is_data_source[func][split] = is_data_source
        return is_data_source

    def find_last_fitted_model (self, split=None):
        self.load_all_estimators = False
        all_components_fitted = True
        for i, component in enumerate(self.components):
            self.cb.set_component_info (component, i)
            if isinstance (component, MultiComponent):
                if not component.find_last_fitted_model (split=split):
                    all_components_fitted = False
            elif (component.is_model and
                  not (component.data_io.can_load_model () and 
                       component.data_io.exists_estimator (include_memory=True))):
                    all_components_fitted = False
            self.cb.store_component_find_last_fitted_model_info (component, i)
        if all_components_fitted and self.data_io.exists_result (split=split):
            self.data_io.load_estimator = self.data_io.load_estimators
            self.load_all_estimators = True
        self.all_components_fitted = all_components_fitted
        return all_components_fitted

# %% ../../nbs/core/compose.ipynb 149
#@delegates ()
class ParallelChannelsCallback (ParallelCallback):
    def __init__ (self, pcomponent, **kwargs):
        super ().__init__ (pcomponent, **kwargs)

    def select_input_to_fit (self, components, i, *X):
        X, y = self.data_converter.convert_varargs_to_x_y (X)
        if y is not None:
            return X[components[i].key], y
        else:
            return (X[components[i].key],)

    def initialize_result (self):
        return {component.key: None for component in self.components}

    def select_input (self, components, i, X):
        return X[components[i].key]

    def join_result (self, Xr, Xi_r, components, i):
        Xr[components[i].key] = Xi_r
        return Xr

# %% ../../nbs/core/compose.ipynb 151
#@delegates ()
class ParallelChannels (Parallel):
    """
    Processes multiple channels using Parallel data flow.

    Useful for cases when a basically the same pipeline is used to process different 
    channels of information. The input is a dictionary where each key holds a
    different channel, and the output is another dictionary where each key holds
    the result for the respective channel.
    
    Parameters
    ----------
    components: list of components, optional.
        List of components to be run sequentially.
    use_name: bool or None, optional.
        Whether or not the i-th channel has the name of the i-th component of 
        the pipeline. The name of the i-th channel is used as a key for 
        retrieving the data of that channel in the input dictionary, and for 
        storing the result of processing that channel in the output dictionary.
        If the name of the i-th component is not used as name of the i-th channel,
        then the name of the channel is taken from the folder associated with 
        the i-th component's data_io. This folder is where the results of processing 
        the i-th channel is stored. 
    component_class: class or None, optional.
        Class or function to be used for constructing the components of the pipeline.
        The i-th component is constructed by passing the configuration stored in the
        i-th element of the configs list (see below)
    configs: collection of configurations, one per component, or None, optional.
        List of dictionaries, where the i-th dictionary of the list contains the 
        configuration to be used for constructing the i-th component of the pipeline.
    callback: ParallelCallback or None, optional
        Callback called at different steps of each methods in the
        Parallel component. Useful for modifying the behaviour of the 
        Parallel component. If not provided, the `ParallelChannelsCallback` is used, 
        and this callback implements the behaviour described for this class.
    """
    def __init__ (self, 
                  *components, 
                  use_name: bool=False, 
                  component_class: Union[type, Callable, None]=None, 
                  configs=List[dict],
                  callback:Optional[ParallelCallback]=None, 
                  **kwargs):
        if callback is None: callback = ParallelChannelsCallback (self, **kwargs)
        if component_class is not None and configs is not None and isinstance(configs, dict):
            new_components = []
            for k in configs:
                new_components.append (component_class (**configs[k], folder=k, suffix=k))
            components = list(components) + new_components
        super().__init__ (*components, callback=callback, **kwargs)
        for component in components:
            component.key = component.name if use_name else component.data_io.folder

    def __repr__ (self):
        return self._repr ('ParallelChannels')

# %% ../../nbs/core/compose.ipynb 157
#@delegates ()
class ColumnSelector (NoSaverComponent):
    def __init__ (self,
                  columns=[],
                  remainder=False,
                  verbose=dflt.verbose,
                  force_verbose=False,
                  logger=None,
                  direct_apply=True,
                  **kwargs):
        verbose = 0 if not force_verbose else verbose
        if verbose==0:
            logger = set_empty_logger ()
        super().__init__ (verbose=verbose,
                          logger=logger,
                          direct_apply=direct_apply,
                          **kwargs)

    def _apply (self, df, **kwargs):
        if self.remainder:
            return df[[c for c in df.columns if c not in self.columns]]
        else:
            return df[self.columns]

# %% ../../nbs/core/compose.ipynb 161
#@delegates ()
class Concat (NoSaverComponent):
    def __init__ (self,
                  verbose=dflt.verbose,
                  force_verbose=False,
                  logger=None,
                  direct_apply=True,
                  **kwargs):
        verbose = 0 if not force_verbose else verbose
        if verbose==0:
            logger = set_empty_logger ()
        super().__init__ (verbose=verbose,
                          logger=logger,
                          direct_apply=direct_apply,
                          **kwargs)

    def _apply (self, *dfs, **kwargs):
        return pd.concat(list(dfs), axis=1)

# %% ../../nbs/core/compose.ipynb 166
#@delegates ()
class _BaseColumnTransformer (MultiComponent):
    def __init__ (self, name=None, class_name=None, direct_apply=True, direct_fit=True,**kwargs):
        super().__init__ (name=name, class_name=class_name, direct_apply=direct_apply,
                          direct_fit=direct_fit, **kwargs)
        self.concat = Concat (**kwargs)
        del self.concat.nick_name

    def set_components (self, *components):
        components = list(components)
        components.append (self.concat)
        super().set_components (*components)

    def _fit (self, df, y=None, **kwargs):
        assert len(self.components) > 0
        assert self.components[-1] is self.concat
        for component in self.components[:-1]:
            component.fit (df, **kwargs)
        return self

    def _apply (self, df, **kwargs):
        dfs = []
        assert len(self.components) > 0
        assert self.components[-1] is self.concat
        index = None
        for component in self.components[:-1]:
            df_component=component.transform (df, **kwargs)
            if df_component.shape[0]==df.shape[0]:
                if not (df_component.index==df.index).all():
                    message = f'index obtained by {component} is different than original index'
                    warnings.warn (message)
                    self.logger.warning (message)
                df_component.index=df.index
            elif index is not None:
                df_component.index=index
            else:
                index=df_component.index
            dfs.append (df_component)
        df_result = self.concat.transform (*dfs)
        return df_result

# %% ../../nbs/core/compose.ipynb 168
#@delegates ()
class ColumnTransformer (_BaseColumnTransformer):
    """
    Apply different transformers to different subsets of columns. 
    
    Parameters
    ----------
    transformers_with_name: list of tuples `(transformer_name, transformer, columns)`
        Applies `transformer` to `columns` of the input DataFrame. 
    remainder: string or estimator, optional. 
        Indicates what to do with the remaining columns,  i.e., those not included in 
        any of the given tuples.
        Valid values can be:
        - `passthrough`: remaining columns are included  without changes in the ouput.
        - `drop` (default): remaining columns are not included in the output.
        - estimator / Component: the given estimator or Component is applied to the
        remaining columns.
    """
    def __init__ (self, 
                  *transformers_with_name, 
                  remainder: Union[str, 'Estimator', Component] = 'drop', 
                  **kwargs):
        super().__init__ (**kwargs)
        pipelines = make_column_transformer_pipelines (*transformers_with_name, remainder=remainder, 
                                                        **kwargs)
        super().set_components(*pipelines)

# %% ../../nbs/core/compose.ipynb 170
#@delegates ()
class Identity (NoSaverComponent):
    def __init__ (self,
                  verbose=dflt.verbose,
                  force_verbose=False,
                  logger=None,
                  direct_apply=True,
                  **kwargs):
        verbose = 0 if not force_verbose else verbose
        if verbose==0:
            logger = set_empty_logger ()
        super().__init__ (verbose=verbose,
                          logger=logger,
                          direct_apply=direct_apply,
                          **kwargs)

    def _apply (self, X, **kwargs):
        return X
    
Pass = Identity

# %% ../../nbs/core/compose.ipynb 174
def _append_pipeline (pipelines, name, transformer, columns, remainder= False, **kwargs):
    drop = False
    if isinstance(transformer, str):
        if transformer == 'passthrough':
            transformer = Identity (**kwargs)
        elif transformer == 'drop':
            drop = True
        else:
            raise ValueError (f'name {transformer} not recognized')

    if not drop:
        config=kwargs.copy()
        config.update({name:dict(data_io='NoSaverIO')})
        config.update (direct_apply=True)
        pipeline = make_pipeline(ColumnSelector(columns, remainder=remainder, **kwargs),
                                 transformer,
                                 name=name,
                                 **config)
        pipelines.append (pipeline)

def _get_transformer_name (transformer, columns):
    columns_name = ''.join([x[0] for x in columns])
    if len(columns_name) > 5:
        columns_name = columns_name[:5]
    if isinstance(transformer,str):
        if transformer == 'passthrough':
            transformer_name = 'pass'
        elif transformer == 'drop':
            transformer_name = 'drop'
        else:
            raise ValueError (f'name {transformer} not recognized')
    elif hasattr(transformer, 'name'):
        transformer_name = transformer.name
    else:
        transformer_name = transformer.__class__.__name__
    name = f'{transformer_name}_{columns_name}'
    return name

def make_column_transformer_pipelines (*transformers, remainder='drop', **kwargs):
    pipelines = []
    all_columns = []
    for name, transformer, columns in transformers:
        _append_pipeline (pipelines, name, transformer, columns, **kwargs)
        all_columns.extend(columns)

    all_columns = list(set(all_columns))
    name = _get_transformer_name (remainder, ['r','e','m'])
    _append_pipeline (pipelines, name, remainder, all_columns, remainder=True, **kwargs)

    return pipelines

def make_column_transformer (*transformers, 
                             remainder: Union[str, 'Estimator', Component] ='drop', 
                             name: Optional[str]=None, 
                             class_name: Optional[str]=None, 
                             **kwargs):
    """
    Given input tuples (`transformer`, `columns`), ..., builds a BaseColumnTransformer that
    applies `transformer` to `columns` of the input DataFrame. Similar to scikit-learn.
    `remainder` can be `passthrough`, `drop` (default), or an estimator / Component.
    """
    transformers_with_name = []
    for transformer, columns in transformers:
        transformer_name = _get_transformer_name (transformer, columns)
        transformers_with_name.append ((transformer_name, transformer, columns))

    pipelines = make_column_transformer_pipelines (*transformers_with_name,
                                                   remainder=remainder,
                                                   **kwargs)
    column_transformer = _BaseColumnTransformer (name=name, class_name=class_name, **kwargs)
    column_transformer.set_components(*pipelines)
    return column_transformer

# %% ../../nbs/core/compose.ipynb 190
#@delegates ()
class MultiSplitComponent (MultiComponent, metaclass=abc.ABCMeta):
    """
    Apply component to multiple splits (training/validation/test) or subsets of data.

    Wrapper around given component. By default, the component is applied to training, 
    validation and test subsets in a loop, during inference, and it is applied to
    training set only during training, with the option to include validation and 
    test subsets as *additional data* for metric monitoring purposes.

    This is an abstract class that is to be subclassed for accommodating different
    data structures for holding the different splits. See for example `MultiSplitDict`
    for the case where we have each split in a separate field of a dictionary, 
    and `MultiSplitDFColumn` for the case where we have all the splits in the same
    dataframe, with one of the columns indicating the split.
    
    Parameters
    ----------
    component: Component.
        Component to be applied to the different splits or subsets of data.
    name: string or None, optional.
        Name given to the MultiSplitComponet object.
    class_name: string or None, optional.
        Class name given to the MultiSplitComponet object.
    fit_to: string, optional.
        Name of the split to be used for training.
    fit_additional: list of strings or None, optional.
        Name of additional splits to be passed to the `fit` method of the 
        wrapped component. These additional splits are gathered in a separate 
        argument called `additional_data`.
    apply_to: string or list of strings, optional.
        Name of the splits to be used for applying inference, transforming
        those subsets of data, etc.
    raise_error_if_split_doesnot_exist: bool or None, optional.
        Whether or not to raise an error if the split does not exist in the 
        input data.
    raise_warning_if_split_doesnot_exist: bool or None, optional.
        Whether or not to raise a warning if the split does not exist in the 
        input data.
    """
    def __init__ (self,
                  component: Union[Component, Callable, None]=None,
                  name: Optional[str]=None,
                  class_name: Optional[str]=None,
                  fit_to: str='training',
                  fit_additional: List[str]=[],
                  apply_to: List[str]=['training', 'validation', 'test'],
                  raise_error_if_split_doesnot_exist: bool=False,
                  raise_warning_if_split_doesnot_exist: bool=True,
                  **kwargs):
        
        self.warning_if_nick_name_exists=False # needed when calling set_components in next line
        super().__init__ (component, name=name, class_name=class_name, **kwargs)
        component = self.components[0]
        if not component.is_model: 
            self._fit = self._fit_
            self.is_model = False
        if class_name is None: self.class_name = f'{component.class_name}MultiSplit'
        if name is None: self.set_name (f'{component.name}_multi_split')
        self.component = component

    @abc.abstractmethod
    def _initialize_fit (self, X):
        pass

    @abc.abstractmethod
    def _include_split_in_fit (self, additional_data, split, X):
        pass

    @abc.abstractmethod
    def _select_training_split (self, X, fit_to):
        pass

    def _fit (self, *X, fit_to=None, **kwargs):
        fit_to = self.fit_to if fit_to is None else fit_to
        X = self._initialize_fit (*X)
        component = self.components[0]
        additional_data = {}
        for split in self.fit_additional:
            if split not in ['validation', 'test']:
                raise ValueError (f'split {split} not valid')
            self._include_split_in_fit (additional_data, split, X)
        X_fit = self._select_training_split (X, fit_to)
        component.fit(X_fit, split=self.fit_to, **additional_data, **kwargs)

    @abc.abstractmethod
    def _get_split_keys (self, X):
        pass

    def _issue_error_or_warning (self, split, X):
        message = f'split {split} not found in X ({self._get_split_keys (X)})'
        if self.raise_error_if_split_doesnot_exist:
            raise RuntimeError (message)
        elif self.raise_warning_if_split_doesnot_exist:
            warnings.warn (message)

    @abc.abstractmethod
    def _initialize_apply (self, X, apply_to, split):
        pass

    @abc.abstractmethod
    def _included_split (self, split, X):
        pass

    @abc.abstractmethod
    def _select_split (self, X, split):
        pass

    @abc.abstractmethod
    def _convert_result (self, result, input_not_dict, output_not_dict):
        pass

    @abc.abstractmethod
    def _initialize_result (self):
        pass

    @abc.abstractmethod
    def _add_result (self, result, split, result_split, X):
        pass

    def _apply (self, *X, apply_to = None, output_not_dict=False, split=None, **kwargs):
        apply_to = self.apply_to if apply_to is None else apply_to
        apply_to = apply_to if isinstance(apply_to, list) else [apply_to]
        X, input_not_dict = self._initialize_apply (*X, apply_to, split)

        component = self.components[0]
        result = self._initialize_result ()

        for split in apply_to:
            if self._included_split (split, X):
                X_split = self._select_split (X, split)
                result_split = component.apply (X_split, split=split, **kwargs)
                result = self._add_result (result, split, result_split, X)
            else:
                self._issue_error_or_warning (split, X)

        result = self._convert_result (result, input_not_dict, output_not_dict)
        return result

    def find_last_result (self, apply_to = None, split=None, **kwargs):
        apply_to = self.apply_to if apply_to is None else apply_to
        apply_to = apply_to if isinstance(apply_to, list) else [apply_to]

        self.is_data_source = True
        component = self.component
        for split in apply_to:
            if not (component.data_io.can_load_result () and 
                    component.data_io.exists_result (split=split, include_memory=True)):
                if isinstance (component, MultiComponent):
                    # TODO: have one flag is_data_source per split
                    # or make MultiSplitComponent a Parallel object
                    # or always use DataFrame
                    self.is_data_source = self.is_data_source and component.find_last_result (split=split)
                else:
                    self.is_data_source = False
        return self.is_data_source

    def find_last_fitted_model (self, apply_to = None, split=None, **kwargs):
        all_components_fitted = True
        self.load_all_estimators = False
        split = self.fit_to
        component = self.component
        if isinstance (component, MultiComponent):
            if not component.find_last_fitted_model (split=split):
                all_components_fitted = False
        elif (component.is_model and
              not (component.data_io.can_load_model () and 
                   component.data_io.exists_estimator (include_memory=True))):
                all_components_fitted = False

        if all_components_fitted and self.data_io.exists_result (split=split):
            self.data_io.load_estimator = self.data_io.load_estimators
            self.load_all_estimators = True
        self.all_components_fitted = all_components_fitted

        return all_components_fitted

# %% ../../nbs/core/compose.ipynb 192
#@delegates ()
class MultiSplitDict (MultiSplitComponent):
    """
    Apply component to multiple splits (training/validation/test) or 
    subsets of data.
    
    Subclass of `MultiSplitComponent` used for the case where we 
    have each split in a separate field of a dictionary. See parent 
    class for a detailed description.
    """
    def __init__ (self, component=None, **kwargs):
        super().__init__ (component=component, **kwargs)

    def __repr__ (self):
        return self._repr ('MultiSplitDict')

    def _initialize_fit (self, X):
        if not isinstance(X, dict):
            X = {self.fit_to: X}
        return X

    def _include_split_in_fit (self, additional_data, split, X):
        if split in X.keys():
            additional_data[f'{split}_data'] = X[split]
        else:
            self._issue_error_or_warning (split, X)

    def _select_training_split (self, X, fit_to):
        return X[fit_to]

    def _get_split_keys (self, X):
        return X.keys()

    def _initialize_apply (self, X, apply_to, split):
        if not isinstance(X, dict):
            key = apply_to[0] if len(apply_to)==1 else split if split is not None else 'test'
            X = {key: X}
            input_not_dict = True
            self.key = key
        else:
            input_not_dict = False

        return X, input_not_dict

    def _included_split (self, split, X):
        return split in X.keys()

    def _select_split (self, X, split):
        return X[split]

    def _initialize_result (self):
        return {}

    def _add_result (self, result, split, result_split, X):
        result[split] = result_split
        return result

    def _convert_result (self, result, input_not_dict, output_not_dict):
        if input_not_dict:
            result = result[self.key]
        elif output_not_dict and len(result)==1:
            result = list(result.items())[0][1]
        return result

# %% ../../nbs/core/compose.ipynb 215
#@delegates ()
class MultiSplitDFColumn (MultiSplitComponent):
    """
    Apply component to multiple splits (training/validation/test) or 
    subsets of data.
    
    Subclass of `MultiSplitComponent` used for the case where we have 
    all the splits in the same dataframe, with one of the columns 
    indicating the split. See parent class for a detailed description.
    """
    def __init__ (self, component=None, apply_to=['whole'], drop_split=False, add_split=True, **kwargs):
        if (getattr(component, 'apply_to_separate_splits', None)==True
            and ((isinstance (apply_to, list) and apply_to==['whole'])
                 or (isinstance(apply_to, str) and apply_to=='whole'))):
            apply_to = ['training', 'validation', 'test']
        super().__init__ (component=component, apply_to=apply_to, **kwargs)

    def __repr__ (self):
        return self._repr ('MultiSplitDF')

    def _initialize_fit (self, X):
        if 'split' not in X.columns:
            # X.index = pd.MultiIndex.from_arrays ([[self.fit_to]*X.shape[0], X.index], names=('split', 'number'))
            X['split'] = self.fit_to
        return X

    def _include_split_in_fit (self, additional_data, split, X):
        if split in X.split.values:
            additional_data[f'{split}_data'] = X[X.split==split]
        else:
            self._issue_error_or_warning (split, X)

    def _select_training_split (self, X, fit_to):
        return X[X.split==fit_to]

    def _get_split_keys (self, X):
        #X.index.get_level_values(0).unique()
        return X.split.unique()

    def _initialize_apply (self, X, apply_to, split):
        if 'split' not in X.columns:
            key = apply_to[0] if len(apply_to)==1 else split if split is not None else 'test'
            X['split'] = key
            input_not_dict = True
            self.key = key
        else:
            input_not_dict = False

        return X, input_not_dict

    def _included_split (self, split, X):
        #split in X.index.get_level_values(0).values
        return split == 'whole' or split in X.split.values

    def _select_split (self, X, split):
        X_split = X[X.split==split] if split != 'whole' else X
        if self.drop_split: X_split = X_split.drop(columns='split')
        return X_split

    def _initialize_result (self):
        return []

    def _add_result (self, result, split, result_split, X):
        if self.drop_split or (not hasattr(result_split, 'split') and self.add_split):
            if split=='whole': result_split['split'] = X.split
            else: result_split['split']=split
        result.append (result_split)
        return result

    def _convert_result (self, result, input_not_dict, output_not_dict):
        result = pd.concat (result, axis=0)
        return result

# %% ../../nbs/core/compose.ipynb 228
#@delegates ()
class ParallelInstancesCallback (ParallelCallback):
    def __init__ (self, pcomponent, create_component_storage_info=None, set_component_config=None, **kwargs):
        self.storage = None
        super ().__init__ (pcomponent, **kwargs)
        self.create_component_storage_info = (self.create_component_storage_info if create_component_storage_info is None
                                              else partial(create_component_storage_info, self))
        self.set_component_config = (self.set_component_config if set_component_config is None
                                              else partial(set_component_config, self))
            
            
    def create_component_storage_info (self):
        self.storage = Bunch (start_idx=[self.start_idx]*self.n_iterations,
                              is_data_source=[self.is_data_source]*self.n_iterations,
                              all_components_fitted=[self.all_components_fitted]*self.n_iterations,
                              load_all_estimators=[self.load_all_estimators]*self.n_iterations)

    def set_component_config (self, component, i):
        suffix = self.configs[i].get('suffix', '')
        component.set_suffix (suffix)

    def set_component_info (self, component, i):
        if self.storage is None: self.create_component_storage_info ()
        self.set_component_config (component, i)
        component.start_idx = copy.deepcopy(self.storage.start_idx[i])
        component.is_data_source = copy.deepcopy(self.storage.is_data_source[i])
        component.all_components_fitted = self.storage.all_components_fitted[i]
        component.load_all_estimators = self.storage.load_all_estimators[i]

    def store_component_find_last_result_info (self, component, i):
        self.storage.start_idx[i] = copy.deepcopy(component.start_idx)
        self.storage.is_data_source[i] = copy.deepcopy(component.is_data_source)

    def store_component_find_last_fitted_model_info (self, component, i):
        self.storage.start_idx[i] = copy.deepcopy(component.start_idx)
        self.storage.is_data_source[i] = copy.deepcopy(component.is_data_source)
        self.storage.all_components_fitted[i] = component.all_components_fitted
        self.storage.load_all_estimators[i] = component.load_all_estimators

# %% ../../nbs/core/compose.ipynb 230
#@delegates ()
class ParallelInstances (Parallel):
    """
    Parallel pipeline which applies the same component object
    multiple times.

    Usually, the component object is applied with a different
    configuration in each occasion, or receiving a different piece
    of information. 
    
    One of the differences with Parallel, is that information specific 
    about each iteration of the loop is held in the ParallelInstances 
    object, instead of the component object. 

    Using a `ParallelInstances` class is useful, for instance, when 
    we want to apply the same pipeline N times, but have N copies of 
    the pipeline is expensive in terms of memory. 

    Parameters
    ----------
    component: Component.
        Component object to be applied
    configs: list, optional.
        List of configurations, one for each time the component 
        is applied. Typically, each configuration is a dictionary of
        arguments to be used for the i-th application of the component.
    n_iterations: int, optional.
        Number of times the component is applied. If not given, this is 
        inferred from the length of the configs list.
    callback: ParallelCallback, optional.
        Callback called at different steps of each one of methods of this
        class. If not given, the default callback used is of 
        `ParallelInstancesCallback` class.
    """
    def __init__ (self, 
                  component: Component, 
                  configs: list=[], 
                  n_iterations: int=None, 
                  callback: Optional[ParallelCallback]=None, 
                  **kwargs):
        """Assigns attributes and calls parent constructor.
        """
        if callback is None: callback = ParallelInstancesCallback (self, **kwargs)
        n_iterations = len(configs) if n_iterations is None else n_iterations
        components = (component, ) * n_iterations
        super().__init__ (*components, callback=callback, **kwargs)
        self.cb.create_component_storage_info ()

    def __repr__ (self):
        return self._repr ('ParallelInstances')

    def set_unique_names (self):
        self.register_global_name (self)
        component = self.components[0]
        if isinstance (component, MultiComponent):
            component.set_unique_names ()
        else:
            self.register_global_name (component)

# %% ../../nbs/core/compose.ipynb 233
#@delegates ()
class CrossValidatorCallback (ParallelInstancesCallback):
    def __init__ (self, pcomponent, **kwargs):
        super ().__init__ (pcomponent, **kwargs)
        self.dict_results = None
        self.stored_fit_info = False

    def store_component_fit_info (self, component, i):
        if self.score_method is not None:
            score_method = self.find_method (self.score_method)
            if score_method is None:
                raise ValueError (f'score method {self.score_method} not found in {component}')

            dict_results = score_method()
            self._add_dict_results (dict_results)
            if self.trial is not None:
                if self.key_score is None:
                    self.logger.warning (f'key_score is None but trial is {self.trial}')
                else:
                    self.pcomponent._apply_pruner (dict_results, i)

    store_component_fit_apply_info = store_component_fit_info

    def _add_dict_results (self, dict_results):
        dict_results = copy.deepcopy(dict_results)
        if self.dict_results is None:
            self.dict_results = {k: np.array(v) if isinstance(v, list) else v
                                 for k, v in dict_results.items()}
        else:
            for k in dict_results: self.dict_results[k] += dict_results[k]

    def join_result (self, Xr, Xi_r, components, i):
        if self.evaluator is not None and self.add_evaluation:
            self._add_dict_results (Xi_r)
            return None
        elif self.dict_results is None:
            return super().join_result (Xr, Xi_r, components, i)

    def store_fit_info (self):
        if self.stored_fit_info:
            return
        if self.dict_results is not None:
            for k in self.dict_results:
                self.dict_results[k] = self.dict_results[k] / self.n_iterations
        self.data_io.save_result (self.dict_results, result_file_name='cross_validation_metrics.pk')
        if self.select_epoch:
            final_dict_results = self.dict_results.copy()
            for k in self.dict_results:
                if isinstance (self.dict_results[k], np.ndarray):
                    final_dict_results[f'last_{k}'] = self.dict_results[k][-1]
                    final_dict_results['n_iterations'] = self.n_iterations
                    if self.optimization_mode is None or self.optimization_mode=='max':
                        final_dict_results[f'argmax_{k}'] = np.argmax(self.dict_results[k])
                        final_dict_results[f'{self.max_prefix}{k}'] = np.max(self.dict_results[k])
                    if self.optimization_mode is None or self.optimization_mode=='min':
                        final_dict_results[f'argmin_{k}'] = np.argmin(self.dict_results[k])
                        final_dict_results[f'{self.min_prefix}{k}'] = np.min(self.dict_results[k])
                    if self.optimization_mode is None: del final_dict_results[k]
            self.dict_results = final_dict_results
            self.pcomponent.dict_results = self.dict_results
        self.stored_fit_info = True
        self.data_io.save_result (self.dict_results, result_file_name='cross_validation_final_metrics.pk')

    def finalize_result (self, Xr, components=None):
        if self.dict_results is not None:
            self.cb.store_fit_info ()
            result = self.dict_results
        else:
            result = super().finalize_result (Xr, components=components)
        self.pcomponent.dict_results = self.dict_results
        return result

# %% ../../nbs/core/compose.ipynb 235
#@delegates ()
class CrossValidator (ParallelInstances):
    """
    Runs cross-validation on given pipeline.

    Parameters
    ----------
    component: Component.
        Pipeline to be cross-validated.
    splitter: splitter object or None, optional.
        Generates the different training/test splits used at
        each cross-validation round. See `blocks.blocks.SkSplitGenerator`
        class.
    evaluator: list, optional.
        Evaluates the result at each cross-validation round, potentially
        using multiple metrics. See `blocks.blocks.Evaluator` class.
    n_iterations: list, optional.
        Number of cross-validation rounds. If not given, this is 
        inferred by calling `splitter.split_generator.get_n_splits()`
    score_method: string, optional.
        If an evaluator is not provided, it is assumed that the last component 
        of the pipeline contains a scoring method with name `score_method`,
        which is used for evaluation.
    select_epoch: bool, optional.
        Typically used for pipelines containing a Neural Network. In this 
        case, we might want to select the epoch that has the best average 
        performance across cross-validation rounds, assuming each round
        provides an array of performance metrics, one per N epochs. In 
        general, this can be used for any iterative pipelines that provide
        an array of performance metrics for each N iterations.
    add_evaluation: bool, optional.
        Whether or not to aggregate the evaluation scores across cross-validation
        rounds, by adding them up. 
    optimization_mode: string, optional.
        Whether the best score is the maximum ('max') or the minimum ('min'). If 
        None is provided, both the min and the max are included in the final 
        set of cross-validation metrics.
    trial: optuna's trial object, optional.
        Optuna's trial object that can be used when CrossValidator is used 
        in conjunction with Optuna's hyper-parameter search.
    key_score: list, optional.
        Score used for applying Optuna's pruner, which stops the learning 
        process when the value of the key_score does not compare favorably
        to the history of values obtained for other models trained before.
    pruner_optimization_mode: list, optional.
        Whether the best score used by the pruner is the maximum ('max') or the 
        minimum ('min'). 
    indicate_same_step: bool, optional.
        Whether we should report a constant epoch/step number to the Optuna's pruner,
        or report a different number per epoch/step.
    callback: Callback, optional.
        Callback called at different steps of each one of methods of this
        class. If not given, the default callback used is of 
        `CrossValidatorCallback` class.
    """

    def __init__ (self, 
                  component: Component, 
                  splitter: Optional['Splitter']=None, 
                  evaluator: Optional['Evaluator']=None, 
                  n_iterations: Optional[int]=None, 
                  score_method: Optional[str]=None,
                  select_epoch: bool=False, 
                  add_evaluation: bool=True, 
                  optimization_mode: Optional[str]=None, 
                  trial: Optional['Trial']=None,
                  key_score: Optional[str]=None, 
                  pruner_optimization_mode: Optional[str]=None, 
                  indicate_same_step: bool=False,
                  callback: Optional[ParallelCallback]=None, 
                  **kwargs):
        """Assigns attributes and calls parent constructor."""
        if callback is None: callback = CrossValidatorCallback (self, **kwargs)
        components = (splitter, component) if splitter is not None else (component, )
        components += (evaluator, ) if evaluator is not None else ()
        pipeline = Sequential (*components, **kwargs) if len(components)>1 else component

        assert splitter is not None or n_iterations is not None, 'either splitter or n_iterations need to be specified'
        n_iterations = splitter.split_generator.get_n_splits() if n_iterations is None else n_iterations
        configs = [dict(suffix=i) for i in range(n_iterations)]

        super().__init__ (pipeline, configs=configs, n_iterations=n_iterations, callback=callback, **kwargs)
        if optimization_mode is None:
            self.max_prefix = 'max_'
            self.min_prefix = 'min_'
        else:
            self.max_prefix = ''
            self.min_prefix = ''
        if self.trial is not None and self.key_score is not None and self.pruner_optimization_mode is None:
            self.pruner_optimization_mode = self.optimization_mode

    def _apply_pruner (self, dict_results, i):
        result = dict_results[self.key_score]
        if self.pruner_optimization_mode=='max': result = np.max(result)
        elif self.pruner_optimization_mode=='min': result = np.min(result)
        step_number = 0 if self.indicate_same_step else i
        self.logger.debug (f'reporting {result} at step {step_number}')
        self.trial.report (result, step_number)
        if self.trial.should_prune():
            self.logger.info (f'prunning at {i}-th fold')
            self.force_end = True
            self.n_iterations = i+1

# %% ../../nbs/core/compose.ipynb 267
def finalize_result_parallel_models (self, Xr, components=None):
    Xr = np.array(Xr)
    ndim = Xr.ndim
    if Xr.ndim > 2: Xr = np.squeeze (Xr)
    if Xr.ndim > 2: raise RuntimeError (f'Xr.ndim {ndim} > 2, not supported')
    assert Xr.shape[0] == self.n_models or Xr.shape[1] == self.n_models
    if Xr.shape[1]==self.n_models: Xr = Xr.T
    return Xr

# %% ../../nbs/core/compose.ipynb 269
#@delegates ()
class ParallelModelInstancesCallback (ParallelInstancesCallback):
    def __init__ (self, pcomponent, **kwargs):
        super ().__init__ (pcomponent, **kwargs)

    def set_component_config (self, component, i):
        super().set_component_config (component, i) # this needs to be called first
        if self.separate_model_paths is not None:
            component.data_io.set_full_path_models (self.separate_model_paths[i])

    def finalize_result (self, Xr, components=None):
        return finalize_result_parallel_models (self, Xr, components=components)

# %% ../../nbs/core/compose.ipynb 271
#@delegates ()
class ParallelModelInstances (ParallelInstances):
    """
    Ensemble that applies the same component (or model) object instance N times.

    Class similar in behaviour to its parent `ParallelInstances` except for
    two small details:
    - The i-th time the component is applied, the component is given a suffix i appended to its name, 
    for saving the results and trained model with a different name than others.
    - The result of all the N runs is converted into a 2D numpy array.

    Parameters
    ----------
    component: Component.
        Component object to be applied
    separate_model_paths: list of strings or None, optional.
        List of paths to each model file, one for each time the component 
        is run. Useful for the case where the individual models are saved
        in very different locations, without a common `path_models`
    n_models: int or None, optional.
        Number of models that is used in this ensemble. This is equivalent to
        the number of times the wrapped component is fitted to the data.
    callback: ParallelCallback or None, optional.
        Callback called at different steps of each one of methods of this
        class. If not given, the default callback used is of 
        `ParallelModelInstancesCallback` class.
    """
    def __init__ (self, 
                  component: Component, 
                  separate_model_paths: Optional[List[str]]=None, 
                  n_models: Optional[int]=None, 
                  callback: Optional[ParallelCallback]=None, 
                  **kwargs):
        """Assigns attributes and calls parent constructor."""
        if callback is None: callback = ParallelModelInstancesCallback (self, **kwargs)
        if n_models is None:
            if separate_model_paths is not None:
                n_models = len(separate_model_paths)
            else:
                raise ValueError ('either n_models or separate_model_paths need to be indicated')
        elif separate_model_paths is not None:
            assert n_models==len(separate_model_paths)

        configs = [dict(suffix=i) for i in range(n_models)]

        super().__init__ (component, configs=configs, n_iterations=n_models, callback=callback, **kwargs)

# %% ../../nbs/core/compose.ipynb 273
#@delegates ()
class WeightedClassifier (Component):
    def __init__ (self, weights=None, **kwargs):
        super().__init__ (**kwargs)
    def _apply (self, X, **kwargs):
        if self.weights is None:
            n_models = X.shape[0]
            self.weights = np.ones ((n_models, )) * 1.0/n_models
        return (self.weights.reshape(-1,1) * X).sum(axis=0)

# %% ../../nbs/core/compose.ipynb 275
def make_parallel_models (*components, 
                          n_models: Optional[int]=None, 
                          separate_model_paths: Optional[List[str]]=None, 
                          separate_result_paths: Optional[List[str]]=None, 
                          **kwargs):
    """
    Constructs an ensemble that applies N different components (or models), or N 
    copies of the same component (or model).

    Builds a class similar in behaviour to the `Parallel` class, except for
    two small details:
    - If the N components are copies of the same, the i-th component of the 
    ensemble is given a suffix i appended to its name, for saving the results 
    and trained model with a different name than others.
    - The result of all the N runs is converted into a 2D numpy array.

    Parameters
    ----------
    components: List of Component.
        List of components to be applied in the ensemble.
    separate_model_paths: list of strings, optional.
        List of paths to each model file, one for each time the component 
        is run. Useful for the case where the individual models are saved
        in very different locations, without a common `path_models`
    separate_result_paths: list of strings, optional.
        List of paths to each model file, one for each time the component 
        is run. Useful for the case where the individual models are saved
        in very different locations, without a common `path_models`
    n_models: int, optional.
        Number of models that is used in this ensemble. This is equivalent to
        the number of times the wrapped component is fitted to the data.
    """
    if len(components)==1:
        if type(components[0]) is not type: 
            warnings.warn ('building ensemble with a single component')
        else:
            component_cls = components[0]
            if n_models is not None and separate_model_paths is not None:
                raise ValueError ('only one of n_models or separate_model_paths must be passed, not both')
            if n_models is None and separate_model_paths is None:
                raise ValueError ('either n_models or separate_model_paths must be passed')
            if n_models is not None:
                components = (component_cls (**kwargs, suffix=i) for i in range(n_models))
            else:    
                n_models = len(separate_model_paths)
                components = [component_cls (**kwargs) for i in range(n_models)]    
                for i, component in enumerate(components):
                    if component.path_results is not None:
                        if separate_result_paths is not None:
                            component.data_io.set_full_path_results (separate_result_paths[i])
                        else:
                            component.set_suffix (i)
                            separate_result_path = component.data_io.get_path_result_file ()
                            component.set_suffix (None)
                            component.data_io.set_full_path_results (separate_result_path)
                    component.data_io.set_full_path_models (separate_model_paths[i])
    parallel_models = Parallel (*components, finalize_result=finalize_result_parallel_models, 
                                class_name='ParallelModels', **kwargs)
    parallel_models.n_models = len(parallel_models.components)
    return parallel_models

# %% ../../nbs/core/compose.ipynb 277
def make_ensembler (*components, 
                    instances: bool=False, 
                    final_classifier: Optional[Component]=None, 
                    root: bool=False, 
                    class_name: str='Ensembler', 
                    name: str='ensembler', 
                    **kwargs):
    """
    Constructs an ensembler pipeline.

    Parameters
    ----------
    components: List of Component.
        List of components to be applied in the ensemble.
    instances: bool, optional.
        Only used when we only have one component in the ensemble. In this
        case, If `instances` is False, the ensemble makes N copies of this
        component (see `make_parallel_models`). If it is True, it applies the 
        same instance N times, without making new copies, and modifies the 
        configuration of this instance each time before applying it.
    final_classifier: Component, optional.
        After the N components of the ensemble have been applied, the N outputs
        are combined using either a final classifier or some component that 
        performs this combination. If final_classifier is not provided, a
        `WeightedClassifier` is used, which simply performs a weighted sum
        of the ouputs, with uniform weights by default.
    root: bool, optional.
        Whther or not the constructed ensemble is the root of our pipeline.
    class_name: string, optional.
        Class name given to the constructed ensemble object. 
    name: string, optional.
        Name given to the constructed ensemble object
    """
    if len(components)==1 and type(components[0]) is not type:
        instances = True
    if instances:
        assert len(components)==1, 'when building an ensembler of instances, only one component can be provided'
        parallel_models = ParallelModelInstances (components[0], **kwargs)
    else:
        parallel_models = make_parallel_models (*components, **kwargs)
    if final_classifier is None:
        final_classifier = WeightedClassifier (**kwargs)

    return Sequential (parallel_models, final_classifier, root=root, class_name=class_name, name=name,
                       **kwargs)
