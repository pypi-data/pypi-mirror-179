{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["------ standard imports ------ #"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import scipy as sp\n", "import matplotlib.pyplot as plt\n", "import cv2 as cv"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import ansitable\n", "ansitable.options(unicode=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from spatialmath import *\n", "from spatialmath.base import *\n", "BasePoseMatrix._color=False\n", "from roboticstoolbox import *"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from spatialmath.base import *\n", "import math\n", "from math import pi"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from machinevisiontoolbox import *\n", "from machinevisiontoolbox.base import *"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["np.set_printoptions(\n", "    linewidth=120, formatter={\n", "        'float': lambda x: f\"{0:8.4g}\" if abs(x) < 1e-10 else f\"{x:8.4g}\"})"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["np.random.seed(0)\n", "cv.setRNGSeed(0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["------------------------------ #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Region Features"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Pixel Classification"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Monochrome Image Classification"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["castle = Image.Read(\"castle.png\", dtype=\"float\");\n", "(castle >= 0.7).disp();\n", "castle.hist().plot();\n", "t = castle.otsu()\n", "castle2 = Image.Read(\"castle2.png\", dtype=\"float\");\n", "t = castle2.otsu()\n", "castle2.adaptive_threshold().disp();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Color Image Classification"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["targets = Image.Read(\"yellowtargets.png\", dtype=\"float\", gamma=\"sRGB\");\n", "garden = Image.Read(\"tomato_124.png\", dtype=\"float\", gamma=\"sRGB\");\n", "ab = targets.colorspace(\"L*a*b*\").plane(\"a*:b*\")\n", "ab.plane(\"b*\").disp();\n", "targets_labels, targets_centroids, resid = ab.kmeans_color(k=2, seed=0)\n", "targets_labels.disp(colormap=\"jet\", colorbar=True);\n", "targets_centroids\n", "plot_chromaticity_diagram(colorspace=\"a*b*\");\n", "plot_point(targets_centroids, marker=\"*\", text=\"{}\");\n", "[color2name(c, \"a*b*\") for c in targets_centroids.T]\n", "resid / ab.npixels\n", "labels = ab.kmeans_color(centroids=targets_centroids)\n", "objects = (labels == 0)\n", "objects.disp();\n", "ab = garden.colorspace(\"L*a*b*\").plane(\"a*:b*\")\n", "garden_labels, garden_centroids, resid = ab.kmeans_color(k=3, seed=0);\n", "garden_centroids\n", "[color2name(c, \"a*b*\") for c in garden_centroids.T]\n", "tomatoes = (garden_labels == 2);\n", "data = np.random.rand(500, 2);  # 500 x 2D data points\n", "from scipy.cluster.vq import kmeans2\n", "centroids, labels = kmeans2(data, k=3)\n", "for i in range(3):\n", "  plot_point(data[labels==i, :].T, color=\"rgb\"[i], marker=\".\", markersize=10)\n", "tomatoes_binary = tomatoes.close(Kernel.Circle(radius=15));\n", "tomatoes_binary.disp();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Semantic Classification"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["scene = Image.Read(\"image3.jpg\")\n", "scene.disp();\n", "import torch\n", "import torchvision as tv\n", "transform = tv.transforms.Compose([\n", "   tv.transforms.ToTensor(),\n", "   tv.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n", "                           std=[0.229, 0.224, 0.225])]);\n", "in_tensor = transform(scene.image);\n", "model = tv.models.segmentation.fcn_resnet50(pretrained=True).eval();\n", "outputs = model(torch.stack([in_tensor]));\n", "labels = Image(torch.argmax(outputs[\"out\"].squeeze(), dim=0).detach().cpu().numpy());\n", "labels.disp(colormap=\"viridis\", ncolors=20, colorbar=True);\n", "(labels == 15).disp();\n", "scene.choose([255, 255, 255], labels != 15).disp();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Object Instance Representation"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Creating Binary Blobs"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sharks = Image.Read(\"sharks.png\");\n", "sharks.disp();\n", "labels, m = sharks.labels_binary()\n", "m\n", "labels.disp(colorbar=True);\n", "right_shark = (labels == 3);\n", "right_shark.disp();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Maximally Stable Extremal Regions (MSER)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["labels, m = castle2.labels_MSER()\n", "m\n", "labels.disp(colormap=\"viridis_r\", ncolors=m);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Graph-Based Segmentation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["grain = Image.Read(\"58060.png\")\n", "grain.disp();\n", "labels, m = grain.labels_graphseg()\n", "m\n", "labels.disp(colormap=\"viridis_r\", ncolors=m);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Object Instance Description"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Area"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["right_shark.sum()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Bounding Boxes"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["u, v = right_shark.nonzero()\n", "u.shape\n", "umin = u.min()\n", "umax = u.max()\n", "vmin = v.min()\n", "vmax = v.max()\n", "plot_box(lrbt=[umin, umax, vmin, vmax], color=\"g\");"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Moments"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["m00 = right_shark.mpq(0, 0)\n", "uc = right_shark.mpq(1, 0) / m00\n", "vc = right_shark.mpq(0, 1) / m00\n", "plot_point((uc, vc), [\"bo\", \"bx\"]);\n", "u20 = right_shark.upq(2, 0); u02 = right_shark.upq(0, 2); u11 = right_shark.upq(1, 1);\n", "J = np.array([[u20, u11], [u11, u02]])\n", "plot_ellipse(4 * J  / m00, centre=(uc, vc), inverted=True, color=\"blue\");\n", "lmbda, x = np.linalg.eig(J)\n", "lmbda\n", "a = 2 * np.sqrt(lmbda.max() / m00)\n", "b = 2 * np.sqrt(lmbda.min() / m00)\n", "b / a\n", "x\n", "i = np.argmax(lmbda)  # get index of largest eigenvalue\n", "v = x[:, i]\n", "np.rad2deg(np.arctan2(v[1], v[0]))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Blob Descriptors"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["blobs = sharks.blobs();\n", "blobs\n", "len(blobs)\n", "blobs[3]\n", "blobs[3].area\n", "blobs[3].umin\n", "blobs[3].aspect\n", "blobs[3].centroid\n", "blobs[3].moments.m00   # moment p=q=0\n", "blobs[3].moments.mu11  # central moment p=q=1\n", "blobs[3].moments.nu03  # normalized central moment p=0, q=3\n", "blobs.area\n", "blobs[3].plot_box(color=\"red\")\n", "blobs[:2].plot_box(color=\"red\")\n", "blobs.plot_centroid(marker=\"+\", color=\"blue\")\n", "blobs.plot_box(color=\"red\")\n", "sharks.roi(blobs[1].bbox).rotate(blobs[1].orientation).disp();\n", "blobs[blobs.area > 10_000]\n", "tomato_blobs = tomatoes_binary.blobs()\n", "tomato_blobs.filter(area=(1_000, 5_000))\n", "tomato_blobs.filter(touch=False)\n", "tomato_blobs.filter(area=[1000, 5000], touch=False, color=1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Blob Hieararchy"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["multiblobs = Image.Read(\"multiblobs.png\");\n", "multiblobs.disp();\n", "labels, m = multiblobs.labels_binary()\n", "m\n", "multiblobs.disp();\n", "blobs = multiblobs.blobs()\n", "blobs[1].children\n", "blobs[1].parent\n", "blobs.label_image().disp();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Shape from Moments"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["blobs = sharks.blobs()\n", "blobs.aspect\n", "blobs.humoments"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Shape from Perimeter"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["blobs[1].perimeter[:, :5]\n", "blobs[1].perimeter.shape\n", "blobs[1].plot_perimeter(color=\"orange\")\n", "sharks.disp();\n", "blobs.plot_perimeter(color=\"orange\")\n", "blobs.plot_centroid()\n", "p = blobs[1].perimeter_length\n", "blobs.circularity\n", "p = Polygon2(blobs[1].perimeter).moment(0, 0)\n", "r, th = blobs[1].polar();\n", "plt.plot(r, \"r\", th, \"b\");\n", "for blob in blobs:\n", "  r, theta = blob.polar()\n", "  plt.plot(r / r.sum());\n", "similarity, _ = blobs.polarmatch(1)\n", "similarity"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Object Detection using Deep Learning"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["scene = Image.Read(\"image3.jpg\")\n", "scene.disp();\n", "import torch\n", "import torchvision as tv\n", "transform = tv.transforms.ToTensor();\n", "in_tensor = transform(scene.image);\n", "model = tv.models.detection.fasterrcnn_resnet50_fpn(pretrained=True).eval();\n", "outputs = model(torch.stack([in_tensor]));\n", "scores = outputs[0][\"scores\"].detach().numpy(); # list of confidence scores\n", "labels = outputs[0][\"labels\"].detach().numpy(); # list of class names as strings\n", "boxes = outputs[0][\"boxes\"].detach().numpy();   # list of boxes as array([x1, y1, x2, y2])\n", "len(scores)\n", "classname_dict = {1: \"person\", 2: \"bicycle\", 3: \"car\", 4: \"motorcycle\", 18: \"dog\"};\n", "for score, label, box in zip(scores, labels, boxes):\n", "  if score > 0.5:  # only confident detections\n", "    plot_labelbox(classname_dict[label], lbrt=box, filled=True, alpha=0.3,\n", "                  color=\"yellow\", linewidth=2);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Summary"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Line Features"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["points5 = Image.Read(\"5points.png\", dtype=\"float\");\n", "square = Image.Squares(number=1, size=256, fg=128).rotate(0.3)\n", "edges = square.canny();\n", "h = edges.Hough();\n", "h.plot_accumulator()\n", "plt.plot(h.votes);\n", "plt.yscale(\"log\");\n", "lines = h.lines(60)\n", "square.disp();\n", "h.plot_lines(lines);\n", "church = Image.Read(\"church.png\", mono=True)\n", "edges = church.canny()\n", "h = edges.Hough();\n", "lines = h.lines_p(100, minlinelength=200, maxlinegap=5, seed=0);\n", "church.disp();\n", "h.plot_lines_p(lines, \"r--\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Summary"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Point Features"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Classical Corner Detectors"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["view1 = Image.Read(\"building2-1.png\", mono=True);\n", "view1.disp();\n", "harris1 = view1.Harris(nfeat=500)\n", "len(harris1)\n", "harris1[0]\n", "harris1[0].p\n", "harris1[0].strength\n", "harris1[:5].p\n", "harris1[:5].strength\n", "view1.disp(darken=True);\n", "harris1.plot();\n", "harris1[::5].plot()\n", "harris1.subset(20).plot()\n", "harris1 = view1.Harris(nfeat=500, scale=15)\n", "view1.Harris_corner_strength().disp();\n", "view2 = Image.Read(\"building2-2.png\", mono=True);\n", "harris2 = view2.Harris(nfeat=250);\n", "view2.disp(darken=True);\n", "harris2.plot();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Scale-Space Corner Detectors"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["foursquares = Image.Read(\"scale-space.png\", dtype=\"float\");\n", "G, L, s = foursquares.scalespace(60, sigma=2);\n", "L[5].disp(colormap=\"signed\");\n", "s[5]\n", "plt.plot(s[:-1], [-Ls.image[63, 63] for Ls in L]);\n", "features = findpeaks3d(np.stack([np.abs(Lk.image) for Lk in L], axis=2), npeaks=4)\n", "foursquares.disp();\n", "for feature in features:\n", "  plt.plot(feature[0], feature[1], 'k+')\n", "  scale = s[int(feature[2])]\n", "  plot_circle(radius=scale * np.sqrt(2), centre=feature[:2], color=\"y\")\n", "mona = Image.Read(\"monalisa.png\", dtype=\"float\");\n", "G, L, _ = mona.scalespace(8, sigma=8);\n", "Image.Hstack(G).disp();\n", "Image.Hstack(L).disp();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Scale-Space Point Feature"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sift1 = view1.SIFT(nfeat=200)\n", "sift1[0]\n", "view1.disp(darken=True);\n", "sift1.plot(filled=True, color=\"y\", hand=True, alpha=0.3)\n", "plt.hist(sift1.scale, bins=100);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Applications"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Character Recognition"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pytesseract as tess\n", "penguins = Image.Read(\"penguins.png\");\n", "ocr = tess.image_to_data(penguins.image < 100, output_type=tess.Output.DICT);\n", "for confidence, text in zip(ocr[\"conf\"], ocr[\"text\"]):\n", "  if text.strip() != \"\" and confidence > 0:\n", "    print(confidence, text)\n", "for i, (text, confidence) in enumerate(zip(ocr[\"text\"], ocr[\"conf\"])):\n", "  if text.replace(\" \", \"\") != \"\" and confidence > 50:\n", "    plot_labelbox(text,\n", "       lb=(ocr[\"left\"][i], ocr[\"top\"][i]), wh=(ocr[\"width\"][i], ocr[\"height\"][i]),\n", "       color=\"y\", filled=True, alpha=0.2)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Image Retrieval"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["images = ImageCollection(\"campus/*.png\", mono=True);\n", "features = [];\n", "for image in images:\n", "  features += image.SIFT()\n", "features.sort(by=\"scale\", inplace=True);\n", "len(features)\n", "features[:10].table()\n", "supports = [];\n", "for feature in features[:400]:\n", "   supports.append(feature.support(images))\n", "Image.Tile(supports, columns=20).disp(plain=True);\n", "feature = features[108]\n", "images[feature.id].disp();\n", "feature.plot(filled=True, color=\"y\", hand=True, alpha=0.5)\n", "bag = BagOfWords(features, 2_000, seed=0)\n", "w = bag.word(108)\n", "bag.occurrence(w)\n", "bag.contains(w)\n", "bag.exemplars(w, images).disp();\n", "word, freq = bag.wordfreq();\n", "np.max(freq)\n", "np.median(freq)\n", "plt.bar(word, -np.sort(-freq), width=1);  # sort in descending order\n", "bag = BagOfWords(features, 2_000, nstopwords=50, seed=0)\n", "v10 = bag.wwfv(10);\n", "v10.shape\n", "sim_10 = bag.similarity(v10);\n", "k = np.argsort(-sim_10)\n", "query = ImageCollection(\"campus/holdout/*.png\", mono=True);\n", "S = bag.similarity(query);\n", "Image(S).disp(colorbar=True);\n", "np.argmax(S, axis=1)\n", "bag.retrieve(query[0])\n", "bag.retrieve(query[1])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Wrapping Up"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Further Reading"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Exercises"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}